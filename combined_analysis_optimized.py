"""
Optimized Soccer Analysis Script
- Batch processing for YOLO (better GPU utilization)
- Performance monitoring
- Memory optimization
"""

import cv2
import numpy as np
import argparse
import gc  # Garbage collection for memory management
from collections import deque
import csv
import os
import sys
import time
import json
import subprocess
import shutil
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from multiprocessing import cpu_count
import multiprocessing as mp

# Logging setup
try:
    from logger_config import get_logger
    logger = get_logger("main")
    tracking_logger = get_logger("tracking")
    gallery_logger = get_logger("gallery")
except ImportError:
    import logging
    logger = logging.getLogger(__name__)
    tracking_logger = logger
    gallery_logger = logger
    logger.warning("Logger config not available - using basic logging")

# Conditional imports
try:
    from ultralytics import YOLO  # type: ignore
    import supervision as sv  # type: ignore
    import imutils  # type: ignore
    YOLO_AVAILABLE = True
except ImportError:
    YOLO_AVAILABLE = False
    logger.warning("YOLO dependencies not available. Player tracking will be skipped.")

# Re-ID tracker import
try:
    from reid_tracker import ReIDTracker  # type: ignore
    REID_AVAILABLE = True
except ImportError as e:
    REID_AVAILABLE = False
    logger.warning("âš  Re-ID tracker not available (reid_tracker.py not found or dependencies missing). "
                   "Re-ID features will be disabled. Install torch: pip install torch")
    logger.debug(f"Re-ID tracker import error: {e}")

try:
    from gsi_smoothing import apply_gsi_realtime, SKLEARN_AVAILABLE as GSI_AVAILABLE  # type: ignore
except ImportError:
    GSI_AVAILABLE = False
    logger.warning("GSI smoothing not available. Install sklearn: pip install scikit-learn")

# Advanced recognition modules
try:
    from jersey_number_ocr import JerseyNumberOCR
    JERSEY_OCR_AVAILABLE = True
except ImportError as e:
    JERSEY_OCR_AVAILABLE = False
    logger.warning("âš  Jersey Number OCR not available (jersey_number_ocr.py not found or dependencies missing). "
                   "Jersey number recognition features will be disabled.")
    logger.debug(f"Jersey Number OCR import error: {e}")

try:
    from gait_analyzer import GaitAnalyzer
    GAIT_ANALYZER_AVAILABLE = True
except ImportError as e:
    GAIT_ANALYZER_AVAILABLE = False
    logger.warning("âš  Gait Analyzer not available (gait_analyzer.py not found or dependencies missing). "
                   "Gait analysis features will be disabled.")
    logger.debug(f"Gait Analyzer import error: {e}")

try:
    from hard_negative_mining import HardNegativeMiner
    HARD_NEGATIVE_AVAILABLE = True
except ImportError as e:
    HARD_NEGATIVE_AVAILABLE = False
    logger.warning("âš  Hard Negative Mining not available (hard_negative_mining.py not found or dependencies missing). "
                   "Hard negative mining features will be disabled.")
    logger.debug(f"Hard Negative Mining import error: {e}")

try:
    from graph_tracker import GraphTracker
    GRAPH_TRACKER_AVAILABLE = True
except ImportError as e:
    GRAPH_TRACKER_AVAILABLE = False
    logger.warning("âš  Graph Tracker not available (graph_tracker.py not found or dependencies missing). "
                   "Graph-based tracking features will be disabled.")
    logger.debug(f"Graph Tracker import error: {e}")

# HOTA-guided tracking import
try:
    from hota_guided_tracking import HOTAGuidedTracker, integrate_hota_guidance_into_tracking
    HOTA_GUIDED_AVAILABLE = True
except ImportError as e:
    HOTA_GUIDED_AVAILABLE = False
    logger.warning("âš  HOTA-guided tracking not available (hota_guided_tracking.py not found or dependencies missing). "
                   "HOTA-guided tracking features will be disabled.")
    logger.debug(f"HOTA-guided tracking import error: {e}")

# Advanced tracking utilities for Expansion IOU (does not require Re-ID)
try:
    from advanced_tracking_utils import (
        calculate_expansion_iou,
        calculate_track_velocity
    )
    ADVANCED_TRACKING_UTILS_AVAILABLE = True
except ImportError as e:
    ADVANCED_TRACKING_UTILS_AVAILABLE = False
    logger.warning("âš  Advanced tracking utilities not available (advanced_tracking_utils.py not found or dependencies missing). "
                   "Expansion IoU and advanced velocity tracking features will be disabled.")
    logger.debug(f"Advanced tracking utilities import error: {e}")

# Unit conversion functions (meters to feet, m/s to mph)
def meters_to_feet(meters):
    """Convert meters to feet"""
    if meters is None:
        return None
    return meters * 3.28084

def mps_to_mph(mps):
    """Convert meters per second to miles per hour"""
    if mps is None:
        return None
    return mps * 2.23694

def draw_direction_arrow(frame, center, direction_angle, arrow_length=20, arrow_color=(255, 255, 255), 
                        arrow_thickness=2, arrow_head_size=8):
    """
    Draw direction arrow under feet pointing in direction of travel.
    
    Args:
        frame: Frame to draw on
        center: (x, y) center position
        direction_angle: Angle in radians (0 = right, Ï€/2 = down)
        arrow_length: Length of arrow shaft
        arrow_color: BGR color tuple
        arrow_thickness: Line thickness
        arrow_head_size: Size of arrowhead
    """
    import math
    cx, cy = center
    
    # Calculate arrow end point
    end_x = int(cx + arrow_length * math.cos(direction_angle))
    end_y = int(cy + arrow_length * math.sin(direction_angle))
    
    # Draw arrow shaft
    cv2.line(frame, (cx, cy), (end_x, end_y), arrow_color, arrow_thickness, cv2.LINE_AA)
    
    # Draw arrowhead
    arrow_angle1 = direction_angle + math.pi - math.pi / 6  # 150 degrees
    arrow_angle2 = direction_angle + math.pi + math.pi / 6  # 210 degrees
    
    head1_x = int(end_x + arrow_head_size * math.cos(arrow_angle1))
    head1_y = int(end_y + arrow_head_size * math.sin(arrow_angle1))
    head2_x = int(end_x + arrow_head_size * math.cos(arrow_angle2))
    head2_y = int(end_y + arrow_head_size * math.sin(arrow_angle2))
    
    # Draw arrowhead triangle
    arrow_points = np.array([[end_x, end_y], [head1_x, head1_y], [head2_x, head2_y]], np.int32)
    cv2.fillPoly(frame, [arrow_points], arrow_color)
    cv2.polylines(frame, [arrow_points], True, arrow_color, arrow_thickness, cv2.LINE_AA)


def draw_enhanced_feet_marker(frame, center, axes, style, color, opacity,
                              enable_glow, glow_intensity,
                              enable_shadow, shadow_offset, shadow_opacity,
                              enable_gradient, enable_pulse, pulse_speed, frame_num,
                              enable_particles, particle_count,
                              outline_thickness=3, fps=30,
                              show_direction_arrow=False, direction_angle=None, arrow_color=None):
    """
    Draw enhanced feet marker with high-quality graphics effects.
    Standalone function for use in combined_analysis_optimized.py
    OPTIMIZED: Early returns for disabled effects to improve performance
    """
    import math
    cx, cy = center
    width, height = axes[0] * 2, axes[1] * 2
    
    # OPTIMIZATION: Early return if all effects disabled and simple style
    # This avoids expensive calculations when only basic shapes are needed
    simple_styles = {"circle", "ellipse", "diamond", "hexagon", "ring"}
    if (style in simple_styles and 
        not enable_glow and not enable_shadow and not enable_gradient and 
        not enable_pulse and not enable_particles and not show_direction_arrow and
        outline_thickness == 0):
        # Fast path: just draw the basic shape
        base_color = tuple(int(c * opacity / 255.0) for c in color)
        if style == "circle":
            radius = (axes[0] + axes[1]) // 2
            cv2.circle(frame, center, radius, base_color, -1)
        elif style == "ellipse":
            cv2.ellipse(frame, center, axes, 0, 0, 360, base_color, -1)
        elif style == "diamond":
            half_w, half_h = axes[0], axes[1]
            points = np.array([
                [cx, cy - half_h], [cx + half_w, cy],
                [cx, cy + half_h], [cx - half_w, cy]
            ], np.int32)
            cv2.fillPoly(frame, [points], base_color)
        elif style == "hexagon":
            base_radius = (axes[0] + axes[1]) // 2
            points = []
            for i in range(6):
                angle = i * math.pi / 3
                x = int(cx + base_radius * math.cos(angle))
                y = int(cy + base_radius * math.sin(angle))
                points.append([x, y])
            cv2.fillPoly(frame, [np.array(points, np.int32)], base_color)
        elif style == "ring":
            base_radius = (axes[0] + axes[1]) // 2
            inner_radius = int(base_radius * 0.6)
            cv2.circle(frame, center, base_radius, base_color, base_radius - inner_radius)
        return  # Early return for fast path
    
    # Apply opacity to color
    base_color = tuple(int(c * opacity / 255.0) for c in color)
    
    # Calculate pulse scale if enabled
    if enable_pulse:
        pulse_phase = (frame_num * pulse_speed / fps) % (2 * math.pi)
        pulse_scale = 0.8 + 0.4 * (1.0 + math.sin(pulse_phase)) / 2.0
        width = int(width * pulse_scale)
        height = int(height * pulse_scale)
        axes = (width // 2, height // 2)
    
    # Draw shadow first (if enabled)
    if enable_shadow:
        shadow_color = (0, 0, 0)
        shadow_alpha = shadow_opacity / 255.0
        shadow_center = (cx + shadow_offset, cy + shadow_offset)
        
        if style == "circle":
            # Use average radius for circular shadow
            base_radius = (axes[0] + axes[1]) // 2
            shadow_layer = np.zeros_like(frame)
            cv2.circle(shadow_layer, shadow_center, base_radius, shadow_color, -1)
            frame[:, :] = cv2.addWeighted(frame, 1.0, shadow_layer, shadow_alpha, 0)
        elif style == "ellipse":
            shadow_layer = np.zeros_like(frame)
            cv2.ellipse(shadow_layer, shadow_center, axes, 0, 0, 360, shadow_color, -1)
            frame[:, :] = cv2.addWeighted(frame, 1.0, shadow_layer, shadow_alpha, 0)
        elif style == "diamond":
            half_w, half_h = axes[0], axes[1]
            shadow_points = np.array([
                [shadow_center[0], shadow_center[1] - half_h],
                [shadow_center[0] + half_w, shadow_center[1]],
                [shadow_center[0], shadow_center[1] + half_h],
                [shadow_center[0] - half_w, shadow_center[1]]
            ], np.int32)
            shadow_layer = np.zeros_like(frame)
            cv2.fillPoly(shadow_layer, [shadow_points], shadow_color)
            frame[:, :] = cv2.addWeighted(frame, 1.0, shadow_layer, shadow_alpha, 0)
        elif style == "star":
            # Use average radius for consistent sizing
            base_radius = (axes[0] + axes[1]) // 2
            outer_radius = base_radius
            inner_radius = outer_radius // 2
            shadow_points = []
            for i in range(10):
                angle = i * math.pi / 5
                radius = outer_radius if i % 2 == 0 else inner_radius
                x = int(shadow_center[0] + radius * math.cos(angle - math.pi / 2))
                y = int(shadow_center[1] + radius * math.sin(angle - math.pi / 2))
                shadow_points.append([x, y])
            shadow_points = np.array(shadow_points, np.int32)
            shadow_layer = np.zeros_like(frame)
            cv2.fillPoly(shadow_layer, [shadow_points], shadow_color)
            frame[:, :] = cv2.addWeighted(frame, 1.0, shadow_layer, shadow_alpha, 0)
        elif style == "hexagon":
            # Use average radius for consistent sizing
            base_radius = (axes[0] + axes[1]) // 2
            hex_radius = base_radius
            shadow_points = []
            for i in range(6):
                angle = i * math.pi / 3
                x = int(shadow_center[0] + hex_radius * math.cos(angle))
                y = int(shadow_center[1] + hex_radius * math.sin(angle))
                shadow_points.append([x, y])
            shadow_points = np.array(shadow_points, np.int32)
            shadow_layer = np.zeros_like(frame)
            cv2.fillPoly(shadow_layer, [shadow_points], shadow_color)
            frame[:, :] = cv2.addWeighted(frame, 1.0, shadow_layer, shadow_alpha, 0)
        elif style == "circle":
            # Use average radius for circular shadow
            base_radius = (axes[0] + axes[1]) // 2
            shadow_layer = np.zeros_like(frame)
            cv2.circle(shadow_layer, shadow_center, base_radius, shadow_color, -1)
            frame[:, :] = cv2.addWeighted(frame, 1.0, shadow_layer, shadow_alpha, 0)
    
    # Calculate base size from width and height settings
    # Use average for circular shapes, or use both dimensions for shapes that support it
    base_radius = (axes[0] + axes[1]) // 2  # Average radius for circular shapes
    max_radius = max(axes[0], axes[1])  # Maximum radius for some shapes
    
    # Draw main shape
    if style == "circle":
        # Circle: use average of width and height for true circle
        circle_radius = base_radius
        if enable_gradient:
            for r in range(circle_radius, 0, -2):
                gradient_alpha = int(opacity * (r / circle_radius))
                gradient_color = tuple(int(c * gradient_alpha / 255.0) for c in color)
                cv2.circle(frame, center, r, gradient_color, -1)
        else:
            cv2.circle(frame, center, circle_radius, base_color, -1)
        if outline_thickness > 0:
            cv2.circle(frame, center, circle_radius, (255, 255, 255), outline_thickness)
    elif style == "ellipse":
        # Ellipse: use both width and height dimensions
        if enable_gradient:
            for r in range(max(axes[0], axes[1]), 0, -2):
                gradient_alpha = int(opacity * (r / max(axes[0], axes[1])))
                gradient_color = tuple(int(c * gradient_alpha / 255.0) for c in color)
                cv2.ellipse(frame, center, (r, r), 0, 0, 360, gradient_color, -1)
        else:
            cv2.ellipse(frame, center, axes, 0, 0, 360, base_color, -1)
        if outline_thickness > 0:
            cv2.ellipse(frame, center, axes, 0, 0, 360, (255, 255, 255), outline_thickness)
    elif style == "diamond":
        # Diamond: uses both width and height
        half_w, half_h = axes[0], axes[1]
        points = np.array([
            [cx, cy - half_h], [cx + half_w, cy],
            [cx, cy + half_h], [cx - half_w, cy]
        ], np.int32)
        if enable_gradient:
            for scale in [1.0, 0.8, 0.6, 0.4, 0.2]:
                scaled_points = np.array([
                    [cx, int(cy - half_h * scale)],
                    [int(cx + half_w * scale), cy],
                    [cx, int(cy + half_h * scale)],
                    [int(cx - half_w * scale), cy]
                ], np.int32)
                gradient_alpha = int(opacity * scale)
                gradient_color = tuple(int(c * gradient_alpha / 255.0) for c in color)
                cv2.fillPoly(frame, [scaled_points], gradient_color)
        else:
            cv2.fillPoly(frame, [points], base_color)
        if outline_thickness > 0:
            cv2.polylines(frame, [points], True, (255, 255, 255), outline_thickness)
    elif style == "star":
        # Star: use average radius for consistent sizing
        outer_radius = base_radius
        inner_radius = outer_radius // 2
        points = []
        for i in range(10):
            angle = i * math.pi / 5
            radius = outer_radius if i % 2 == 0 else inner_radius
            x = int(cx + radius * math.cos(angle - math.pi / 2))
            y = int(cy + radius * math.sin(angle - math.pi / 2))
            points.append([x, y])
        points = np.array(points, np.int32)
        if enable_gradient:
            for scale in [1.0, 0.7, 0.4]:
                scaled_points = []
                for i in range(10):
                    angle = i * math.pi / 5
                    radius = (outer_radius if i % 2 == 0 else inner_radius) * scale
                    x = int(cx + radius * math.cos(angle - math.pi / 2))
                    y = int(cy + radius * math.sin(angle - math.pi / 2))
                    scaled_points.append([x, y])
                scaled_points = np.array(scaled_points, np.int32)
                gradient_alpha = int(opacity * scale)
                gradient_color = tuple(int(c * gradient_alpha / 255.0) for c in color)
                cv2.fillPoly(frame, [scaled_points], gradient_color)
        else:
            cv2.fillPoly(frame, [points], base_color)
        if outline_thickness > 0:
            cv2.polylines(frame, [points], True, (255, 255, 255), outline_thickness)
    elif style == "hexagon":
        # Hexagon: use average radius for consistent sizing
        hex_radius = base_radius
        points = []
        for i in range(6):
            angle = i * math.pi / 3
            x = int(cx + hex_radius * math.cos(angle))
            y = int(cy + hex_radius * math.sin(angle))
            points.append([x, y])
        points = np.array(points, np.int32)
        if enable_gradient:
            for scale in [1.0, 0.8, 0.6, 0.4, 0.2]:
                scaled_points = []
                for i in range(6):
                    angle = i * math.pi / 3
                    x = int(cx + hex_radius * scale * math.cos(angle))
                    y = int(cy + hex_radius * scale * math.sin(angle))
                    scaled_points.append([x, y])
                scaled_points = np.array(scaled_points, np.int32)
                gradient_alpha = int(opacity * scale)
                gradient_color = tuple(int(c * gradient_alpha / 255.0) for c in color)
                cv2.fillPoly(frame, [scaled_points], gradient_color)
        else:
            cv2.fillPoly(frame, [points], base_color)
        if outline_thickness > 0:
            cv2.polylines(frame, [points], True, (255, 255, 255), outline_thickness)
    elif style == "ring":
        # Ring: use average radius for circular ring
        ring_radius = base_radius
        inner_radius = int(ring_radius * 0.6)
        if outline_thickness > 0:
            cv2.circle(frame, center, ring_radius, base_color, outline_thickness * 2)
            cv2.circle(frame, center, inner_radius, (0, 0, 0), -1)
            cv2.circle(frame, center, ring_radius, (255, 255, 255), outline_thickness)
        else:
            # If no outline, just draw filled circle
            cv2.circle(frame, center, ring_radius, base_color, -1)
    elif style == "arrow":
        # Draw arrow pointing up - uses height for length, width for base
        arrow_length = axes[1] * 2
        arrow_width = axes[0]
        points = np.array([
            [cx, cy - arrow_length],  # Tip
            [cx - arrow_width // 2, cy - arrow_length // 2],  # Left base
            [cx - arrow_width // 4, cy - arrow_length // 2],  # Left inner
            [cx - arrow_width // 4, cy],  # Left bottom
            [cx + arrow_width // 4, cy],  # Right bottom
            [cx + arrow_width // 4, cy - arrow_length // 2],  # Right inner
            [cx + arrow_width // 2, cy - arrow_length // 2],  # Right base
        ], np.int32)
        if enable_gradient:
            for scale in [1.0, 0.8, 0.6, 0.4, 0.2]:
                scaled_length = int(arrow_length * scale)
                scaled_width = int(arrow_width * scale)
                scaled_points = np.array([
                    [cx, cy - scaled_length],
                    [cx - scaled_width // 2, cy - scaled_length // 2],
                    [cx - scaled_width // 4, cy - scaled_length // 2],
                    [cx - scaled_width // 4, cy],
                    [cx + scaled_width // 4, cy],
                    [cx + scaled_width // 4, cy - scaled_length // 2],
                    [cx + scaled_width // 2, cy - scaled_length // 2],
                ], np.int32)
                gradient_alpha = int(opacity * scale)
                gradient_color = tuple(int(c * gradient_alpha / 255.0) for c in color)
                cv2.fillPoly(frame, [scaled_points], gradient_color)
        else:
            cv2.fillPoly(frame, [points], base_color)
        if outline_thickness > 0:
            cv2.polylines(frame, [points], True, (255, 255, 255), outline_thickness)
    elif style == "glow":
        # Glow: use average radius for circular glow effect
        glow_radius = base_radius
        glow_layers = glow_intensity // 10
        for i in range(glow_layers, 0, -1):
            glow_alpha = int(opacity * (i / glow_layers) * 0.3)
            glow_color = tuple(int(c * glow_alpha / 255.0) for c in color)
            current_radius = glow_radius + i * 2
            cv2.circle(frame, center, current_radius, glow_color, -1)
        cv2.circle(frame, center, glow_radius, base_color, -1)
        if outline_thickness > 0:
            cv2.circle(frame, center, glow_radius, (255, 255, 255), outline_thickness)
    elif style == "pulse":
        # Pulse: use average radius for circular pulse
        pulse_radius = base_radius
        cv2.circle(frame, center, pulse_radius, base_color, -1)
        if outline_thickness > 0:
            cv2.circle(frame, center, pulse_radius, (255, 255, 255), outline_thickness)
    
    # ENHANCEMENT: Draw direction arrow if enabled
    if show_direction_arrow and direction_angle is not None:
        arrow_col = arrow_color if arrow_color is not None else (255, 255, 255)  # Default white
        # Draw arrow below feet marker (offset downward)
        arrow_center = (cx, cy + axes[1] + 5)  # 5 pixels below marker
        draw_direction_arrow(frame, arrow_center, direction_angle, 
                           arrow_length=20, arrow_color=arrow_col, 
                           arrow_thickness=2, arrow_head_size=8)
    
    # Apply glow effect (post-processing) - ENHANCED for better visibility
    if enable_glow and style != "glow":
        glow_radius = max(glow_intensity // 5, 3)  # Minimum glow radius for visibility
        if glow_radius > 0:
            mask = np.zeros(frame.shape[:2], dtype=np.uint8)
            if style == "circle":
                # Use average radius for circular shape
                base_radius = (axes[0] + axes[1]) // 2
                cv2.circle(mask, center, base_radius, 255, -1)
            elif style == "ellipse":
                cv2.ellipse(mask, center, axes, 0, 0, 360, 255, -1)
            elif style == "ring":
                # Use average radius for circular ring
                base_radius = (axes[0] + axes[1]) // 2
                cv2.circle(mask, center, base_radius, 255, -1)
            elif style == "arrow":
                # Arrow shape for glow mask
                arrow_length = axes[1] * 2
                arrow_width = axes[0]
                points = np.array([
                    [cx, cy - arrow_length],
                    [cx - arrow_width // 2, cy - arrow_length // 2],
                    [cx - arrow_width // 4, cy - arrow_length // 2],
                    [cx - arrow_width // 4, cy],
                    [cx + arrow_width // 4, cy],
                    [cx + arrow_width // 4, cy - arrow_length // 2],
                    [cx + arrow_width // 2, cy - arrow_length // 2],
                ], np.int32)
                cv2.fillPoly(mask, [points], 255)
            elif style == "diamond":
                half_w, half_h = axes[0], axes[1]
                points = np.array([
                    [cx, cy - half_h], [cx + half_w, cy],
                    [cx, cy + half_h], [cx - half_w, cy]
                ], np.int32)
                cv2.fillPoly(mask, [points], 255)
            elif style == "star":
                # Use average radius for consistent sizing
                base_radius = (axes[0] + axes[1]) // 2
                outer_radius = base_radius
                inner_radius = outer_radius // 2
                points = []
                for i in range(10):
                    angle = i * math.pi / 5
                    radius = outer_radius if i % 2 == 0 else inner_radius
                    x = int(cx + radius * math.cos(angle - math.pi / 2))
                    y = int(cy + radius * math.sin(angle - math.pi / 2))
                    points.append([x, y])
                points = np.array(points, np.int32)
                cv2.fillPoly(mask, [points], 255)
            elif style == "hexagon":
                # Use average radius for consistent sizing
                base_radius = (axes[0] + axes[1]) // 2
                hex_radius = base_radius
                points = []
                for i in range(6):
                    angle = i * math.pi / 3
                    x = int(cx + hex_radius * math.cos(angle))
                    y = int(cy + hex_radius * math.sin(angle))
                    points.append([x, y])
                points = np.array(points, np.int32)
                cv2.fillPoly(mask, [points], 255)
            elif style == "circle":
                # Use average radius for circular shape
                base_radius = (axes[0] + axes[1]) // 2
                cv2.circle(mask, center, base_radius, 255, -1)
            
            blurred = cv2.GaussianBlur(mask, (glow_radius * 2 + 1, glow_radius * 2 + 1), 0)
            glow_layer = np.zeros_like(frame)
            glow_alpha = glow_intensity / 100.0
            for c in range(3):
                glow_layer[:, :, c] = blurred * color[c] * glow_alpha
            frame[:, :] = cv2.addWeighted(frame, 1.0, glow_layer, 0.5, 0)
    
    # Draw particle effects
    if enable_particles:
        import random
        particle_radius = max(2, min(axes[0], axes[1]) // 4)
        for _ in range(particle_count):
            angle = random.uniform(0, 2 * math.pi)
            distance = random.uniform(max(axes[0], axes[1]) * 1.2, max(axes[0], axes[1]) * 2.0)
            px = int(cx + distance * math.cos(angle))
            py = int(cy + distance * math.sin(angle))
            particle_color = tuple(min(255, int(c * random.uniform(0.7, 1.3))) for c in base_color)
            cv2.circle(frame, (px, py), particle_radius, particle_color, -1)
            cv2.circle(frame, (px, py), particle_radius, (255, 255, 255), 1)

def mps2_to_fts2(mps2):
    """Convert meters per second squared to feet per second squared"""
    if mps2 is None:
        return None
    return mps2 * 3.28084

# Analytics formatting functions
def format_analytics_value(key, value, use_imperial=False):
    """Format analytics value for display"""
    if value is None or (isinstance(value, float) and np.isnan(value)):
        return None
    
    # Format based on key and unit preference
    if 'speed' in key.lower() or 'mph' in key.lower():
        if isinstance(value, (int, float)):
            if use_imperial or 'mph' in key.lower():
                return f"{value:.1f} mph"
            else:
                return f"{value:.1f} m/s"
    elif 'distance' in key.lower() or 'dist' in key.lower():
        if isinstance(value, (int, float)):
            if use_imperial or 'ft' in key.lower():
                return f"{value:.1f} ft"
            else:
                return f"{value:.1f} m"
    elif 'acceleration' in key.lower():
        if isinstance(value, (int, float)):
            if use_imperial or 'fts2' in key.lower():
                return f"{value:.1f} ft/sÂ²"
            else:
                return f"{value:.1f} m/sÂ²"
    elif 'angle' in key.lower():
        if isinstance(value, (int, float)):
            return f"{value:.0f}Â°"
    elif 'time' in key.lower():
        if isinstance(value, (int, float)):
            return f"{value:.1f}s"
    elif 'pct' in key.lower() or 'percent' in key.lower():
        if isinstance(value, (int, float)):
            return f"{value:.1f}%"
    elif 'count' in key.lower() or 'events' in key.lower() or 'changes' in key.lower():
        return f"{int(value)}"
    
    return str(value)

def get_analytics_text_for_player(analytics_dict, analytics_preferences, use_imperial=False):
    """Get formatted analytics text lines for a player"""
    if not analytics_dict or not analytics_preferences:
        return []
    
    lines = []
    
    # Map preference keys to analytics dictionary keys (supports both analytics dict format and CSV column format)
    preference_to_key = {
        'current_speed': ['player_speed_mph', 'player_speed_mps', 'speed_mps'],  # CSV format and analytics dict format
        'average_speed': ['avg_speed_mph', 'avg_speed_mps', 'avg_speed_mps'],
        'max_speed': ['max_speed_mph', 'max_speed_mps', 'max_speed_mps'],
        'acceleration': ['player_acceleration_fts2', 'player_acceleration_mps2', 'acceleration_mps2'],
        'distance_traveled': ['distance_traveled_ft', 'distance_traveled_m', 'distance_traveled_m'],
        'distance_to_ball': ['distance_to_ball_px', 'distance_to_ball'],
        'distance_from_center': ['distance_from_center_ft', 'distance_from_center_m', 'distance_from_center_m'],
        'distance_from_goal': ['distance_from_goal_ft', 'distance_from_goal_m', 'distance_from_goal_m'],
        'nearest_teammate': ['nearest_teammate_dist_ft', 'nearest_teammate_dist_m', 'nearest_teammate_dist_m'],
        'nearest_opponent': ['nearest_opponent_dist_ft', 'nearest_opponent_dist_m', 'nearest_opponent_dist_m'],
        'field_zone': ['field_zone'],
        'field_position': ['field_position_x_pct', 'field_position_y_pct'],
        'movement_angle': ['player_movement_angle', 'movement_angle'],
        'possession_time': ['possession_time_s'],
        'time_stationary': ['time_stationary_s'],
        'sprint_count': ['sprint_count'],
        'direction_changes': ['direction_changes'],
        'acceleration_events': ['acceleration_events'],
        'distance_walking': ['distance_walking_ft', 'distance_walking_m', 'distance_walking_m'],
        'distance_jogging': ['distance_jogging_ft', 'distance_jogging_m', 'distance_jogging_m'],
        'distance_running': ['distance_running_ft', 'distance_running_m', 'distance_running_m'],
        'distance_sprinting': ['distance_sprinting_ft', 'distance_sprinting_m', 'distance_sprinting_m']
    }
    
    display_labels = {
        'current_speed': 'Speed',
        'average_speed': 'Avg',
        'max_speed': 'Max',
        'acceleration': 'Accel',
        'distance_traveled': 'Dist',
        'distance_to_ball': 'Ball',
        'distance_from_center': 'Center',
        'distance_from_goal': 'Goal',
        'nearest_teammate': 'Tm',
        'nearest_opponent': 'Opp',
        'field_zone': 'Zone',
        'field_position': 'Pos',
        'movement_angle': 'Angle',
        'possession_time': 'Poss',
        'time_stationary': 'Stat',
        'sprint_count': 'Sprint',
        'direction_changes': 'Dir',
        'acceleration_events': 'Accel',
        'distance_walking': 'Walk',
        'distance_jogging': 'Jog',
        'distance_running': 'Run',
        'distance_sprinting': 'Sprint'
    }
    
    for pref_key, enabled in analytics_preferences.items():
        if not enabled:
            continue
        
        if pref_key in preference_to_key:
            keys = preference_to_key[pref_key]
            value = None
            
            for key in keys:
                if key in analytics_dict:
                    value = analytics_dict[key]
                    break
            
            if value is not None:
                # Convert to imperial if needed (analytics dict always has metric values)
                # Check which key was found to determine if conversion is needed
                found_key = None
                for key in keys:
                    if key in analytics_dict:
                        found_key = key
                        break
                
                if use_imperial and found_key:
                    # Convert metric values to imperial
                    if pref_key in ['current_speed', 'max_speed', 'average_speed']:
                        if 'mps' in found_key and isinstance(value, (int, float)):
                            value = mps_to_mph(value)
                    elif pref_key in ['distance_traveled', 'distance_from_center', 'distance_from_goal', 
                                     'nearest_teammate', 'nearest_opponent', 'distance_walking', 
                                     'distance_jogging', 'distance_running', 'distance_sprinting']:
                        if 'm' in found_key and 'ft' not in found_key and isinstance(value, (int, float)):
                            value = meters_to_feet(value)
                    elif pref_key == 'acceleration':
                        if 'mps2' in found_key and 'fts2' not in found_key and isinstance(value, (int, float)):
                            value = mps2_to_fts2(value)
                
                formatted = format_analytics_value(pref_key, value, use_imperial)
                if formatted:
                    label = display_labels.get(pref_key, pref_key)
                    
                    # Special handling for field_position
                    if pref_key == 'field_position':
                        x_pct = analytics_dict.get('field_position_x_pct')
                        y_pct = analytics_dict.get('field_position_y_pct')
                        if x_pct is not None and y_pct is not None:
                            lines.append(f"{label}: {x_pct:.0f}%,{y_pct:.0f}%")
                    else:
                        lines.append(f"{label}: {formatted}")
    
    return lines

# Player Gallery import for cross-video identification
try:
    from player_gallery import PlayerGallery  # type: ignore
    GALLERY_AVAILABLE = True
except ImportError:
    GALLERY_AVAILABLE = False
    logger.warning("Player Gallery not available. player_gallery.py not found.")

# Enhanced tracking imports
try:
    from enhanced_tracking import EnhancedKalmanFilter, EMASmoother, filter_by_confidence, filter_by_size  # type: ignore
    ENHANCED_TRACKING_AVAILABLE = True
except ImportError:
    ENHANCED_TRACKING_AVAILABLE = False
    logger.debug("Enhanced tracking not available.")

# OC-SORT tracker import
try:
    from ocsort_tracker import OCSortTracker  # type: ignore
    OCSORT_AVAILABLE = True
except ImportError:
    OCSORT_AVAILABLE = False
    logger.info("OC-SORT tracker not available. Using ByteTrack as fallback.")

# Track post-processing import
try:
    from track_postprocessing import TrackPostProcessor  # type: ignore
    POSTPROCESSING_AVAILABLE = True
except ImportError:
    POSTPROCESSING_AVAILABLE = False
    logger.debug("Track post-processing not available.")

# BoxMOT tracker import
try:
    from boxmot_tracker_wrapper import create_boxmot_tracker, BOXMOT_AVAILABLE as BOXMOT_WRAPPER_AVAILABLE  # type: ignore
    BOXMOT_AVAILABLE = BOXMOT_WRAPPER_AVAILABLE
except ImportError:
    BOXMOT_AVAILABLE = False
    logger.debug("BoxMOT tracker wrapper not available. Install boxmot (pip install boxmot) to use BoxMOT trackers.")

try:
    import matplotlib.pyplot as plt  # type: ignore
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    logger.warning("matplotlib not available. Heatmap generation will be skipped.")


def remove_net_pattern(frame, kernel_size=None, sigma=None, use_bilateral=False):
    """
    Aggressive low-pass + morphological cleanup to erase regular net grid.
    Battle-tested solution for removing safety net patterns that interfere with player detection.
    Now with adaptive parameters based on frame size and optional bilateral filter for better edge preservation.
    
    Args:
        frame: Input BGR frame
        kernel_size: Gaussian blur kernel size (None = adaptive based on frame size)
        sigma: Gaussian blur sigma (None = adaptive based on frame size)
        use_bilateral: If True, use bilateral filter instead of Gaussian for better edge preservation
    
    Returns:
        Processed frame with net pattern removed
    """
    height, width = frame.shape[:2]
    frame_area = height * width
    
    # Adaptive parameters based on frame size
    # For larger frames, use larger kernels to maintain effectiveness
    if kernel_size is None:
        # Scale kernel size based on frame area (reference: 1920x1080 = ~2M pixels)
        base_size = 21
        scale_factor = max(1.0, frame_area / (1920 * 1080))
        kernel_size = int(base_size * scale_factor)
        # Ensure odd number for kernel
        if kernel_size % 2 == 0:
            kernel_size += 1
    
    if sigma is None:
        # Scale sigma proportionally to kernel size
        base_sigma = 7
        sigma = base_sigma * (kernel_size / 21.0)
    
    # 1. Blur (Gaussian or bilateral for better edge preservation)
    if use_bilateral:
        # Bilateral filter preserves edges better than Gaussian
        # Parameters: d=9 (diameter), sigmaColor=75, sigmaSpace=75
        blurred = cv2.bilateralFilter(frame, 9, 75, 75)
    else:
        # Gaussian blur (softens net lines)
        blurred = cv2.GaussianBlur(frame, (kernel_size, kernel_size), sigma)
    
    # 2. Morphological opening (removes thin net lines)
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
    opened = cv2.morphologyEx(blurred, cv2.MORPH_OPEN, kernel, iterations=2)
    
    # 3. Inpaint small holes (for stubborn net shadows)
    mask = cv2.absdiff(frame, opened)
    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
    mask = cv2.threshold(mask, 30, 255, cv2.THRESH_BINARY)[1]
    inpainted = cv2.inpaint(frame, mask, 3, cv2.INPAINT_TELEA)
    
    return inpainted


def predict_detections_with_optical_flow(detections, prev_gray, current_gray, flow_scale=1.0, remove_net=False):
    """
    Use optical flow to predict where detections should be in the current frame.
    This helps with tracking continuity when players are occluded or detection is missed.
    Enhanced with adaptive flow scale and net-aware masking for indoor net footage.
    
    Args:
        detections: Current detections (sv.Detections object)
        prev_gray: Previous frame (grayscale, uint8)
        current_gray: Current frame (grayscale, uint8)
        flow_scale: Base scale factor for flow prediction (default: 1.0, will be adapted based on flow magnitude)
        remove_net: Whether net removal is enabled (for net-aware flow masking)
    
    Returns:
        detections with predicted xyxy positions based on optical flow
    """
    if prev_gray is None or current_gray is None or len(detections) == 0:
        return detections
    
    try:
        # Net-aware flow mask: Create mask to identify net regions
        # Indoor nets create high-frequency noise that makes flow unreliable
        net_mask = None
        if remove_net:
            try:
                # Create net mask using same logic as remove_net_pattern
                # Detect high-frequency patterns (net grid) in previous frame
                blurred_prev = cv2.GaussianBlur(prev_gray, (21, 21), 7)
                net_mask = cv2.absdiff(prev_gray, blurred_prev)
                net_mask = cv2.threshold(net_mask, 30, 255, cv2.THRESH_BINARY)[1]
            except Exception:
                net_mask = None  # If mask creation fails, continue without it
        
        # Compute optical flow using Farneback method
        # Parameters optimized for soccer video:
        # - pyr_scale=0.5: Image pyramid scale
        # - levels=3: Number of pyramid levels
        # - winsize=15: Averaging window size
        # - iterations=3: Number of iterations at each pyramid level
        # - poly_n=5: Size of pixel neighborhood
        # - poly_sigma=1.2: Standard deviation for Gaussian smoothing
        # Create empty flow array (2-channel float32: x and y flow components)
        flow = np.zeros((prev_gray.shape[0], prev_gray.shape[1], 2), dtype=np.float32)
        flow = cv2.calcOpticalFlowFarneback(
            prev_gray, 
            current_gray, 
            flow,
            0.5,  # pyr_scale
            3,    # levels
            15,   # winsize
            3,    # iterations
            5,    # poly_n
            1.2,  # poly_sigma
            0     # flags
        )
        
        # Apply net-aware flow masking: Distrust flow in net regions
        if net_mask is not None:
            # Reduce flow magnitude in net regions (multiply by 0.3)
            # This prevents net noise from corrupting player motion prediction
            flow[net_mask > 0] *= 0.3
        
        # ðŸ›¡ï¸ GRAVITY BIAS CORRECTION: Detect and correct downward bias in optical flow
        # Indoor nets and camera shake can create systematic downward drift
        # Calculate mean Y-flow (vertical component) to detect bias
        mean_y_flow = np.mean(flow[:, :, 1])
        
        # If there's a significant downward bias (>5 pixels), correct it
        # This prevents boxes from sliding down due to net artifacts or camera shake
        if mean_y_flow > 5:  # Downward bias detected
            # Subtract bias from Y-flow (reduces downward drift)
            # Use adaptive correction strength based on bias magnitude
            correction_strength = min(0.7, mean_y_flow / 20.0)  # Stronger correction for larger bias
            flow[:, :, 1] -= mean_y_flow * correction_strength
        
        # Adaptive flow scale: Adjust based on flow magnitude
        # Indoor nets create chaotic motion â†’ use lower scale for reliability
        # Calm motion (players moving smoothly) â†’ trust flow more
        flow_magnitude = np.mean(np.linalg.norm(flow, axis=-1))
        adaptive_flow_scale = 0.8 if flow_magnitude < 15 else 0.5  # Calm = trust, chaotic = cautious
        # Combine with base flow_scale
        final_flow_scale = flow_scale * adaptive_flow_scale
        
        # Predict new positions for each detection
        predicted_xyxy = detections.xyxy.copy()
        
        for i in range(len(detections)):
            x1, y1, x2, y2 = detections.xyxy[i]
            
            # Get center point of bounding box
            center_x = int((x1 + x2) / 2)
            center_y = int((y1 + y2) / 2)
            
            # Ensure coordinates are within frame bounds
            h, w = current_gray.shape
            center_x = max(0, min(w - 1, center_x))
            center_y = max(0, min(h - 1, center_y))
            
            # Get flow vector at center point
            flow_x = flow[center_y, center_x, 0]
            flow_y = flow[center_y, center_x, 1]
            
            # Apply adaptive scale factor
            flow_x *= final_flow_scale
            flow_y *= final_flow_scale
            
            # Predict new bounding box position
            # Use average flow in a small region around center for more stable prediction
            y_start = max(0, center_y - 5)
            y_end = min(h, center_y + 6)
            x_start = max(0, center_x - 5)
            x_end = min(w, center_x + 6)
            
            if y_end > y_start and x_end > x_start:
                # Average flow in small region for stability
                flow_region = flow[y_start:y_end, x_start:x_end]
                avg_flow_x = np.mean(flow_region[:, :, 0]) * final_flow_scale
                # PATCH 3: Damp vertical flow by 30% to reduce downward slide from net artifacts
                avg_flow_y = np.mean(flow_region[:, :, 1]) * final_flow_scale * 0.7  # Damp y-flow by 30%
            else:
                avg_flow_x = flow_x
                # PATCH 3: Damp vertical flow by 30% to reduce downward slide from net artifacts
                avg_flow_y = flow_y * 0.7  # Damp y-flow by 30%
            
            # Calculate box dimensions
            box_width = x2 - x1
            box_height = y2 - y1
            
            # Predict new position
            new_x1 = x1 + avg_flow_x
            new_y1 = y1 + avg_flow_y
            new_x2 = x2 + avg_flow_x
            new_y2 = y2 + avg_flow_y
            
            # Ensure predicted box stays within frame bounds
            new_x1 = max(0, min(w, new_x1))
            new_y1 = max(0, min(h, new_y1))
            new_x2 = max(0, min(w, new_x2))
            new_y2 = max(0, min(h, new_y2))
            
            # Ensure valid box (x2 > x1, y2 > y1)
            if new_x2 > new_x1 and new_y2 > new_y1:
                predicted_xyxy[i] = [new_x1, new_y1, new_x2, new_y2]
        
        # Create new detections with predicted positions
        # Keep original detections but update xyxy
        detections.xyxy = predicted_xyxy
        
        return detections
        
    except Exception as e:
        # If optical flow fails, return original detections
        # Log error occasionally to avoid spam
        import traceback
        if hasattr(predict_detections_with_optical_flow, '_error_count'):
            predict_detections_with_optical_flow._error_count += 1
        else:
            predict_detections_with_optical_flow._error_count = 1
        
        if predict_detections_with_optical_flow._error_count % 100 == 1:
            print(f"âš  Optical flow prediction failed: {e}")
        
        return detections


def filter_net_detections(dets, frame_h, frame_w, exclude_zones=None, min_bbox_area=200, min_bbox_width=10, min_bbox_height=15):
    """
    Remove detections that look like net or are in exclusion zones.
    Net fragments are typically small, square-ish, and have low confidence.
    Also filters detections in foreground/upper areas where nets typically appear.
    
    Args:
        dets: Array of detections [x1, y1, x2, y2, conf, cls]
        frame_h: Frame height
        frame_w: Frame width
        exclude_zones: List of exclusion zones [(x1, y1, x2, y2), ...] from field calibration
        min_bbox_area: Minimum bbox area (from GUI settings, default: 200)
        min_bbox_width: Minimum bbox width (from GUI settings, default: 10)
        min_bbox_height: Minimum bbox height (from GUI settings, default: 15)
    
    Returns:
        Filtered detections array
    """
    if len(dets) == 0:
        return dets
    
    # Use GUI settings for minimum sizes (multiplied for net filtering to be more lenient)
    # Net filtering should be more aggressive than player filtering
    net_min_area = max(800, min_bbox_area * 4)  # At least 4x player minimum, but minimum 800
    net_min_height = max(60, min_bbox_height * 4)  # At least 4x player minimum, but minimum 60
    net_min_width = max(30, min_bbox_width * 3)  # At least 3x player minimum, but minimum 30
    
    valid = []
    for det in dets:
        if len(det) < 6:
            continue
            
        x1, y1, x2, y2, conf, cls = det[:6]
        w = x2 - x1
        h = y2 - y1
        area = w * h
        aspect = w / h if h > 0 else 0
        center_x = (x1 + x2) / 2
        center_y = (y1 + y2) / 2
        
        # Check if detection is in exclusion zone
        if exclude_zones:
            in_exclusion_zone = False
            for ex_x1, ex_y1, ex_x2, ex_y2 in exclude_zones:
                if (ex_x1 <= center_x <= ex_x2 and ex_y1 <= center_y <= ex_y2):
                    in_exclusion_zone = True
                    break
            if in_exclusion_zone:
                continue  # Skip detections in exclusion zones
        
        # Filter net fragments: small area, square-ish, low conf
        # Use configurable minimums (scaled for net filtering)
        if area < net_min_area:
            continue
        if 0.7 < aspect < 1.4 and conf < 0.3:
            continue
        if h < net_min_height or w < net_min_width:  # Use configurable minimums
            continue
        
        # Filter detections in upper/foreground areas (where nets typically are)
        # If detection is in upper 30% of frame and has characteristics of net
        if center_y < frame_h * 0.3:
            # Large detections in upper area are likely net (not players)
            if area > 50000:  # Very large detection in upper area
                continue
            # Square-ish detections in upper area with medium confidence
            if 0.8 < aspect < 1.2 and conf < 0.5:
                continue
        
        # Filter detections in left/right edges (where nets often appear)
        edge_threshold = 0.15  # 15% of frame width
        if (center_x < frame_w * edge_threshold or center_x > frame_w * (1 - edge_threshold)):
            # Large detections at edges are likely net
            if area > 30000 and conf < 0.6:
                continue
            
        valid.append(det)
    
    return np.array(valid) if len(valid) > 0 else np.array([]).reshape(0, 6)


def load_ball_color_config():
    """Load ball color configuration from file if available"""
    config_file = "ball_color_config.json"
    if os.path.exists(config_file):
        try:
            with open(config_file, 'r') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"Could not load ball color config: {e}", exc_info=True)
    return None


def load_team_color_config():
    """Load team color configuration from file if available"""
    config_file = "team_color_config.json"
    if os.path.exists(config_file):
        try:
            with open(config_file, 'r') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"Could not load team color config: {e}", exc_info=True)
    return None


def normalize_team_name(team_name, team_colors):
    """
    Normalize team name to handle both team names (e.g., "Gray", "Blue") and team keys (e.g., "team1", "team2").
    Returns the canonical team name from config, or the original if not found.
    
    Args:
        team_name: Team name or key (e.g., "Gray", "Blue", "team1", "team2")
        team_colors: Team color configuration dict
    
    Returns:
        Normalized team name (e.g., "Gray" or "Blue"), or original if not found
    """
    if not team_name or not team_colors or 'team_colors' not in team_colors:
        return team_name
    
    team_name_lower = team_name.lower() if team_name else ""
    
    # Check if it's already a team key
    if team_name_lower in ['team1', 'team2']:
        team_data = team_colors['team_colors'].get(team_name_lower, {})
        return team_data.get('name', team_name)
    
    # Check if it matches a team name
    for team_key in ["team1", "team2"]:
        team_data = team_colors['team_colors'].get(team_key, {})
        team_data_name = team_data.get('name', '').lower()
        if team_name_lower == team_data_name:
            return team_data.get('name', team_name)  # Return the actual name from config
    
    # If no match found, return original
    return team_name


def teams_match(team1, team2, team_colors):
    """
    Check if two team names/keys refer to the same team.
    Handles both team names (e.g., "Gray", "Blue") and team keys (e.g., "team1", "team2").
    
    Args:
        team1: First team name or key
        team2: Second team name or key
        team_colors: Team color configuration dict
    
    Returns:
        True if teams match, False otherwise
    """
    if not team1 or not team2:
        return False
    
    # Normalize both team names
    norm1 = normalize_team_name(team1, team_colors)
    norm2 = normalize_team_name(team2, team_colors)
    
    # Compare normalized names (case-insensitive)
    return norm1.lower() == norm2.lower()


def save_team_color_config(team_colors):
    """Save team color configuration to file with JSON protection"""
    config_file = "team_color_config.json"
    try:
        # Use safe JSON saving with corruption protection
        try:
            from json_utils import safe_json_save
            from pathlib import Path
            success = safe_json_save(Path(config_file), team_colors, create_backup=True, validate=True)
            if success:
                logger.info(f"Saved team color config: {config_file}")
            else:
                logger.error(f"Failed to save team color config: {config_file}")
            return success
        except ImportError:
            # Fallback to standard JSON if json_utils not available
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(team_colors, f, indent=2, ensure_ascii=False)
            logger.info(f"Saved team color config: {config_file}")
            return True
    except Exception as e:
        logger.error(f"Could not save team color config: {e}", exc_info=True)
        return False


def learn_team_colors_from_detections(team_colors, learned_colors_by_team, min_samples=10, max_samples=50):
    """
    Learn team colors from accumulated jersey color samples
    
    Args:
        team_colors: Current team color config dict
        learned_colors_by_team: Dict of {team_name: list of HSV colors}
        min_samples: Minimum number of samples before updating ranges
        max_samples: Maximum number of samples to use (stops learning after this)
    
    Returns:
        Updated team_colors dict, or None if not enough samples or learning complete
    """
    if not team_colors or 'team_colors' not in team_colors:
        return None
    
    updated = False
    for team_key in ["team1", "team2"]:
        if team_key not in team_colors['team_colors']:
            continue
        
        team_data = team_colors['team_colors'][team_key]
        team_name = team_data.get('name', team_key)
        
        if team_name not in learned_colors_by_team:
            continue
        
        color_samples = learned_colors_by_team[team_name]
        if len(color_samples) < min_samples:
            continue
        
        # OPTIMIZATION: Once we have enough samples, use only the most recent/diverse samples
        # This prevents over-learning and keeps the ranges stable
        if len(color_samples) > max_samples:
            # Use the most recent samples (they're likely more representative of current conditions)
            # Also mix in some older samples for diversity
            recent_samples = color_samples[-max_samples//2:]  # Most recent half
            older_samples = color_samples[::max(1, len(color_samples)//max_samples)][:max_samples//2]  # Diverse older samples
            color_samples = recent_samples + older_samples
        
        # Convert to numpy array for easier processing
        colors_array = np.array(color_samples)
        
        # Calculate HSV ranges from learned colors
        # Use percentiles to handle outliers (5th to 95th percentile)
        h_min = np.percentile(colors_array[:, 0], 5)
        h_max = np.percentile(colors_array[:, 0], 95)
        s_min = max(0, np.percentile(colors_array[:, 1], 5))
        s_max = min(255, np.percentile(colors_array[:, 1], 95))
        v_min = max(0, np.percentile(colors_array[:, 2], 5))
        v_max = min(255, np.percentile(colors_array[:, 2], 95))
        
        # Expand ranges slightly for robustness (10% margin)
        h_range = h_max - h_min
        s_range = s_max - s_min
        v_range = v_max - v_min
        
        h_lower = max(0, int(h_min - h_range * 0.1))
        h_upper = min(180, int(h_max + h_range * 0.1))
        s_lower = max(0, int(s_min - s_range * 0.1))
        s_upper = min(255, int(s_max + s_range * 0.1))
        v_lower = max(0, int(v_min - v_range * 0.1))
        v_upper = min(255, int(v_max + v_range * 0.1))
        
        # Check if we already have HSV ranges - only update if there's a significant change
        existing_ranges = team_data.get('hsv_ranges', {})
        should_update = True
        
        if existing_ranges and 'lower' in existing_ranges and 'upper' in existing_ranges:
            # Compare new ranges with existing ones
            existing_lower = existing_ranges['lower']
            existing_upper = existing_ranges['upper']
            
            # Calculate how much the ranges have changed
            h_change = abs((h_lower + h_upper) / 2 - (existing_lower[0] + existing_upper[0]) / 2)
            s_change = abs((s_lower + s_upper) / 2 - (existing_lower[1] + existing_upper[1]) / 2)
            v_change = abs((v_lower + v_upper) / 2 - (existing_lower[2] + existing_upper[2]) / 2)
            
            # Only update if there's a significant change (more than 5% in any dimension)
            # This prevents constant re-learning once we have good coverage
            threshold = 5.0  # 5% change threshold
            if h_change < threshold and s_change < threshold and v_change < threshold:
                should_update = False
                # Still print occasionally to show learning is complete
                total_samples = len(learned_colors_by_team[team_name])
                if total_samples % 20 == 0:  # Print every 20 new samples
                    print(f"   âœ“ {team_name} team colors stable ({total_samples} samples, range unchanged)")
        
        if should_update:
            # Update HSV ranges in config
            if 'hsv_ranges' not in team_data:
                team_data['hsv_ranges'] = {}
            
            team_data['hsv_ranges']['lower'] = [int(h_lower), int(s_lower), int(v_lower)]
            team_data['hsv_ranges']['upper'] = [int(h_upper), int(s_upper), int(v_upper)]
            
            updated = True
            total_samples = len(learned_colors_by_team[team_name])
            print(f"   ðŸ“š Learned {team_name} team colors from {total_samples} samples: HSV range [{h_lower}-{h_upper}, {s_lower}-{s_upper}, {v_lower}-{v_upper}]")
    
    if updated:
        return team_colors
    return None


def load_seed_config():
    """Load seed configuration from setup wizard if available"""
    config_file = "seed_config.json"
    seed_ball_positions = {}
    rejected_ids = set()
    player_roster = {}  # Video-specific player roster with active/inactive settings

    if os.path.exists(config_file):
        try:
            with open(config_file, 'r') as f:
                config = json.load(f)
                # Extract ball positions: list of (frame_num, x, y) tuples
                ball_positions = config.get("ball_positions", [])
                # Convert to dict for faster lookup: {frame_num: (x, y)}
                for pos in ball_positions:
                    if isinstance(pos, (list, tuple)) and len(pos) >= 3:
                        frame_num = int(pos[0])
                        x = float(pos[1])
                        y = float(pos[2])
                        seed_ball_positions[frame_num] = (x, y)
                if seed_ball_positions:
                    print(
                        f"âœ“ Loaded {len(seed_ball_positions)} seed ball positions from setup wizard")

                # Extract rejected IDs (coach, false positives, etc.)
                rejected_ids_list = config.get("rejected_ids", [])
                for rid in rejected_ids_list:
                    try:
                        rejected_ids.add(int(rid))
                    except (ValueError, TypeError):
                        pass
                if rejected_ids:
                    print(
                        f"âœ“ Loaded {len(rejected_ids)} rejected IDs (coach/false positives) from setup wizard")
                
                # Extract player_roster (video-specific active/inactive settings)
                player_roster = config.get("player_roster", {})
                if player_roster:
                    active_count = sum(1 for p in player_roster.values() if isinstance(p, dict) and p.get('active', True))
                    inactive_count = len(player_roster) - active_count
                    print(f"âœ“ Loaded player roster with {len(player_roster)} players ({active_count} active, {inactive_count} inactive) from seed config")
        except Exception as e:
            logger.warning(f"Could not load seed config: {e}", exc_info=True)

    return seed_ball_positions, rejected_ids, player_roster


def safe_apply_detection_mask(detections, mask, frame_num=None):
    """
    Safely apply a boolean mask to detections object.
    Returns (success: bool, filtered_detections or original_detections)

    The supervision library checks ALL arrays (xyxy, tracker_id, confidence, etc.)
    so we must validate ALL of them, not just xyxy.
    """
    if detections is None or mask is None:
        return False, detections

    try:
        # Check if detections is empty
        if len(detections) == 0:
            return False, detections

        # CRITICAL: Check ALL arrays in detections object - supervision library validates all of them
        # We need to check not just shape[0], but also verify the arrays actually exist and have data
        # NOTE: tracker_id can be None initially (before tracking), so we only
        # check non-None arrays
        array_sizes = {}
        arrays_to_check = ['xyxy', 'tracker_id', 'confidence', 'class_id']

        for attr_name in arrays_to_check:
            try:
                if hasattr(detections, attr_name):
                    attr_value = getattr(detections, attr_name)
                    if attr_value is not None:
                        if isinstance(attr_value, np.ndarray):
                            # Check shape exists and has at least 1 dimension
                            if len(attr_value.shape) > 0:
                                array_size = attr_value.shape[0]
                                # Only include arrays that have data (size > 0)
                                # Empty arrays (size 0) are treated as None -
                                # they shouldn't cause validation failures
                                if array_size > 0:
                                    # Also verify we can actually access the
                                    # first element (proves array has data)
                                    try:
                                        # Try to access first element
                                        _ = attr_value[0]
                                        array_sizes[attr_name] = array_size
                                    except (IndexError, ValueError):
                                        # Array shape says it has data, but accessing fails - treat as empty
                                        # Don't add to array_sizes - it's
                                        # effectively empty
                                        pass
                                # If array_size == 0, don't add it - it's empty and shouldn't be validated
                        # Not an array, skip
            except (AttributeError, IndexError, TypeError):
                continue

        # CRITICAL: If xyxy is missing or empty, cannot apply mask
        if 'xyxy' not in array_sizes or array_sizes['xyxy'] == 0:
            if frame_num is not None:
                print(
                    f"âš  Cannot apply mask at frame {frame_num}: xyxy array is empty (detections reports {len(detections)}, arrays checked: {list(array_sizes.keys())})")
            return False, detections

        # CRITICAL: ALL non-None, non-empty arrays must have the same size as xyxy
        # Empty arrays (size 0) are ignored - they're treated as None
        xyxy_size = array_sizes['xyxy']
        for attr_name, attr_size in array_sizes.items():
            if attr_size != xyxy_size:
                if frame_num is not None:
                    print(
                        f"âš  Cannot apply mask at frame {frame_num}: array size mismatch - {attr_name}={attr_size}, xyxy={xyxy_size}, detections reports {len(detections)}")
                return False, detections

        # FINAL CHECK: Verify xyxy array is actually accessible right before applying mask
        # Also check that no other arrays are empty (size 0) - supervision
        # library can't handle this
        try:
            if not hasattr(detections, 'xyxy') or detections.xyxy is None:
                if frame_num is not None:
                    print(
                        f"âš  Cannot apply mask at frame {frame_num}: xyxy became None")
                return False, detections
            if detections.xyxy.shape[0] == 0:
                if frame_num is not None:
                    print(
                        f"âš  Cannot apply mask at frame {frame_num}: xyxy became empty (was {xyxy_size})")
                return False, detections
            # Try to access first element to prove array has data
            _ = detections.xyxy[0]

            # CRITICAL: Check if any other arrays are empty (size 0) - supervision library can't handle this
            # Empty arrays (size 0) must be None, not empty arrays
            for attr_name in arrays_to_check:
                if attr_name == 'xyxy':
                    continue  # Already checked
                try:
                    if hasattr(detections, attr_name):
                        attr_value = getattr(detections, attr_name)
                        # If it's an array and has size 0, that's a problem -
                        # supervision library will fail
                        if isinstance(
                                attr_value, np.ndarray) and len(
                                attr_value.shape) > 0 and attr_value.shape[0] == 0:
                            if frame_num is not None:
                                print(
                                    f"âš  Cannot apply mask at frame {frame_num}: {attr_name} is empty array (size 0) but xyxy has {xyxy_size} elements - supervision library cannot handle this")
                            return False, detections
                except (AttributeError, IndexError, TypeError):
                    continue
        except (AttributeError, IndexError, ValueError) as final_check_error:
            if frame_num is not None:
                print(
                    f"âš  Cannot apply mask at frame {frame_num}: xyxy final check failed - {final_check_error}")
            return False, detections

        # Check mask size matches xyxy
        if len(mask) != xyxy_size:
            if frame_num is not None:
                print(
                    f"âš  Cannot apply mask at frame {frame_num}: mask size mismatch - mask={len(mask)}, xyxy={xyxy_size}, detections={len(detections)}")
            return False, detections

        # All checks passed - try to apply mask
        # Initialize debug_info before try block to ensure it's always defined
        debug_info = {}
        try:
            # DEBUG: Log all array sizes right before applying to catch any
            # inconsistencies
            for attr_name in arrays_to_check:
                try:
                    if hasattr(detections, attr_name):
                        attr_value = getattr(detections, attr_name)
                        if attr_value is not None and isinstance(
                                attr_value, np.ndarray):
                            debug_info[attr_name] = attr_value.shape[0] if len(
                                attr_value.shape) > 0 else 0
                        else:
                            debug_info[attr_name] = "None"
                except:
                    debug_info[attr_name] = "error"

            filtered = detections[mask]
            return True, filtered
        except (IndexError, ValueError) as e:
            # Supervision library detected size mismatch despite our checks
            # This can happen if arrays become empty between check and
            # application
            if frame_num is not None:
                print(
                    f"âš  IndexError when applying mask at frame {frame_num}: {e}")
                print(
                    f"   Debug info: {debug_info}, mask size={len(mask)}, len(detections)={len(detections)}")
            return False, detections
    except Exception as e:
        # Any other error
        if frame_num is not None:
            print(
                f"âš  Error in safe_apply_detection_mask at frame {frame_num}: {type(e).__name__}: {e}")
        return False, detections


def extract_dominant_colors_kmeans(region, k=3):
    """
    Extract dominant colors from a region using K-means clustering
    ENHANCED: Based on football_analysis repository techniques
    - Uses LAB color space for better perceptual uniformity
    - Filters background/shadow pixels more aggressively
    - Focuses on center region (jersey area)
    Returns list of (B, G, R) colors sorted by frequency
    """
    try:
        from sklearn.cluster import KMeans  # type: ignore
    except ImportError:
        # Fallback if sklearn not available
        return None

    if region.size == 0:
        return None

    h, w = region.shape[:2]
    if h < 5 or w < 5:
        return None

    # ENHANCED: Focus on center region (jersey center, less affected by
    # edges/shadows)
    center_h_start = int(h * 0.2)
    center_h_end = int(h * 0.8)
    center_w_start = int(w * 0.2)
    center_w_end = int(w * 0.8)
    center_region = region[center_h_start:center_h_end,
                           center_w_start:center_w_end]

    if center_region.size == 0:
        center_region = region

    # Reshape region to list of pixels
    pixels = center_region.reshape(-1, 3)

    # ENHANCED: Better pixel filtering (remove background, shadows, and noise)
    # Convert to LAB color space for better perceptual uniformity
    lab_pixels = cv2.cvtColor(pixels.reshape(
        1, -1, 3), cv2.COLOR_BGR2LAB).reshape(-1, 3)
    hsv_pixels = cv2.cvtColor(pixels.reshape(
        1, -1, 3), cv2.COLOR_BGR2HSV).reshape(-1, 3)

    # Filter criteria:
    # 1. Remove very dark pixels (shadows, black background)
    # 2. Remove very bright pixels (likely white/reflections, not jersey color)
    # 3. For saturation: Be more lenient - gray jerseys have low saturation (10-50)
    #    but we still want to filter out pure grays (saturation < 5)
    bright_mask = (hsv_pixels[:, 2] > 40) & (
        hsv_pixels[:, 2] < 240)  # Value between 40-240
    # CRITICAL FIX: Lower saturation threshold to handle gray jerseys (was 30, now 5)
    # Gray jerseys typically have saturation 10-50, so we need to be more lenient
    sat_mask = hsv_pixels[:, 1] > 5  # Saturation > 5 (filter out pure grays, but keep gray jerseys)
    valid_mask = bright_mask & sat_mask

    filtered_pixels = pixels[valid_mask]

    # If filtering removed too many pixels, use less aggressive filtering
    if len(filtered_pixels) < k * 10:
        # Less aggressive: just remove very dark pixels and pure grays
        filtered_pixels = pixels[(hsv_pixels[:, 2] > 30) & (hsv_pixels[:, 1] > 3)]

    if len(filtered_pixels) < k:
        # Still not enough, use all pixels
        filtered_pixels = pixels

    if len(filtered_pixels) < 10:
        # Too few pixels for clustering - use average color as fallback
        # But try less aggressive filtering first
        if len(pixels) > 0:
            # Try with just brightness filter (less aggressive) - keep low saturation for gray jerseys
            less_filtered = pixels[hsv_pixels[:, 2] > 20]  # Just remove very dark pixels
            if len(less_filtered) >= 5:
                # Use less filtered pixels
                avg_color = np.mean(less_filtered, axis=0).astype(np.uint8)
                return [tuple(avg_color)]
            else:
                # Last resort: use all pixels (even if they include some dark pixels)
                # This helps with very small jersey regions
                avg_color = np.mean(pixels, axis=0).astype(np.uint8)
                return [tuple(avg_color)]
        return None

    # ENHANCED: Use LAB color space for clustering (more perceptually uniform)
    # Convert filtered pixels to LAB for better color clustering
    filtered_lab = cv2.cvtColor(filtered_pixels.reshape(
        1, -1, 3), cv2.COLOR_BGR2LAB).reshape(-1, 3)

    # Apply K-means clustering in LAB space
    # CRITICAL FIX: For small regions or low-saturation colors, use fewer clusters
    # This prevents clustering from failing on uniform gray jerseys
    n_clusters = min(k, len(filtered_lab), max(1, len(filtered_lab) // 50))
    if n_clusters < 1:
        # Not enough pixels for clustering - return average color
        avg_color = np.mean(filtered_pixels, axis=0).astype(np.uint8)
        return [tuple(avg_color)]
    
    kmeans = KMeans(
        n_clusters=n_clusters,
        n_init="auto",
        random_state=42,
        max_iter=300)
    kmeans.fit(filtered_lab)

    # Get cluster centers in LAB, convert back to BGR
    lab_centers = kmeans.cluster_centers_.astype(np.uint8)
    lab_centers_reshaped = lab_centers.reshape(1, -1, 3)
    bgr_centers = cv2.cvtColor(
        lab_centers_reshaped, cv2.COLOR_LAB2BGR).reshape(-1, 3)

    # Get labels and count frequency
    labels = kmeans.labels_
    unique, counts = np.unique(labels, return_counts=True)

    # Sort by frequency (most common first)
    sorted_indices = np.argsort(counts)[::-1]

    # Return dominant colors sorted by frequency (as BGR tuples)
    dominant_colors = [tuple(bgr_centers[i].astype(int))
                       for i in sorted_indices]
    return dominant_colors


def extract_uniform_colors(frame, bbox):
    """
    Extract uniform colors (jersey, shorts, socks) from a player detection.
    
    Args:
        frame: Input frame (BGR)
        bbox: Bounding box [x1, y1, x2, y2]
    
    Returns:
        Dict with uniform info: {'jersey_color': 'gray', 'shorts_color': 'black', 'socks_color': 'white'}
        Returns None if extraction fails
    """
    try:
        x1, y1, x2, y2 = map(int, bbox)
        h, w = frame.shape[:2]
        
        # Ensure valid bounds
        x1 = max(0, min(x1, w - 1))
        y1 = max(0, min(y1, h - 1))
        x2 = max(x1 + 1, min(x2, w))
        y2 = max(y1 + 1, min(y2, h))
        
        if x2 <= x1 or y2 <= y1:
            return None
        
        bbox_height = y2 - y1
        bbox_width = x2 - x1
        
        # Sample jersey region (top 10-40% of bbox)
        jersey_y1 = int(y1 + bbox_height * 0.10)
        jersey_y2 = int(y1 + bbox_height * 0.40)
        jersey_x1 = int(x1 + bbox_width * 0.20)
        jersey_x2 = int(x1 + bbox_width * 0.80)
        
        # Sample shorts region (middle 40-75% of bbox)
        shorts_y1 = int(y1 + bbox_height * 0.40)
        shorts_y2 = int(y1 + bbox_height * 0.75)
        shorts_x1 = int(x1 + bbox_width * 0.20)
        shorts_x2 = int(x1 + bbox_width * 0.80)
        
        # Sample socks region (bottom 75-95% of bbox)
        socks_y1 = int(y1 + bbox_height * 0.75)
        socks_y2 = int(y1 + bbox_height * 0.95)
        socks_x1 = int(x1 + bbox_width * 0.30)
        socks_x2 = int(x1 + bbox_width * 0.70)
        
        # Ensure valid bounds
        jersey_y1 = max(0, min(jersey_y1, h - 1))
        jersey_y2 = max(jersey_y1 + 1, min(jersey_y2, h))
        jersey_x1 = max(0, min(jersey_x1, w - 1))
        jersey_x2 = max(jersey_x1 + 1, min(jersey_x2, w))
        
        shorts_y1 = max(0, min(shorts_y1, h - 1))
        shorts_y2 = max(shorts_y1 + 1, min(shorts_y2, h))
        shorts_x1 = max(0, min(shorts_x1, w - 1))
        shorts_x2 = max(shorts_x1 + 1, min(shorts_x2, w))
        
        socks_y1 = max(0, min(socks_y1, h - 1))
        socks_y2 = max(socks_y1 + 1, min(socks_y2, h))
        socks_x1 = max(0, min(socks_x1, w - 1))
        socks_x2 = max(socks_x1 + 1, min(socks_x2, w))
        
        def classify_color_name(region):
            """Classify a region's color to a color name"""
            if region.size == 0:
                return 'unknown'
            
            # Get dominant color using K-means
            dominant_colors = extract_dominant_colors_kmeans(region, k=2)
            if not dominant_colors or len(dominant_colors) == 0:
                # Fallback: use average color
                avg_bgr = np.mean(region.reshape(-1, 3), axis=0).astype(np.uint8)
                dominant_colors = [tuple(avg_bgr)]
            
            # Convert to HSV for color classification
            dominant_bgr = np.array([list(dominant_colors[0])], dtype=np.uint8).reshape(1, 1, 3)
            dominant_hsv = cv2.cvtColor(dominant_bgr, cv2.COLOR_BGR2HSV)[0, 0]
            
            h, s, v = dominant_hsv
            
            # Classify color based on HSV values
            # Gray: low saturation, medium value
            if s < 30 and 50 < v < 200:
                return 'gray'
            # Orange: hue around 10-25, medium-high saturation
            elif 10 <= h <= 25 and s > 100:
                return 'orange'
            # Blue: hue around 100-130, medium-high saturation
            elif 100 <= h <= 130 and s > 100:
                return 'blue'
            # Black: very low value
            elif v < 50:
                return 'black'
            # White: low saturation, high value
            elif s < 30 and v > 200:
                return 'white'
            # Red: hue around 0-10 or 170-180, high saturation
            elif (h <= 10 or h >= 170) and s > 100:
                return 'red'
            else:
                return 'unknown'
        
        # Extract colors from each region
        jersey_region = frame[jersey_y1:jersey_y2, jersey_x1:jersey_x2]
        shorts_region = frame[shorts_y1:shorts_y2, shorts_x1:shorts_x2]
        socks_region = frame[socks_y1:socks_y2, socks_x1:socks_x2]
        
        jersey_color = classify_color_name(jersey_region) if jersey_region.size > 0 else 'unknown'
        shorts_color = classify_color_name(shorts_region) if shorts_region.size > 0 else 'unknown'
        socks_color = classify_color_name(socks_region) if socks_region.size > 0 else 'unknown'
        
        return {
            'jersey_color': jersey_color,
            'shorts_color': shorts_color,
            'socks_color': socks_color
        }
    except Exception as e:
        # Silently fail - uniform extraction is optional
        return None


def blend_color_with_opacity(color, alpha, background_color=(0, 0, 0)):
    """
    Blend a color with background based on opacity (alpha).
    
    Args:
        color: BGR color tuple (B, G, R)
        alpha: Opacity value (0-255, 255 = fully opaque)
        background_color: Background color to blend with (default: black)
    
    Returns:
        Blended BGR color tuple
    """
    if alpha >= 255:
        return color
    if alpha <= 0:
        return background_color
    
    alpha_factor = alpha / 255.0
    blended = tuple(int(c * alpha_factor + bg * (1 - alpha_factor)) 
                    for c, bg in zip(color, background_color))
    return blended

def classify_player_team(frame, bbox, team_colors, use_clustering=False, debug=False):
    """
    Classify player team based on jersey color
    OPTIMIZED: Uses fast HSV matching by default (use_clustering=False for speed)
    K-means clustering available but slow (use_clustering=True for accuracy)
    
    Args:
        frame: Input frame (BGR)
        bbox: Bounding box [x1, y1, x2, y2]
        team_colors: Team color configuration dict
        use_clustering: Whether to use K-means clustering (default: False for speed)
        debug: If True, print diagnostic information (default: False)
    """
    if not team_colors or 'team_colors' not in team_colors:
        if debug:
            logger.warning("classify_player_team: team_colors is None or missing 'team_colors' key")
        return None

    x1, y1, x2, y2 = bbox
    # Sample from upper portion of bounding box (jersey area)
    sample_y1 = int(y1 + (y2 - y1) * 0.1)  # Top 10% to 40% of bounding box
    sample_y2 = int(y1 + (y2 - y1) * 0.4)
    sample_x1 = int(x1 + (x2 - x1) * 0.2)  # Middle 60% of width
    sample_x2 = int(x1 + (x2 - x1) * 0.8)

    # Ensure valid bounds
    h, w = frame.shape[:2]
    sample_y1 = max(0, sample_y1)
    sample_y2 = min(h, sample_y2)
    sample_x1 = max(0, sample_x1)
    sample_x2 = min(w, sample_x2)

    if sample_y2 <= sample_y1 or sample_x2 <= sample_x1:
        return None

    # Sample jersey region
    jersey_region = frame[sample_y1:sample_y2, sample_x1:sample_x2]
    if jersey_region.size == 0:
        return None

    hsv_region = cv2.cvtColor(jersey_region, cv2.COLOR_BGR2HSV)

    # ENHANCED: Try K-means clustering first for more robust color matching
    dominant_colors_hsv = None  # Initialize to None
    if use_clustering:
        dominant_colors = extract_dominant_colors_kmeans(jersey_region, k=3)
        if dominant_colors and len(dominant_colors) > 0:
            # Convert dominant colors to HSV for comparison
            dominant_colors_bgr = np.array(
                [list(c) for c in dominant_colors[:2]], dtype=np.uint8).reshape(1, -1, 3)
            dominant_colors_hsv = cv2.cvtColor(
                dominant_colors_bgr, cv2.COLOR_BGR2HSV).reshape(-1, 3)
            if debug:
                print(f"   â„¹ Clustering found {len(dominant_colors)} dominant colors (BGR): {dominant_colors[:2]}")
                print(f"   â„¹ Dominant colors (HSV): {dominant_colors_hsv[:2].tolist()}")
                # Show what HSV ranges are configured
                team1_ranges = team_colors.get('team_colors', {}).get('team1', {}).get('hsv_ranges', {})
                team2_ranges = team_colors.get('team_colors', {}).get('team2', {}).get('hsv_ranges', {})
                if team1_ranges:
                    print(f"   â„¹ Team1 HSV ranges: {team1_ranges}")
                if team2_ranges:
                    print(f"   â„¹ Team2 HSV ranges: {team2_ranges}")
        # CRITICAL FIX: Only print clustering failures occasionally to reduce spam
        # (clustering failures are common and expected, no need to log every one)
        elif debug and jersey_region.size > 1000:  # Only log for larger regions (more significant failures)
            # Only print every 100th failure to reduce spam
            import random
            if random.random() < 0.01:  # 1% chance to print (reduces spam by 99%)
                logger.debug(f"Clustering failed: no dominant colors found (jersey_region size: {jersey_region.shape})")
        
        # FALLBACK: If clustering failed, use average color
        # CRITICAL FIX: For gray/muted jerseys, use center-weighted average (jersey center is more reliable)
        if dominant_colors_hsv is None and jersey_region.size > 0:
            try:
                # Use center-weighted average for better accuracy on small regions
                h, w = jersey_region.shape[:2]
                center_h_start = max(0, int(h * 0.3))
                center_h_end = min(h, int(h * 0.7))
                center_w_start = max(0, int(w * 0.3))
                center_w_end = min(w, int(w * 0.7))
                center_region = jersey_region[center_h_start:center_h_end, center_w_start:center_w_end]
                
                if center_region.size > 0:
                    # Prefer center region (more likely to be jersey, less shadow/edge)
                    avg_bgr = np.mean(center_region.reshape(-1, 3), axis=0).astype(np.uint8)
                else:
                    # Fallback to full region
                    avg_bgr = np.mean(jersey_region.reshape(-1, 3), axis=0).astype(np.uint8)
                
                avg_bgr_array = np.array([[avg_bgr]], dtype=np.uint8)
                avg_hsv = cv2.cvtColor(avg_bgr_array, cv2.COLOR_BGR2HSV)[0, 0]
                if debug:
                    print(f"   â„¹ Using average color as fallback (BGR): {avg_bgr.tolist()}, (HSV): {avg_hsv.tolist()}")
                # Use average color as fallback for HSV matching
                dominant_colors_hsv = np.array([avg_hsv], dtype=np.float32)
            except Exception as e:
                if debug:
                    logger.warning(f"Error calculating average color: {e}", exc_info=True)

    team1_match = 0
    team2_match = 0

    # Compare dominant colors to team color ranges (only if clustering succeeded)
    if dominant_colors_hsv is not None and len(dominant_colors_hsv) > 0:
        for team_key in ["team1", "team2"]:
            if team_key not in team_colors['team_colors']:
                continue

            team_data = team_colors['team_colors'][team_key]
            if not team_data.get("hsv_ranges"):
                continue

            hsv_ranges = team_data["hsv_ranges"]
            team_match_score = 0

            # Check each dominant color against team ranges
            for dom_color_hsv in dominant_colors_hsv:
                # Handle single range or two ranges (for red colors that wrap
                # around)
                if "lower" in hsv_ranges and "upper" in hsv_ranges:
                    lower = np.array(hsv_ranges["lower"])
                    upper = np.array(hsv_ranges["upper"])
                    # Check if dominant color is within range
                    if np.all(
                            dom_color_hsv >= lower) and np.all(
                            dom_color_hsv <= upper):
                        team_match_score += 1.0
                elif "lower1" in hsv_ranges:
                    lower1 = np.array(hsv_ranges["lower1"])
                    upper1 = np.array(hsv_ranges["upper1"])
                    lower2 = np.array(hsv_ranges["lower2"])
                    upper2 = np.array(hsv_ranges["upper2"])
                    # Check both ranges (for red that wraps around)
                    in_range1 = np.all(
                        dom_color_hsv >= lower1) and np.all(
                        dom_color_hsv <= upper1)
                    in_range2 = np.all(
                        dom_color_hsv >= lower2) and np.all(
                        dom_color_hsv <= upper2)
                    if in_range1 or in_range2:
                        team_match_score += 1.0

            # Normalize score by number of dominant colors checked
            if len(dominant_colors_hsv) > 0:
                team_match_score = team_match_score / len(dominant_colors_hsv)

            if team_key == "team1":
                team1_match = team_match_score
            else:
                team2_match = team_match_score

    # Return team with highest match (threshold 0.2 for clustering)
    # CRITICAL FIX: Lower threshold for better team classification (was 0.3, now 0.15)
    # For young players with similar jerseys, we need more lenient matching
    # Also return the actual team name (e.g., "Gray" or "Blue") instead of "team1"/"team2"
    if debug:
        print(f"   ðŸ” Team classification (clustering): team1_match={team1_match:.3f}, team2_match={team2_match:.3f}")
    
    # Lower threshold to 0.15 to catch more matches
    if team1_match > 0.15 and team1_match > team2_match:
        # Get actual team name from config
        team1_name = team_colors.get('team_colors', {}).get('team1', {}).get('name', 'team1')
        if debug:
            print(f"   âœ“ Classified as {team1_name} (clustering match: {team1_match:.3f})")
        return team1_name
    elif team2_match > 0.15 and team2_match > team1_match:
        # Get actual team name from config
        team2_name = team_colors.get('team_colors', {}).get('team2', {}).get('name', 'team2')
        if debug:
            print(f"   âœ“ Classified as {team2_name} (clustering match: {team2_match:.3f})")
        return team2_name
    # CRITICAL FIX: Only print clustering failures occasionally to reduce spam
    # (clustering failures are common and expected, no need to log every one)
    elif debug:
        # Only print every 50th failure to reduce spam
        import random
        if random.random() < 0.02:  # 2% chance to print (reduces spam by 98%)
            logger.debug(f"Clustering failed: team1_match={team1_match:.3f} (threshold: 0.15), team2_match={team2_match:.3f} (threshold: 0.15)")

    # Fallback to original HSV range matching (if clustering failed or
    # disabled)
    team1_match = 0
    team2_match = 0

    for team_key in ["team1", "team2"]:
        if team_key not in team_colors['team_colors']:
            continue

        team_data = team_colors['team_colors'][team_key]
        if not team_data.get("hsv_ranges"):
            continue

        hsv_ranges = team_data["hsv_ranges"]

        # Handle single range or two ranges (for red colors that wrap around)
        if "lower" in hsv_ranges and "upper" in hsv_ranges:
            lower = np.array(hsv_ranges["lower"])
            upper = np.array(hsv_ranges["upper"])
            mask = cv2.inRange(hsv_region, lower, upper)
        elif "lower1" in hsv_ranges:
            lower1 = np.array(hsv_ranges["lower1"])
            upper1 = np.array(hsv_ranges["upper1"])
            lower2 = np.array(hsv_ranges["lower2"])
            upper2 = np.array(hsv_ranges["upper2"])
            mask1 = cv2.inRange(hsv_region, lower1, upper1)
            mask2 = cv2.inRange(hsv_region, lower2, upper2)
            mask = cv2.bitwise_or(mask1, mask2)
        else:
            continue

        match_pixels = np.sum(mask > 0)
        total_pixels = mask.size

        if total_pixels > 0:
            match_ratio = match_pixels / total_pixels
            if team_key == "team1":
                team1_match = match_ratio
            else:
                team2_match = match_ratio
            if debug:
                team_name = team_data.get('name', team_key)
                print(f"   â„¹ {team_name}: {match_pixels}/{total_pixels} pixels match ({match_ratio:.3f})")
        elif debug:
            team_name = team_data.get('name', team_key)
            print(f"   âš  {team_name}: mask.size is 0 (HSV range matching failed)")

    # Return team with highest match
    # CRITICAL FIX: Lower threshold for better team classification (was 0.10, now 0.05)
    # For young players with similar jerseys, we need more lenient matching
    # Also return the actual team name (e.g., "Gray" or "Blue") instead of "team1"/"team2"
    if debug:
        print(f"   ðŸ” Team classification (fallback HSV): team1_match={team1_match:.3f}, team2_match={team2_match:.3f}")
    
    # Lower threshold to 0.05 to catch even weak matches
    if team1_match > 0.05 and team1_match > team2_match:
        # Get actual team name from config
        team1_name = team_colors.get('team_colors', {}).get('team1', {}).get('name', 'team1')
        if debug:
            print(f"   âœ“ Classified as {team1_name} (HSV match: {team1_match:.3f})")
        return team1_name
    elif team2_match > 0.05 and team2_match > team1_match:
        # Get actual team name from config
        team2_name = team_colors.get('team_colors', {}).get('team2', {}).get('name', 'team2')
        if debug:
            print(f"   âœ“ Classified as {team2_name} (HSV match: {team2_match:.3f})")
        return team2_name
    elif debug:
        print(f"   âš  Fallback HSV failed: team1_match={team1_match:.3f} (threshold: 0.05), team2_match={team2_match:.3f} (threshold: 0.05)")
        # Check if HSV ranges are configured
        team1_has_ranges = team_colors.get('team_colors', {}).get('team1', {}).get('hsv_ranges') is not None
        team2_has_ranges = team_colors.get('team_colors', {}).get('team2', {}).get('hsv_ranges') is not None
        print(f"   â„¹ HSV ranges configured: team1={team1_has_ranges}, team2={team2_has_ranges}")

    return None


def detect_team_switch(player_name, jersey_number, current_team, frame_num, player_gallery, video_type="practice", reid_confidence=0.0):
    """
    Detects when a player appears in a different team uniform
    
    Args:
        player_name: Player's name
        jersey_number: Jersey number (if known)
        current_team: Currently detected team
        frame_num: Current frame number
        player_gallery: PlayerGallery instance
        video_type: "practice" or "game" mode
        reid_confidence: Re-ID confidence score (if available)
    
    Returns:
        dict with:
            - detected: True/False
            - from_team: Previous team
            - to_team: New team
            - frame: Frame number
            - confidence: Detection confidence
            - requires_confirmation: Whether user confirmation is needed
    """
    if not player_gallery or not player_name or not current_team:
        return {'detected': False}
    
    # Get player ID from name
    player_id = player_name.lower().replace(" ", "_")
    
    # Check if player exists in gallery
    player_profile = player_gallery.get_player(player_id)
    if not player_profile:
        return {'detected': False}
    
    # Get player's last known team
    last_team = player_profile.team
    if not last_team:
        # No previous team recorded - this is the first assignment
        return {'detected': False}
    
    # Check if team has changed
    if last_team != current_team:
        # Team has changed! This is a potential team switch
        
        # In game mode, team switches are NOT allowed - this is an error
        if video_type == "game":
            return {
                'detected': True,
                'from_team': last_team,
                'to_team': current_team,
                'frame': frame_num,
                'confidence': reid_confidence,
                'requires_confirmation': False,  # Don't allow in game mode
                'error': f'{player_name} switched teams (not allowed in game mode)'
            }
        
        # In practice mode, team switches are allowed but need confirmation
        # Only report if Re-ID confidence is high (> 0.7) to avoid false positives
        if reid_confidence > 0.7:
            return {
                'detected': True,
                'from_team': last_team,
                'to_team': current_team,
                'frame': frame_num,
                'confidence': reid_confidence,
                'requires_confirmation': True
            }
        else:
            # Low confidence - don't report the switch yet
            return {'detected': False}
    
    # No team switch detected
    return {'detected': False}


def validate_game_uniform(player_name, jersey_number, team, frame_num, player_gallery, detected_jersey=None, detected_team=None):
    """
    Strict validation for game mode - ensures consistency
    
    Args:
        player_name: Player's name
        jersey_number: Known/expected jersey number
        team: Known/expected team
        frame_num: Current frame number
        player_gallery: PlayerGallery instance
        detected_jersey: Actually detected jersey number (if OCR available)
        detected_team: Actually detected team from jersey color
    
    Returns:
        dict with:
            - valid: True/False
            - error: Error message (if invalid)
            - warning: Warning message (if suspicious but not invalid)
    """
    if not player_gallery or not player_name:
        return {'valid': True}  # Can't validate without data
    
    # Get player ID from name
    player_id = player_name.lower().replace(" ", "_")
    
    # Check if player exists in gallery
    player_profile = player_gallery.get_player(player_id)
    if not player_profile:
        return {'valid': True}  # New player - can't validate yet
    
    errors = []
    warnings = []
    
    # VALIDATION 1: Jersey number consistency
    if player_profile.jersey_number and jersey_number:
        expected_jersey = str(player_profile.jersey_number).strip()
        provided_jersey = str(jersey_number).strip()
        
        if expected_jersey != provided_jersey:
            errors.append(f"{player_name} should be #{expected_jersey}, but was assigned #{provided_jersey}")
    
    # VALIDATION 2: Detected jersey vs known jersey (if OCR available)
    if detected_jersey and jersey_number:
        expected_jersey = str(jersey_number).strip()
        detected_jersey_str = str(detected_jersey).strip()
        
        if expected_jersey != detected_jersey_str:
            errors.append(f"{player_name} assigned #{expected_jersey}, but jersey reads #{detected_jersey_str} (OCR)")
    
    # VALIDATION 3: Team consistency
    if player_profile.team and team:
        expected_team = player_profile.team
        provided_team = team
        
        if expected_team != provided_team:
            errors.append(f"{player_name} is on {expected_team}, but was detected on {provided_team}")
    
    # VALIDATION 4: Detected team vs known team
    if detected_team and team:
        if detected_team != team:
            warnings.append(f"{player_name} assigned to {team}, but jersey color suggests {detected_team}")
    
    # VALIDATION 5: Team switches not allowed in game mode
    if player_profile.team_switches and len(player_profile.team_switches) > 0:
        # Check if any switches occurred during this video
        for switch in player_profile.team_switches:
            switch_frame = switch.get('frame', 0)
            # If switch is within this analysis, flag it
            if abs(switch_frame - frame_num) < 1000:  # Within 1000 frames
                errors.append(f"{player_name} switched teams at frame {switch_frame} (not allowed in game mode)")
    
    if errors:
        return {
            'valid': False,
            'error': ' | '.join(errors),
            'warnings': warnings
        }
    elif warnings:
        return {
            'valid': True,
            'warning': ' | '.join(warnings)
        }
    else:
        return {'valid': True}


def get_player_color(track_id, team, viz_color_mode, team_colors):
    """Get color for player annotation based on visualization mode"""
    # Use team colors when available, otherwise use subtle single color
    if viz_color_mode == "team" and team:
        # Use team colors if available
        if team_colors and 'team_colors' in team_colors:
            # Handle both team names (e.g., "Gray", "Blue") and team keys (e.g., "team1", "team2")
            # classify_player_team returns team name (e.g., "Gray"), not "team1"/"team2"
            team_name_lower = team.lower() if team else ""
            
            # Try to find team by matching name or key
            team_found = None
            for team_key in ["team1", "team2"]:
                team_data = team_colors['team_colors'].get(team_key, {})
                team_data_name = team_data.get('name', '').lower()
                # Match by name or by key
                if team_name_lower == team_data_name or team_name_lower == team_key or team == team_key:
                    team_found = team_data
                    break
            
            # If found, use the configured tracker_color_bgr
            if team_found:
                # PRIORITY: Use configured tracker_color_bgr if available
                tracker_color = team_found.get('tracker_color_bgr', None)
                if tracker_color and isinstance(tracker_color, (list, tuple)) and len(tracker_color) >= 3:
                    return tuple(tracker_color[:3])  # Return as BGR tuple
                
                # FALLBACK: If no tracker_color_bgr configured, use intelligent defaults based on team name
                team_name = team_found.get('name', team).lower()
                if 'gray' in team_name or 'grey' in team_name:
                    return (180, 180, 180)  # Light gray in BGR (more visible than medium gray)
                elif 'blue' in team_name:
                    return (255, 100, 0)  # Cyan-blue in BGR (brighter, more distinct)
                elif 'red' in team_name:
                    return (0, 0, 255)  # Red in BGR
                elif 'green' in team_name:
                    return (0, 255, 0)  # Green in BGR
                elif 'yellow' in team_name or 'gold' in team_name:
                    return (0, 255, 255)  # Yellow in BGR
                elif 'orange' in team_name:
                    return (0, 165, 255)  # Orange in BGR
                elif 'purple' in team_name or 'violet' in team_name:
                    return (255, 0, 128)  # Purple in BGR
                elif 'white' in team_name:
                    return (255, 255, 255)  # White in BGR
                elif 'black' in team_name:
                    return (0, 0, 0)  # Black in BGR
                else:
                    # Generic fallback: Use configured color_bgr if available (for jersey color)
                    team_color = team_found.get('color_bgr', None)
                    if team_color and isinstance(team_color, (list, tuple)) and len(team_color) >= 3:
                        return tuple(team_color[:3])  # Return as BGR tuple
                    # Final fallback: medium gray
                    return (128, 128, 128)
            else:
                # Team not found in config - try direct name matching as fallback
                if 'gray' in team_name_lower or 'grey' in team_name_lower:
                    return (180, 180, 180)  # Light gray in BGR
                elif 'blue' in team_name_lower:
                    return (255, 100, 0)  # Cyan-blue in BGR
                elif 'red' in team_name_lower:
                    return (0, 0, 255)  # Red in BGR
                else:
                    # Unknown team - use orange as warning color
                    return (0, 165, 255)
        
        # Fallback to orange if team classification fails (for debugging untracked players)
        return (0, 165, 255)
    elif viz_color_mode == "gradient":
        # Disable gradient mode - it creates too many colors
        # Use subtle gray instead
        return (128, 128, 128)  # Gray (subtle, not distracting)
    else:  # single color
        # Use subtle gray instead of bright purple/magenta
        return (128, 128, 128)  # Gray (much more subtle than purple/magenta)


def check_ffmpeg_available():
    """Check if FFmpeg is available (in PATH, WinGet Links, or local directory)"""
    # Check in PATH first
    try:
        result = subprocess.run(['ffmpeg', '-version'],
                                capture_output=True,
                                timeout=5)
        if result.returncode == 0:
            return True
    except:
        pass

    # Check WinGet installation location (common for winget installs)
    winget_ffmpeg = os.path.join(os.environ.get('LOCALAPPDATA', ''),
                                 'Microsoft', 'WinGet', 'Links', 'ffmpeg.exe')
    if os.path.exists(winget_ffmpeg):
        return True

    # Check in current directory
    local_ffmpeg = os.path.join(
        os.path.dirname(
            os.path.abspath(__file__)),
        'ffmpeg.exe')
    if os.path.exists(local_ffmpeg):
        return True

    return False


def merge_audio_from_source(source_video, output_video, preserve_audio=True):
    """
    Merge audio from source video into processed video using FFmpeg

    Args:
        source_video: Original video with audio
        output_video: Processed video without audio
        preserve_audio: Whether to preserve audio (default: True)

    Returns:
        True if successful, False otherwise
    """
    if not preserve_audio:
        return True

    if not os.path.exists(source_video) or not os.path.exists(output_video):
        return False

    # Check if FFmpeg is available
    if not check_ffmpeg_available():
        print("âš  FFmpeg not found in PATH. Audio will not be preserved.")
        print("  To preserve audio, install FFmpeg:")
        print("  1. Download from: https://ffmpeg.org/download.html")
        print("  2. Add to system PATH, or")
        print("  3. Place ffmpeg.exe in the same folder as this script")
        return False

    # Create temporary output filename
    temp_output = output_video.replace('.mp4', '_with_audio_temp.mp4')
    final_output = output_video

    try:
        # Use FFmpeg to merge audio
        # -i input_video: processed video (no audio)
        # -i source_video: original video (with audio)
        # -c:v copy: copy video codec (no re-encoding)
        # -c:a aac: encode audio as AAC
        # -map 0:v:0: use video from first input (processed)
        # -map 1:a:0: use audio from second input (original)
        # -shortest: finish when shortest stream ends

        # Determine FFmpeg path
        ffmpeg_path = 'ffmpeg'

        # Check WinGet installation location first (common for winget installs)
        winget_ffmpeg = os.path.join(os.environ.get('LOCALAPPDATA', ''),
                                     'Microsoft', 'WinGet', 'Links', 'ffmpeg.exe')
        if os.path.exists(winget_ffmpeg):
            ffmpeg_path = winget_ffmpeg

        # Check in current directory
        local_ffmpeg = os.path.join(
            os.path.dirname(
                os.path.abspath(__file__)),
            'ffmpeg.exe')
        if os.path.exists(local_ffmpeg):
            ffmpeg_path = local_ffmpeg

        cmd = [
            ffmpeg_path,
            '-i', output_video,  # Processed video (no audio)
            '-i', source_video,  # Original video (with audio)
            '-c:v', 'copy',      # Copy video stream (no re-encoding = fast)
            '-c:a', 'aac',       # Encode audio as AAC
            '-map', '0:v:0',     # Use video from processed video
            '-map', '1:a:0?',    # Use audio from original (if available)
            '-shortest',         # Finish when shortest stream ends
            '-y',                # Overwrite output file
            temp_output
        ]

        # Run FFmpeg
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=300)

        if result.returncode == 0:
            # Replace original output with audio-merged version
            if os.path.exists(temp_output):
                shutil.move(temp_output, final_output)
                return True
            else:
                print(f"Error: FFmpeg output file not found: {temp_output}")
                return False
        else:
            # Check if audio doesn't exist in source
            if "No audio stream found" in result.stderr or "Stream map '1:a:0' matches no streams" in result.stderr:
                print(
                    "Note: Source video has no audio track. Video processed without audio.")
                if os.path.exists(temp_output):
                    os.remove(temp_output)
                return True  # Not an error, just no audio to merge
            else:
                logger.error(f"FFmpeg error: {result.stderr}")
                if os.path.exists(temp_output):
                    os.remove(temp_output)
                return False

    except subprocess.TimeoutExpired:
        logger.error("FFmpeg timed out while merging audio")
        if os.path.exists(temp_output):
            os.remove(temp_output)
        return False
    except Exception as e:
        logger.error(f"Error merging audio: {e}", exc_info=True)
        if os.path.exists(temp_output):
            os.remove(temp_output)
        return False


def load_field_calibration():
    """Load field calibration data if available"""
    calibration_data = None

    # Try loading from JSON first (newer format)
    if os.path.exists("field_calibration.json"):
        try:
            import json
            with open("field_calibration.json", 'r') as f:
                calibration_data = json.load(f)
        except:
            pass

    # Try loading from .npy files (older format)
    if calibration_data is None:
        if os.path.exists("calibration.npy") and os.path.exists(
                "calibration_metadata.npy"):
            try:
                points = np.load("calibration.npy", allow_pickle=True)
                metadata = np.load(
                    "calibration_metadata.npy",
                    allow_pickle=True).item()
                calibration_data = {
                    "points": points.tolist() if hasattr(points, 'tolist') else points,
                    "mode": metadata.get("mode", "4-point"),
                    "num_points": metadata.get("num_points", len(points))
                }
            except:
                pass

    # If calibration_data exists but missing field dimensions, try to load
    # from field_dimensions.npy
    if calibration_data is not None and (
            "field_length" not in calibration_data or "field_width" not in calibration_data):
        if os.path.exists("field_dimensions.npy"):
            try:
                field_dims = np.load(
                    "field_dimensions.npy",
                    allow_pickle=True).item()
                if "length" in field_dims:
                    calibration_data["field_length"] = float(
                        field_dims["length"])
                if "width" in field_dims:
                    calibration_data["field_width"] = float(
                        field_dims["width"])
            except:
                pass

    return calibration_data


def compute_homography_matrix(
        calibration_data,
        frame_width=None,
        frame_height=None):
    """
    Compute homography matrix from field calibration for perspective transformation.
    Converts pixel coordinates to real-world field coordinates (meters).

    Args:
        calibration_data: Dict with "points" key containing calibration points
        frame_width: Optional frame width for coordinate scaling
        frame_height: Optional frame height for coordinate scaling

    Returns:
        H: Homography matrix (3x3) or None if calibration invalid
        field_dims: Tuple of (field_length, field_width) in meters
    """
    if calibration_data is None or "points" not in calibration_data:
        return None, None

    try:
        points = np.array(calibration_data["points"], dtype=np.float32)
        if len(points) < 4:
            return None, None

        # Get field dimensions (default to indoor field if not specified)
        field_length = calibration_data.get("field_length", 40.0)  # meters
        field_width = calibration_data.get("field_width", 20.0)  # meters

        # Define destination points (real-world coordinates in meters)
        if len(points) == 4:
            # Standard 4-point rectangle
            dst_points = np.array([
                [0, 0],                    # Top-Left
                [field_length, 0],         # Top-Right
                [field_length, field_width],  # Bottom-Right
                [0, field_width]           # Bottom-Left
            ], dtype=np.float32)

            # Calculate homography matrix (4-point)
            H = cv2.getPerspectiveTransform(points[:4], dst_points)
        else:
            # 8-point, 12-point, or 16-point: use findHomography for better accuracy
            # Use outer 4 points for destination (corners)
            dst_corners = np.array([
                [0, 0],
                [field_length, 0],
                [field_length, field_width],
                [0, field_width]
            ], dtype=np.float32)

            # For additional points, distribute them across the field
            if len(points) >= 8:
                # Add midpoints for 8+ point calibration
                additional_dst = []
                if len(points) >= 8:
                    additional_dst.extend([
                        [field_length / 2, 0],            # Top-Mid
                        [field_length, field_width / 2],  # Right-Mid
                        [field_length / 2, field_width],  # Bottom-Mid
                        [0, field_width / 2]              # Left-Mid
                    ])

                # Combine corners with additional points
                all_dst = np.vstack(
                    [dst_corners, np.array(additional_dst, dtype=np.float32)])

                # Use RANSAC for robust homography estimation
                H, _ = cv2.findHomography(
                    points[:len(all_dst)], all_dst, cv2.RANSAC, 5.0)
            else:
                # Fallback to 4-point
                H = cv2.getPerspectiveTransform(points[:4], dst_corners)

        return H, (field_length, field_width)
    except Exception as e:
        print(f"Warning: Could not compute homography: {e}")
        return None, None


def get_field_roi_bounds(field_calibration, frame_width, frame_height, padding=50, min_coverage_ratio=0.3):
    """
    Get ROI bounds from field calibration for cropping frames before YOLO.
    This speeds up processing by only analyzing the field area.
    
    Args:
        field_calibration: Field calibration data (dict with "points" key)
        frame_width: Full frame width
        frame_height: Full frame height
        padding: Padding in pixels around field bounds (default: 50)
        min_coverage_ratio: Minimum ratio of frame that ROI must cover (default: 0.3 = 30%)
                           If ROI is smaller, return None to disable cropping
    
    Returns:
        (x1, y1, x2, y2) ROI bounds, or None if calibration unavailable or ROI too small
    """
    if field_calibration is None or "points" not in field_calibration:
        return None
    
    try:
        points = np.array(field_calibration["points"], dtype=np.float32)
        if len(points) < 4:
            return None
        
        # Get bounding box of calibration points
        x_coords = points[:, 0]
        y_coords = points[:, 1]
        
        # Calculate field bounds
        field_x1 = np.min(x_coords)
        field_y1 = np.min(y_coords)
        field_x2 = np.max(x_coords)
        field_y2 = np.max(y_coords)
        
        # Calculate field coverage
        field_width = field_x2 - field_x1
        field_height = field_y2 - field_y1
        field_coverage_w = field_width / frame_width if frame_width > 0 else 0
        field_coverage_h = field_height / frame_height if frame_height > 0 else 0
        
        # If field calibration only covers a small portion of the frame, it's likely incorrect
        # This can happen if calibration points are only in one area (e.g., lower field)
        if field_coverage_w < min_coverage_ratio or field_coverage_h < min_coverage_ratio:
            print(f"âš  Field calibration ROI too small: {field_coverage_w*100:.1f}% width, {field_coverage_h*100:.1f}% height")
            print(f"   â†’ This suggests calibration points may be incorrect or only cover part of the field")
            print(f"   â†’ Disabling ROI cropping to avoid missing players")
            return None
        
        # Apply padding
        x1 = max(0, int(field_x1 - padding))
        y1 = max(0, int(field_y1 - padding))
        x2 = min(frame_width, int(field_x2 + padding))
        y2 = min(frame_height, int(field_y2 + padding))
        
        # Ensure valid ROI
        if x2 > x1 and y2 > y1:
            return (x1, y1, x2, y2)
        return None
    except Exception as e:
        print(f"Warning: Could not compute field ROI: {e}")
        return None


def crop_frame_for_yolo(frame, roi_bounds):
    """
    Crop frame to ROI bounds for YOLO processing.
    
    Args:
        frame: Full frame
        roi_bounds: (x1, y1, x2, y2) ROI bounds
    
    Returns:
        Cropped frame, or original frame if ROI invalid
    """
    if roi_bounds is None:
        return frame
    
    x1, y1, x2, y2 = roi_bounds
    return frame[y1:y2, x1:x2].copy()


def translate_detections_from_roi(detections, roi_bounds):
    """
    Translate detection coordinates from ROI space back to full frame space.
    
    Args:
        detections: sv.Detections object with coordinates in ROI space
        roi_bounds: (x1, y1, x2, y2) ROI bounds used for cropping
    
    Returns:
        Detections with coordinates translated to full frame space
    """
    if roi_bounds is None:
        return detections
    
    try:
        x1, y1, x2, y2 = roi_bounds
        
        # Validate ROI bounds
        if x1 < 0 or y1 < 0 or x2 <= x1 or y2 <= y1:
            print(f"âš  Warning: Invalid ROI bounds: {roi_bounds} - skipping translation")
            return detections
        
        # Translate xyxy coordinates
        if len(detections.xyxy) > 0:
            # Store original coordinates for debugging
            original_xyxy = detections.xyxy.copy()
            
            # Translate coordinates
            detections.xyxy[:, [0, 2]] += x1  # x coordinates
            detections.xyxy[:, [1, 3]] += y1  # y coordinates
        
        # Validate translated coordinates are within reasonable bounds
            # Check for excessive translation (might indicate wrong ROI bounds)
        if len(detections.xyxy) > 0:
            # Check if any coordinates are negative (shouldn't happen with proper ROI)
                negative_coords = np.any(detections.xyxy < -10)  # Allow small negative due to padding
                
                # Check if translation caused excessive movement (drift detection)
                max_y_translation = np.max(detections.xyxy[:, [1, 3]]) - np.max(original_xyxy[:, [1, 3]])
                
                if negative_coords:
                    print(f"âš  Warning: Some detection coordinates are negative after ROI translation")
                    print(f"   â†’ ROI bounds: {roi_bounds}")
                    print(f"   â†’ This may indicate ROI bounds calculation issue")
                
                # Warn if Y translation is excessive (suggests drift)
                if abs(max_y_translation) > 500:  # More than 500 pixels
                    print(f"âš  Warning: ROI translation caused large Y movement ({max_y_translation:.1f}px)")
                    print(f"   â†’ ROI bounds: {roi_bounds}, Y offset: {y1}")
                    print(f"   â†’ This may cause bboxes to drift off-screen")
    except Exception as e:
        print(f"âš  Error translating detections from ROI: {e}")
        import traceback
        traceback.print_exc()
    
    return detections


def estimate_pixels_per_meter(field_calibration, frame_width, frame_height):
    """
    Estimate pixels per meter from field calibration.
    Used for velocity constraint validation.
    
    Args:
        field_calibration: Field calibration data
        frame_width: Frame width in pixels
        frame_height: Frame height in pixels
    
    Returns:
        Pixels per meter (float), or None if calibration unavailable
    """
    if field_calibration is None:
        return None
    
    try:
        # Get field dimensions in meters
        field_length = field_calibration.get("field_length", 40.0)  # meters
        field_width = field_calibration.get("field_width", 20.0)  # meters
        
        # Get calibration points
        if "points" not in field_calibration:
            return None
        
        points = np.array(field_calibration["points"], dtype=np.float32)
        if len(points) < 4:
            return None
        
        # Calculate field size in pixels from calibration points
        x_coords = points[:, 0]
        y_coords = points[:, 1]
        field_width_px = np.max(x_coords) - np.min(x_coords)
        field_height_px = np.max(y_coords) - np.min(y_coords)
        
        # Average pixels per meter from both dimensions
        px_per_meter_width = field_width_px / field_width if field_width > 0 else None
        px_per_meter_height = field_height_px / field_length if field_length > 0 else None
        
        if px_per_meter_width and px_per_meter_height:
            return (px_per_meter_width + px_per_meter_height) / 2.0
        elif px_per_meter_width:
            return px_per_meter_width
        elif px_per_meter_height:
            return px_per_meter_height
        else:
            return None
    except Exception as e:
        return None


def validate_track_velocity(track_id, xyxy, prev_xyxy, prev_frame, current_frame, fps, 
                           max_velocity_mps=10.0, pixels_per_meter=None, enable_velocity_constraints=True):
    """
    Validate and correct track velocity if it exceeds physical limits.
    Prevents impossible jumps/teleporting.
    
    Args:
        track_id: Track ID
        xyxy: Current bounding box [x1, y1, x2, y2]
        prev_xyxy: Previous bounding box [x1, y1, x2, y2]
        prev_frame: Previous frame number
        current_frame: Current frame number
        fps: Frame rate
        max_velocity_mps: Maximum allowed velocity in m/s (default: 10.0 for soccer players)
        pixels_per_meter: Pixels per meter conversion (from field calibration)
        enable_velocity_constraints: Whether to enable velocity constraints
    
    Returns:
        (corrected_xyxy, velocity_exceeded): Tuple of corrected bbox and boolean flag
    """
    if not enable_velocity_constraints:
        return xyxy, False
    
    if prev_xyxy is None or prev_frame is None:
        return xyxy, False
    
    frame_diff = current_frame - prev_frame
    if frame_diff <= 0:
        return xyxy, False
    
    # Calculate current position (center of bbox)
    curr_center_x = (xyxy[0] + xyxy[2]) / 2.0
    curr_center_y = (xyxy[1] + xyxy[3]) / 2.0
    prev_center_x = (prev_xyxy[0] + prev_xyxy[2]) / 2.0
    prev_center_y = (prev_xyxy[1] + prev_xyxy[3]) / 2.0
    
    # Calculate velocity in pixels per frame
    velocity_x = (curr_center_x - prev_center_x) / frame_diff
    velocity_y = (curr_center_y - prev_center_y) / frame_diff
    velocity_pixels_per_frame = np.sqrt(velocity_x**2 + velocity_y**2)
    
    # Convert to pixels per second
    velocity_pixels_per_second = velocity_pixels_per_frame * fps
    
    # Calculate max velocity in pixels
    if pixels_per_meter is not None:
        max_velocity_pixels = max_velocity_mps * pixels_per_meter
    else:
        # Fallback: estimate based on frame size (rough approximation)
        # Assume field is ~50% of frame width for indoor, ~80% for outdoor
        # Use conservative estimate: 5% of frame width per second
        frame_size = max(xyxy[2] - xyxy[0], xyxy[3] - xyxy[1])  # Use bbox size as reference
        max_velocity_pixels = frame_size * 0.15  # 15% of bbox size per second (conservative)
    
    # Check if velocity exceeds limit
    if velocity_pixels_per_second > max_velocity_pixels:
        # Velocity too high - interpolate position based on previous velocity
        # Use previous velocity if available, otherwise use zero velocity (stay in place)
        # Calculate expected position based on previous movement
        expected_center_x = prev_center_x + velocity_x * frame_diff
        expected_center_y = prev_center_y + velocity_y * frame_diff
        
        # But limit the movement to max velocity
        distance = np.sqrt((curr_center_x - prev_center_x)**2 + (curr_center_y - prev_center_y)**2)
        max_distance = max_velocity_pixels / fps * frame_diff  # Max distance in pixels
        
        if distance > max_distance:
            # Scale down the movement to fit within max velocity
            scale_factor = max_distance / distance if distance > 0 else 1.0
            corrected_center_x = prev_center_x + (curr_center_x - prev_center_x) * scale_factor
            corrected_center_y = prev_center_y + (curr_center_y - prev_center_y) * scale_factor
            
            # Reconstruct bbox with corrected center (maintain bbox size)
            bbox_width = xyxy[2] - xyxy[0]
            bbox_height = xyxy[3] - xyxy[1]
            corrected_xyxy = np.array([
                corrected_center_x - bbox_width / 2,
                corrected_center_y - bbox_height / 2,
                corrected_center_x + bbox_width / 2,
                corrected_center_y + bbox_height / 2
            ], dtype=xyxy.dtype)
            
            return corrected_xyxy, True
    
    return xyxy, False


def validate_reality_checks(track_id, xyxy, prev_xyxy, prev_frame, current_frame, fps,
                            max_pixels_per_frame, max_acceleration, max_position_change,
                            min_bbox_area, max_bbox_area, min_bbox_width, min_bbox_height,
                            max_bbox_width, max_bbox_height, width, height,
                            track_previous_velocity=None, enable_checks=True):
    """
    Comprehensive reality checks for player tracking.
    Validates physical constraints to prevent impossible tracking scenarios.
    
    Args:
        track_id: Track ID
        xyxy: Current bounding box [x1, y1, x2, y2]
        prev_xyxy: Previous bounding box [x1, y1, x2, y2] or None
        prev_frame: Previous frame number or None
        current_frame: Current frame number
        fps: Frame rate
        max_pixels_per_frame: Maximum pixels per frame (velocity limit)
        max_acceleration: Maximum acceleration (px/frameÂ²)
        max_position_change: Maximum position change per frame (px)
        min_bbox_area: Minimum bbox area (pxÂ²)
        max_bbox_area: Maximum bbox area (pxÂ²)
        min_bbox_width: Minimum bbox width (px)
        min_bbox_height: Minimum bbox height (px)
        max_bbox_width: Maximum bbox width (px)
        max_bbox_height: Maximum bbox height (px)
        width: Frame width
        height: Frame height
        track_previous_velocity: Previous velocity [vx, vy] or None
        enable_checks: Whether to enable reality checks
    
    Returns:
        (is_valid, corrected_xyxy, violations): Tuple of (validity, corrected bbox, list of violations)
    """
    if not enable_checks:
        return True, xyxy, []
    
    violations = []
    corrected_xyxy = xyxy.copy() if isinstance(xyxy, np.ndarray) else np.array(xyxy)
    
    # 1. Bounding box size checks
    bbox_width = xyxy[2] - xyxy[0]
    bbox_height = xyxy[3] - xyxy[1]
    bbox_area = bbox_width * bbox_height
    
    if bbox_area < min_bbox_area:
        violations.append(f"bbox too small (area={bbox_area:.0f} < {min_bbox_area})")
        return False, xyxy, violations
    
    if bbox_area > max_bbox_area:
        violations.append(f"bbox too large (area={bbox_area:.0f} > {max_bbox_area})")
        # Scale down to max size
        scale = np.sqrt(max_bbox_area / bbox_area)
        center_x = (xyxy[0] + xyxy[2]) / 2
        center_y = (xyxy[1] + xyxy[3]) / 2
        new_width = bbox_width * scale
        new_height = bbox_height * scale
        corrected_xyxy = np.array([
            center_x - new_width / 2,
            center_y - new_height / 2,
            center_x + new_width / 2,
            center_y + new_height / 2
        ], dtype=xyxy.dtype)
    
    if bbox_width < min_bbox_width or bbox_height < min_bbox_height:
        violations.append(f"bbox dimensions too small (w={bbox_width:.0f}, h={bbox_height:.0f})")
        return False, xyxy, violations
    
    if bbox_width > max_bbox_width or bbox_height > max_bbox_height:
        violations.append(f"bbox dimensions too large (w={bbox_width:.0f}, h={bbox_height:.0f})")
        # Scale down to max dimensions
        scale_w = min(1.0, max_bbox_width / bbox_width) if bbox_width > max_bbox_width else 1.0
        scale_h = min(1.0, max_bbox_height / bbox_height) if bbox_height > max_bbox_height else 1.0
        scale = min(scale_w, scale_h)
        center_x = (xyxy[0] + xyxy[2]) / 2
        center_y = (xyxy[1] + xyxy[3]) / 2
        new_width = bbox_width * scale
        new_height = bbox_height * scale
        corrected_xyxy = np.array([
            center_x - new_width / 2,
            center_y - new_height / 2,
            center_x + new_width / 2,
            center_y + new_height / 2
        ], dtype=xyxy.dtype)
    
    # 2. Position change checks (if previous frame available)
    if prev_xyxy is not None and prev_frame is not None:
        frame_diff = current_frame - prev_frame
        if frame_diff > 0:
            curr_center_x = (xyxy[0] + xyxy[2]) / 2.0
            curr_center_y = (xyxy[1] + xyxy[3]) / 2.0
            prev_center_x = (prev_xyxy[0] + prev_xyxy[2]) / 2.0
            prev_center_y = (prev_xyxy[1] + prev_xyxy[3]) / 2.0
            
            distance = np.sqrt((curr_center_x - prev_center_x)**2 + (curr_center_y - prev_center_y)**2)
            distance_per_frame = distance / frame_diff
            
            # Check maximum pixels per frame
            if distance_per_frame > max_pixels_per_frame:
                violations.append(f"velocity too high ({distance_per_frame:.1f} px/frame > {max_pixels_per_frame})")
                # Scale down movement
                max_distance = max_pixels_per_frame * frame_diff
                scale_factor = max_distance / distance if distance > 0 else 1.0
                corrected_center_x = prev_center_x + (curr_center_x - prev_center_x) * scale_factor
                corrected_center_y = prev_center_y + (curr_center_y - prev_center_y) * scale_factor
                
                # Reconstruct bbox
                bbox_width = xyxy[2] - xyxy[0]
                bbox_height = xyxy[3] - xyxy[1]
                corrected_xyxy = np.array([
                    corrected_center_x - bbox_width / 2,
                    corrected_center_y - bbox_height / 2,
                    corrected_center_x + bbox_width / 2,
                    corrected_center_y + bbox_height / 2
                ], dtype=xyxy.dtype)
            
            # Check maximum position change (hard limit)
            if distance > max_position_change:
                violations.append(f"position change too large ({distance:.1f} px > {max_position_change})")
                # Clamp to max position change
                direction_x = (curr_center_x - prev_center_x) / distance if distance > 0 else 0
                direction_y = (curr_center_y - prev_center_y) / distance if distance > 0 else 0
                corrected_center_x = prev_center_x + direction_x * max_position_change
                corrected_center_y = prev_center_y + direction_y * max_position_change
                
                # Reconstruct bbox
                bbox_width = xyxy[2] - xyxy[0]
                bbox_height = xyxy[3] - xyxy[1]
                corrected_xyxy = np.array([
                    corrected_center_x - bbox_width / 2,
                    corrected_center_y - bbox_height / 2,
                    corrected_center_x + bbox_width / 2,
                    corrected_center_y + bbox_height / 2
                ], dtype=xyxy.dtype)
            
            # Check acceleration (if previous velocity available)
            if track_previous_velocity is not None and frame_diff == 1:
                prev_vx, prev_vy = track_previous_velocity
                curr_vx = (curr_center_x - prev_center_x) / frame_diff
                curr_vy = (curr_center_y - prev_center_y) / frame_diff
                
                accel_x = curr_vx - prev_vx
                accel_y = curr_vy - prev_vy
                acceleration = np.sqrt(accel_x**2 + accel_y**2)
                
                if acceleration > max_acceleration:
                    violations.append(f"acceleration too high ({acceleration:.2f} px/frameÂ² > {max_acceleration})")
                    # Limit acceleration
                    scale = max_acceleration / acceleration if acceleration > 0 else 1.0
                    limited_vx = prev_vx + accel_x * scale
                    limited_vy = prev_vy + accel_y * scale
                    
                    # Calculate corrected position
                    corrected_center_x = prev_center_x + limited_vx * frame_diff
                    corrected_center_y = prev_center_y + limited_vy * frame_diff
                    
                    # Reconstruct bbox
                    bbox_width = xyxy[2] - xyxy[0]
                    bbox_height = xyxy[3] - xyxy[1]
                    corrected_xyxy = np.array([
                        corrected_center_x - bbox_width / 2,
                        corrected_center_y - bbox_height / 2,
                        corrected_center_x + bbox_width / 2,
                        corrected_center_y + bbox_height / 2
                    ], dtype=xyxy.dtype)
    
    # 3. Field boundary checks (optional - can be enabled if field calibration exists)
    # Note: Field boundaries are typically for ball tracking, not players
    # Players can be near sidelines, so this is disabled by default
    
    is_valid = len(violations) == 0 or all("too large" in v or "too high" in v for v in violations)  # Valid if only warnings (correctable)
    return is_valid, corrected_xyxy, violations


def extract_player_name(name_str):
    """
    Extract the actual player name from various formats:
    - 'Cameron Hill' -> 'Cameron Hill'
    - "['Cameron Hill', 'Blue', '']" -> 'Cameron Hill'
    - "['Cameron Hill', '', '']" -> 'Cameron Hill'
    - "['Cameron Hill']" -> 'Cameron Hill'
    """
    if not name_str:
        return ""
    
    # If it's a string representation of a list, extract the first element
    if name_str.startswith('[') and name_str.endswith(']'):
        try:
            # Try to parse as a list literal
            import ast
            parsed = ast.literal_eval(name_str)
            if isinstance(parsed, list) and len(parsed) > 0:
                return str(parsed[0]).strip()
        except (ValueError, SyntaxError):
            # If parsing fails, try string extraction
            if "'," in name_str or '",' in name_str:
                # Extract first quoted string
                import re
                match = re.search(r"['\"]([^'\"]+)['\"]", name_str)
                if match:
                    return match.group(1).strip()
    
    # Otherwise, return as-is (already a plain name)
    return str(name_str).strip()


def should_merge_tracks(track1_id, track2_id, track_state, player_names, detections, 
                       player_to_track_global=None, max_distance_pixels=100, check_distance=True,
                       track_anchor_protection=None, current_frame=0):
    """
    Determine if two tracks should be merged (same player on multiple tracks).
    
    Args:
        track1_id: First track ID
        track2_id: Second track ID
        track_state: Dictionary of track states {track_id: {'xyxy': ..., 'frame': ...}}
        player_names: Dictionary of track_id -> player_name
        detections: Current detections object
        player_to_track_global: Global player->track mapping (for conflict detection)
        max_distance_pixels: Maximum distance for tracks to be considered mergeable (default: 100, more lenient)
        check_distance: Whether to check distance (False = merge based on name conflict only)
        track_anchor_protection: Dictionary of track_id -> (player_name, protection_start, protection_end)
        current_frame: Current frame number (for checking protection windows)
    
    Returns:
        True if tracks should be merged, False otherwise
    """
    # ðŸ›¡ï¸ CRITICAL: Check if tracks have different protected players - NEVER merge them
    # ENHANCED: Block merge if EITHER track is anchor-protected for a different player
    # This prevents Anay's protected track from being merged into Cameron's protected track
    if track_anchor_protection is not None:
        track1_protected = track1_id in track_anchor_protection
        track2_protected = track2_id in track_anchor_protection
        
        # Get player names for comparison
        track1_str = str(int(track1_id))
        track2_str = str(int(track2_id))
        name1 = player_names.get(track1_str, "")
        name2 = player_names.get(track2_str, "")
        
        if track1_protected:
            prot1_name, prot1_start, prot1_end = track_anchor_protection[track1_id]
            prot1_active = prot1_start <= current_frame <= prot1_end
            
            if prot1_active:
                # Extract actual player names for comparison (handle list format)
                prot1_name_clean = extract_player_name(prot1_name)
                name2_clean = extract_player_name(name2)
                
                # Track 1 is protected - check if track 2 has a different player name
                if name2_clean and name2_clean != prot1_name_clean:
                    # Track 2 has a different player - NEVER MERGE
                    print(f"  ðŸ›¡ï¸ MERGE BLOCKED: Track #{track1_id} is anchor-protected for '{prot1_name_clean}' (f{current_frame}), Track #{track2_id} has '{name2_clean}' - different players, blocking merge")
                    return False  # Block merge - protected player vs different player
                
                # If track 2 is also protected, check if same player
                if track2_protected:
                    prot2_name, prot2_start, prot2_end = track_anchor_protection[track2_id]
                    prot2_active = prot2_start <= current_frame <= prot2_end
                    
                    if prot2_active:
                        prot2_name_clean = extract_player_name(prot2_name)
                        if prot1_name_clean != prot2_name_clean:
                            # Both protected, different players - NEVER MERGE
                            print(f"  ðŸ›¡ï¸ MERGE BLOCKED: Track #{track1_id} protected for '{prot1_name_clean}', Track #{track2_id} protected for '{prot2_name_clean}' - different players, blocking merge")
                            return False  # Block merge - different protected players
        
        if track2_protected:
            prot2_name, prot2_start, prot2_end = track_anchor_protection[track2_id]
            prot2_active = prot2_start <= current_frame <= prot2_end
            
            if prot2_active:
                # Extract actual player names for comparison (handle list format)
                prot2_name_clean = extract_player_name(prot2_name)
                name1_clean = extract_player_name(name1)
                
                # Track 2 is protected - check if track 1 has a different player name
                if name1_clean and name1_clean != prot2_name_clean:
                    # Track 1 has a different player - NEVER MERGE
                    print(f"  ðŸ›¡ï¸ MERGE BLOCKED: Track #{track2_id} is anchor-protected for '{prot2_name_clean}' (f{current_frame}), Track #{track1_id} has '{name1_clean}' - different players, blocking merge")
                    return False  # Block merge - protected player vs different player
    
    # Check if both tracks have player names (not generic "Player X" or "#ID")
    track1_str = str(int(track1_id))
    track2_str = str(int(track2_id))
    
    name1 = player_names.get(track1_str, "")
    name2 = player_names.get(track2_str, "")
    
    # Extract actual player names (handle list format)
    name1_clean = extract_player_name(name1)
    name2_clean = extract_player_name(name2)
    
    # Must have same real player name (not generic)
    if not name1_clean or not name2_clean:
        return False
    if name1_clean.startswith("#") or name2_clean.startswith("#"):
        return False
    if name1_clean.startswith("Player ") or name2_clean.startswith("Player "):
        return False
    if name1_clean != name2_clean:
        return False  # Different players
    
    # ENHANCED: Check if this is a known conflict (player assigned to multiple tracks)
    # If player_to_track_global shows this player is on a different track, merge immediately
    # Use cleaned names for lookup (check both original and cleaned formats)
    if player_to_track_global:
        # Check both original name1 and cleaned name1_clean
        for lookup_name in [name1, name1_clean]:
            if lookup_name and lookup_name in player_to_track_global:
                assigned_track = player_to_track_global[lookup_name]
                # If player is assigned to one of these tracks, and we're checking the other, merge them
                if assigned_track == track1_id and track2_id != track1_id:
                    # Player is assigned to track1, but also appears on track2 - merge track2 into track1
                    return True
                elif assigned_track == track2_id and track1_id != track2_id:
                    # Player is assigned to track2, but also appears on track1 - merge track1 into track2
                    return True
    
    # If not a known conflict, check distance (if enabled)
    if not check_distance:
        # If distance check disabled, only merge if it's a known conflict
        return False
    
    # Check if tracks are close in current frame OR in recent history
    # Get current positions from detections
    track1_pos = None
    track2_pos = None
    
    if detections is not None and detections.tracker_id is not None:
        for i, tid in enumerate(detections.tracker_id):
            if tid == track1_id:
                track1_pos = detections.xyxy[i]
            elif tid == track2_id:
                track2_pos = detections.xyxy[i]
    
    # If not in current detections, check track_state
    if track1_pos is None and track1_id in track_state:
        track1_pos = track_state[track1_id].get('xyxy', None)
    if track2_pos is None and track2_id in track_state:
        track2_pos = track_state[track2_id].get('xyxy', None)
    
    # If we have position data for both, check distance
    if track1_pos is not None and track2_pos is not None:
        # Calculate distance between track centers
        center1_x = (track1_pos[0] + track1_pos[2]) / 2.0
        center1_y = (track1_pos[1] + track1_pos[3]) / 2.0
        center2_x = (track2_pos[0] + track2_pos[2]) / 2.0
        center2_y = (track2_pos[1] + track2_pos[3]) / 2.0
        
        distance = np.sqrt((center1_x - center2_x)**2 + (center1_y - center2_y)**2)
        
        # Tracks must be close (same player shouldn't be far apart)
        # ENHANCED: More lenient distance (100 pixels instead of 50)
        if distance > max_distance_pixels:
            return False
    
    # Additional check: tracks should have overlapping time periods
    # (both should be active recently, but more lenient)
    if track1_id in track_state and track2_id in track_state:
        frame1 = track_state[track1_id].get('frame', 0)
        frame2 = track_state[track2_id].get('frame', 0)
        frame_diff = abs(frame1 - frame2)
        # ENHANCED: More lenient time check (200 frames instead of 100)
        # At 116.9fps, 200 frames = ~1.7 seconds
        if frame_diff > 200:
            return False
    
    return True


def merge_tracks(keep_track_id, merge_track_id, player_names, track_state, 
                 player_to_track_global, jersey_to_track_global, track_to_team_global,
                 player_to_team_global, track_name_confidence, detections, 
                 track_anchor_protection=None, track_anchor_assigned=None,
                 track_merge_cooldown=None, current_frame=0, merge_cooldown_frames=10):
    """
    Merge merge_track_id into keep_track_id.
    Transfers all assignments and removes duplicate track.
    
    Args:
        keep_track_id: Track ID to keep (longer/stronger track)
        merge_track_id: Track ID to merge (will be removed)
        player_names: Dictionary of track_id -> player_name
        track_state: Dictionary of track states
        player_to_track_global: Global player->track mapping
        jersey_to_track_global: Global jersey->track mapping
        track_to_team_global: Global track->team mapping
        player_to_team_global: Global player->team mapping
        track_name_confidence: Track confidence scores
        detections: Current detections (will update track IDs)
    
    Returns:
        Number of mappings updated
    """
    keep_str = str(int(keep_track_id))
    merge_str = str(int(merge_track_id))
    
    player_name = player_names.get(keep_str, "")
    if not player_name:
        return 0
    
    updates = 0
    
    # Update detections: change merge_track_id to keep_track_id
    if detections is not None and detections.tracker_id is not None:
        for i, tid in enumerate(detections.tracker_id):
            if tid == merge_track_id:
                detections.tracker_id[i] = keep_track_id
                updates += 1
    
    # Transfer player name assignment (already same, but ensure consistency)
    merged_track_name = None
    if merge_str in player_names:
        # Remove duplicate assignment - CRITICAL: Clear merged track's name to prevent stale identities
        merged_track_name = player_names[merge_str]
        del player_names[merge_str]
        updates += 1
    
    # ENHANCEMENT: Save jersey and team from merged track before clearing mappings
    # This ensures we can transfer them to the kept track
    jersey_to_reassign = None
    team_to_reassign = None
    for jersey, tid in list(jersey_to_track_global.items()):
        if tid == merge_track_id:
            jersey_to_reassign = jersey
            break
    
    if merge_track_id in track_to_team_global:
        team_to_reassign = track_to_team_global[merge_track_id]
    
    # Update global player->track mapping
    if player_name in player_to_track_global:
        if player_to_track_global[player_name] == merge_track_id:
            player_to_track_global[player_name] = keep_track_id
            updates += 1
    
    # Update jersey mapping if merge track had a jersey
    # (Reassign jersey from merge track to keep track)
    if jersey_to_reassign:
        jersey_to_track_global[jersey_to_reassign] = keep_track_id
        updates += 1
    
    # ENHANCEMENT: Clear all remaining mappings for merged track to eliminate stale identities
    # Clear any remaining jersey mappings (should already be reassigned above)
    for jersey, tid in list(jersey_to_track_global.items()):
        if tid == merge_track_id:
            del jersey_to_track_global[jersey]
            updates += 1
    
    # Clear team mapping for merged track (will be reassigned below if needed)
    if merge_track_id in track_to_team_global:
        del track_to_team_global[merge_track_id]
        updates += 1
    
    # Clear player_to_team mapping if it was for merged track
    if merged_track_name and merged_track_name in player_to_team_global:
        old_team, old_tid = player_to_team_global[merged_track_name]
        if old_tid == merge_track_id:
            # Only delete if player is now on keep_track_id (will be updated below)
            # Don't delete if player_name matches (same player, different track)
            if merged_track_name != player_name:
                del player_to_team_global[merged_track_name]
                updates += 1
    
    # Update team mappings (use saved team_to_reassign if available)
    if team_to_reassign:
        track_to_team_global[keep_track_id] = team_to_reassign
        updates += 1
    
    if player_name in player_to_team_global:
        old_team, old_tid = player_to_team_global[player_name]
        if old_tid == merge_track_id:
            player_to_team_global[player_name] = (old_team, keep_track_id)
            updates += 1
    
    # Transfer confidence (keep higher confidence)
    if merge_track_id in track_name_confidence and keep_track_id in track_name_confidence:
        merge_conf = track_name_confidence[merge_track_id]
        keep_conf = track_name_confidence[keep_track_id]
        # Keep the higher confidence
        if merge_conf[1] > keep_conf[1]:  # merge has higher similarity
            track_name_confidence[keep_track_id] = merge_conf
    elif merge_track_id in track_name_confidence:
        track_name_confidence[keep_track_id] = track_name_confidence[merge_track_id]
        del track_name_confidence[merge_track_id]
        updates += 1
    
    # Remove merge track from track_state
    if merge_track_id in track_state:
        del track_state[merge_track_id]
        updates += 1
    
    # ðŸ›¡ï¸ TRANSFER ANCHOR PROTECTION: If merged track had anchor protection, transfer to kept track
    # CRITICAL: Never merge tracks with different protected players - this prevents Anay from taking over Carson's track
    if track_anchor_protection is not None:
        if merge_track_id in track_anchor_protection:
            merge_prot_name, merge_prot_start, merge_prot_end = track_anchor_protection[merge_track_id]
            
            if keep_track_id in track_anchor_protection:
                # Both have protection - check if same player
                keep_prot_name, keep_prot_start, keep_prot_end = track_anchor_protection[keep_track_id]
                if merge_prot_name == keep_prot_name:  # Same player - merge windows
                    merged_start = min(keep_prot_start, merge_prot_start)
                    merged_end = max(keep_prot_end, merge_prot_end)
                    track_anchor_protection[keep_track_id] = (keep_prot_name, merged_start, merged_end)
                    updates += 1
                else:
                    # ðŸ›¡ï¸ DIFFERENT PLAYERS: This merge should NOT happen if both tracks are protected
                    # Keep the protection on keep_track (it's the primary), but log a warning
                    # The merge itself should have been prevented by should_merge_tracks, but if it got here, keep the primary
                    # Don't transfer merge_track's protection - it's for a different player
                    # This prevents Anay's protection from being transferred to Carson's track
                    pass  # Keep keep_track's protection, ignore merge_track's protection
            else:
                # Transfer protection from merged track to kept track (only if keep_track has no protection)
                track_anchor_protection[keep_track_id] = (merge_prot_name, merge_prot_start, merge_prot_end)
                updates += 1
            
            # Remove protection from merged track
            del track_anchor_protection[merge_track_id]
    
    # ðŸ›¡ï¸ TRANSFER PERMANENT ANCHOR ASSIGNMENT: If merged track was permanently anchor-assigned, transfer to kept track
    # CRITICAL FIX: Always preserve anchor-protected player's identity, not the earlier anchor
    if track_anchor_assigned is not None:
        if merge_track_id in track_anchor_assigned:
            merge_owner_name, merge_anchor_frame = track_anchor_assigned[merge_track_id]
            # CRITICAL: If keep track is also anchor-assigned, ensure they're the same player
            if keep_track_id in track_anchor_assigned:
                keep_owner_name, keep_anchor_frame = track_anchor_assigned[keep_track_id]
                if merge_owner_name != keep_owner_name:
                    # ðŸš¨ CRITICAL: Different players - this should NOT happen if merge prevention is working
                    # Check which track has anchor protection - preserve the PROTECTED player's identity
                    merge_has_protection = (track_anchor_protection is not None and 
                                          merge_track_id in track_anchor_protection and
                                          track_anchor_protection[merge_track_id][0] == merge_owner_name)
                    keep_has_protection = (track_anchor_protection is not None and 
                                         keep_track_id in track_anchor_protection and
                                         track_anchor_protection[keep_track_id][0] == keep_owner_name)
                    
                    if merge_has_protection and not keep_has_protection:
                        # Merge track is protected, keep track is not - preserve merge track's identity
                        track_anchor_assigned[keep_track_id] = (merge_owner_name, merge_anchor_frame)
                        print(f"  ðŸš¨ MERGE CONFLICT RESOLVED: Track #{keep_track_id} had '{keep_owner_name}', merged Track #{merge_track_id} had protected '{merge_owner_name}' - preserving PROTECTED player identity")
                    elif keep_has_protection and not merge_has_protection:
                        # Keep track is protected, merge track is not - preserve keep track's identity
                        print(f"  ðŸš¨ MERGE CONFLICT RESOLVED: Track #{keep_track_id} has protected '{keep_owner_name}', merged Track #{merge_track_id} had '{merge_owner_name}' - preserving PROTECTED player identity (keeping '{keep_owner_name}')")
                        # Don't change - keep track's protected identity
                    elif merge_has_protection and keep_has_protection:
                        # Both protected - this should have been blocked by should_merge_tracks
                        # Preserve the one with earlier anchor (more reliable)
                        if merge_anchor_frame < keep_anchor_frame:
                            track_anchor_assigned[keep_track_id] = (merge_owner_name, merge_anchor_frame)
                            print(f"  âš  MERGE CONFLICT: Both tracks protected - Track #{keep_track_id}='{keep_owner_name}', Track #{merge_track_id}='{merge_owner_name}' - keeping earlier anchor ({merge_anchor_frame})")
                        else:
                            print(f"  âš  MERGE CONFLICT: Both tracks protected - Track #{keep_track_id}='{keep_owner_name}', Track #{merge_track_id}='{merge_owner_name}' - keeping existing anchor ({keep_anchor_frame})")
                    else:
                        # Neither protected - keep earlier anchor (original behavior)
                        if merge_anchor_frame < keep_anchor_frame:
                            track_anchor_assigned[keep_track_id] = (merge_owner_name, merge_anchor_frame)
                            print(f"  âš  MERGE CONFLICT: Track #{keep_track_id} had '{keep_owner_name}', merged Track #{merge_track_id} had '{merge_owner_name}' - keeping earlier anchor ({merge_anchor_frame})")
                        # Otherwise keep existing assignment
            else:
                # Transfer permanent assignment from merge to keep
                track_anchor_assigned[keep_track_id] = (merge_owner_name, merge_anchor_frame)
                print(f"  ðŸ›¡ï¸ MERGE: Transferred permanent anchor assignment '{merge_owner_name}' from Track #{merge_track_id} â†’ Track #{keep_track_id}")
            # Remove merge track's permanent assignment
            del track_anchor_assigned[merge_track_id]
            updates += 1
    
    # ðŸ›¡ï¸ DRIFT PREVENTION: Add merge cooldown window (Grok's Fix #8)
    if track_merge_cooldown is not None:
        cooldown_end = current_frame + merge_cooldown_frames
        track_merge_cooldown[keep_track_id] = (current_frame, cooldown_end)
        if current_frame % 50 == 0:
            print(f"  â³ MERGE COOLDOWN: Track #{keep_track_id} merged at frame {current_frame}, cooldown until frame {cooldown_end}")
    
    return updates


def get_adaptive_confidence_threshold(frame, base_thresh=0.25, adaptive_confidence=True):
    """
    Calculate adaptive confidence threshold based on frame brightness/contrast.
    
    Args:
        frame: Input frame (BGR)
        base_thresh: Base confidence threshold (default: 0.25)
        adaptive_confidence: Whether to use adaptive thresholding
    
    Returns:
        Adjusted confidence threshold
    """
    if not adaptive_confidence:
        return base_thresh
    
    try:
        # Convert to grayscale for brightness analysis
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        frame_brightness = np.mean(gray)
        
        # Calculate contrast (standard deviation)
        contrast = np.std(gray)
        
        # Adjust threshold based on brightness
        if frame_brightness < 50:  # Very dark scene
            adjusted_thresh = base_thresh * 0.8  # Lower threshold (0.20)
        elif frame_brightness < 100:  # Dark scene
            adjusted_thresh = base_thresh * 0.9  # Slightly lower (0.225)
        elif frame_brightness > 200:  # Very bright scene
            adjusted_thresh = base_thresh * 1.2  # Higher threshold (0.30)
        elif frame_brightness > 150:  # Bright scene
            adjusted_thresh = base_thresh * 1.1  # Slightly higher (0.275)
        else:
            adjusted_thresh = base_thresh  # Normal lighting
        
        # Adjust for low contrast (harder to detect)
        if contrast < 30:  # Low contrast scene
            adjusted_thresh *= 0.9  # Lower threshold slightly
        
        # CRITICAL: Lowered minimum threshold to detect small/distant players
        # Lowered to 0.08 to catch more small/distant detections
        # Also cap maximum at 0.35 (was 0.4) to avoid being too strict
        adjusted_thresh = max(0.08, min(0.35, adjusted_thresh))
        
        return adjusted_thresh
    except Exception as e:
        # Fallback to base threshold on error
        return base_thresh


def transform_point_to_field(point_px, H):
    """
    Transform pixel coordinates to real-world field coordinates (meters).

    Args:
        point_px: (x, y) tuple in pixel coordinates
        H: Homography matrix (3x3)

    Returns:
        (x_m, y_m) tuple in meters, or None if transformation fails
    """
    if H is None:
        return None

    try:
        point_3d = np.array([[[point_px[0], point_px[1]]]], dtype=np.float32)
        transformed = cv2.perspectiveTransform(point_3d, H)[0][0]
        return tuple(transformed)
    except:
        return None


def transform_field_to_point(point_m, H_inv):
    """
    Transform real-world field coordinates (meters) to pixel coordinates.
    Used to project ball positions onto the field surface.

    Args:
        point_m: (x_m, y_m) tuple in meters
        H_inv: Inverse homography matrix (3x3)

    Returns:
        (x, y) tuple in pixel coordinates, or None if transformation fails
    """
    if H_inv is None:
        return None

    try:
        point_3d = np.array([[[point_m[0], point_m[1]]]], dtype=np.float32)
        transformed = cv2.perspectiveTransform(point_3d, H_inv)[0][0]
        return (int(transformed[0]), int(transformed[1]))
    except:
        return None


def calculate_trajectory_angle(p1_m, p2_m):
    """
    Calculate trajectory angle in degrees from two real-world points.

    Args:
        p1_m: (x1, y1) in meters (earlier point)
        p2_m: (x2, y2) in meters (later point)

    Returns:
        angle_degrees: Angle in degrees (0Â° = right, 90Â° = down, -90Â° = up, 180Â°/-180Â° = left)
    """
    if p1_m is None or p2_m is None:
        return None

    dx = p2_m[0] - p1_m[0]  # Change in x (length direction)
    dy = p2_m[1] - p1_m[1]  # Change in y (width direction)

    # Calculate angle in radians, then convert to degrees
    angle_rad = np.arctan2(dy, dx)
    angle_deg = np.degrees(angle_rad)

    return angle_deg


def calculate_trajectory_speed(p1_m, p2_m, time_seconds):
    """
    Calculate speed in m/s from two real-world points and time difference.

    Args:
        p1_m: (x1, y1) in meters (earlier point)
        p2_m: (x2, y2) in meters (later point)
        time_seconds: Time difference in seconds

    Returns:
        speed_mps: Speed in meters per second, or None if invalid
    """
    if p1_m is None or p2_m is None or time_seconds <= 0:
        return None

    dx = p2_m[0] - p1_m[0]
    dy = p2_m[1] - p1_m[1]
    distance_m = np.sqrt(dx * dx + dy * dy)

    speed_mps = distance_m / time_seconds
    return speed_mps


def is_point_in_field(
        point,
        calibration_data,
        strict_mode=True,
        margin_pixels=10):
    """Check if a point is inside the calibrated field area using point-in-polygon test

    Args:
        point: (x, y) tuple
        calibration_data: Field calibration dict with "points" key
        strict_mode: If True, add margin to exclude points near boundaries (default: True)
        margin_pixels: Number of pixels to shrink the field boundary (default: 10)
    """
    if calibration_data is None or "points" not in calibration_data:
        return True  # If no calibration, allow all points

    try:
        points = np.array(calibration_data["points"], dtype=np.float32)
        if len(points) < 4:
            return True  # Invalid calibration, allow all

        x, y = point

        # If strict_mode, shrink the field boundary by margin_pixels to exclude points near edges
        # This helps filter out balls that are technically in the field but on
        # the sideline
        if strict_mode and margin_pixels > 0:
            # Calculate center of field polygon
            center_x = np.mean(points[:, 0])
            center_y = np.mean(points[:, 1])

            # Shrink polygon by moving each point toward the center
            shrunk_points = points.copy()
            for i in range(len(points)):
                # Calculate direction from center to point
                dx = points[i, 0] - center_x
                dy = points[i, 1] - center_y
                # Calculate distance from center
                dist = np.sqrt(dx * dx + dy * dy)
                if dist > 0:
                    # Move point toward center by margin_pixels
                    shrink_factor = max(0, (dist - margin_pixels) / dist)
                    shrunk_points[i, 0] = center_x + dx * shrink_factor
                    shrunk_points[i, 1] = center_y + dy * shrink_factor

            points = shrunk_points

        # For 4-point calibration: check if point is inside the quadrilateral
        if len(points) == 4:
            # Use ray casting algorithm (more robust than cross product for convex polygons)
            # Cast a ray from point to right and count intersections with
            # polygon edges
            p1, p2, p3, p4 = points

            # Create polygon edges (closed loop)
            edges = [
                (p1, p2),
                (p2, p3),
                (p3, p4),
                (p4, p1)
            ]

            # Count intersections with horizontal ray going right from point
            intersections = 0
            for (px1, py1), (px2, py2) in edges:
                # Check if ray intersects this edge
                if ((py1 > y) != (py2 > y)):  # Edge crosses horizontal line at y
                    # Calculate x-coordinate of intersection
                    if py2 != py1:  # Avoid division by zero
                        x_intersect = px1 + (y - py1) * \
                            (px2 - px1) / (py2 - py1)
                        if x_intersect > x:  # Intersection is to the right of point
                            intersections += 1

            # Point is inside if odd number of intersections
            return (intersections % 2) == 1

        # For 8-point, 12-point, or 16-point calibration: use the outer 4 points (corners)
        # The additional points are used for better homography calculation, but for boundary checking
        # we use the outer quadrilateral defined by the 4 corners
        elif len(points) >= 8:
            # Use first 4 points (outer rectangle/corners)
            outer_points = points[:4]
            return is_point_in_field(point, {"points": outer_points.tolist()})

        return True  # Unknown format, allow all
    except Exception as e:
        # If check fails, allow the point (fail open to avoid false negatives)
        print(f"Warning: Field boundary check failed: {e}")
        return True


def track_ball_in_frame(
        frame,
        pts,
        buffer=64,
        min_radius=5,
        max_radius=50,
        edge_margin=50,
        show_trail=True,
        ball_velocity=None,
        fps=60,
        field_calibration=None,
        frame_num=0,
        seed_ball_positions=None,
        ball_history=None,
        ball_last_seen_frame=-1,
        homography_matrix=None,
        homography_inv=None,
        trail_length=20):
    """
    Track ball in a single frame using improved color-based detection.
    Filters out false positives by checking size, shape, and circularity.
    Supports customizable ball colors via config file. Excludes detections near walls/edges.
    Uses field calibration if available to exclude balls outside the playable field area.
    Uses seed ball positions from setup wizard to improve accuracy.
    Can use ball_history for out-of-bounds recovery (10+ second gaps).

    Args:
        fps: Frame rate of video (default: 60). Used to adjust motion thresholds for high fps.
        field_calibration: Field calibration data (dict with "points" key) or None
        frame_num: Current frame number (for seed position lookup)
        seed_ball_positions: Dict of {frame_num: (x, y)} from setup wizard seed config
        ball_history: Dict of {frame_num: (x, y)} for long-term ball tracking (out-of-bounds recovery)
        ball_last_seen_frame: Last frame where ball was detected (for recovery)
    """
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    height, width = frame.shape[:2]

    # Load field calibration if not provided
    if field_calibration is None:
        field_calibration = load_field_calibration()

    # Try to load custom ball color config
    config = load_ball_color_config()

    if config and "hsv_ranges" in config:
        # Use custom HSV ranges from config
        hsv_ranges = config["hsv_ranges"]
        mask = None

        # Color 1
        if "color1" in hsv_ranges and hsv_ranges["color1"]:
            r1 = hsv_ranges["color1"]
            lower1 = np.array(r1["lower"])
            upper1 = np.array(r1["upper"])
            mask_color1 = cv2.inRange(hsv, lower1, upper1)
            mask = mask_color1 if mask is None else cv2.bitwise_or(
                mask, mask_color1)

        # Color 2 (may have two ranges for red)
        if "color2" in hsv_ranges and hsv_ranges["color2"]:
            r2 = hsv_ranges["color2"]
            if "lower2" in r2:  # Red has two ranges
                lower2_1 = np.array(r2["lower1"])
                upper2_1 = np.array(r2["upper1"])
                lower2_2 = np.array(r2["lower2"])
                upper2_2 = np.array(r2["upper2"])
                mask_color2_1 = cv2.inRange(hsv, lower2_1, upper2_1)
                mask_color2_2 = cv2.inRange(hsv, lower2_2, upper2_2)
                mask_color2 = cv2.bitwise_or(mask_color2_1, mask_color2_2)
            else:
                lower2 = np.array(r2["lower"])
                upper2 = np.array(r2["upper"])
                mask_color2 = cv2.inRange(hsv, lower2, upper2)
            mask = mask_color2 if mask is None else cv2.bitwise_or(
                mask, mask_color2)
    else:
        # Default: red and white ball
        # HSV range for white parts of ball
        lower_white = np.array([0, 0, 200])
        upper_white = np.array([180, 30, 255])
        mask_white = cv2.inRange(hsv, lower_white, upper_white)

        # HSV range for red parts of ball
        # Red in HSV has two ranges (wraps around 0/180)
        # Range 1: 0-10 (red)
        lower_red1 = np.array([0, 50, 50])
        upper_red1 = np.array([10, 255, 255])
        mask_red1 = cv2.inRange(hsv, lower_red1, upper_red1)

        # Range 2: 170-180 (red)
        lower_red2 = np.array([170, 50, 50])
        upper_red2 = np.array([180, 255, 255])
        mask_red2 = cv2.inRange(hsv, lower_red2, upper_red2)

        # Combine red masks
        mask_red = cv2.bitwise_or(mask_red1, mask_red2)

        # Combine white and red masks
        mask = cv2.bitwise_or(mask_white, mask_red)
        kernel = np.ones((3, 3), np.uint8)
        mask = cv2.erode(mask, kernel, iterations=2)
        mask = cv2.dilate(mask, kernel, iterations=2)

        # Create edge exclusion mask (exclude near walls/edges)
        edge_mask = np.ones((height, width), dtype=np.uint8) * 255
        edge_mask[:edge_margin, :] = 0  # Top edge
    edge_mask[-edge_margin:, :] = 0  # Bottom edge
    edge_mask[:, :edge_margin] = 0  # Left edge
    edge_mask[:, -edge_margin:] = 0  # Right edge

    # Apply edge exclusion to ball mask if both masks are valid
    if mask is not None and edge_mask is not None:
        mask = cv2.bitwise_and(mask, edge_mask)
    # If mask is None, skip masking as edge exclusion is meaningless


    cnts = cv2.findContours(
        mask.copy() if mask is not None else np.zeros((height, width), dtype=np.uint8),
        cv2.RETR_EXTERNAL,
        cv2.CHAIN_APPROX_SIMPLE)
    if YOLO_AVAILABLE:
        cnts = imutils.grab_contours(cnts)
    else:
        cnts = cnts[0] if len(cnts) == 2 else cnts[1]

    center = None
    ball_detected = False

    # Filter contours by size, shape, and circularity
    ball_candidates = []
    for c in cnts:
        area = cv2.contourArea(c)
        if area < 20:  # Too small
            continue

        ((x, y), radius) = cv2.minEnclosingCircle(c)

        # Filter by size (ball should be small relative to field)
        if radius < min_radius or radius > max_radius:
            continue

        # Check circularity (ball should be roughly circular)
        perimeter = cv2.arcLength(c, True)
        if perimeter == 0:
            continue
        circularity = 4 * np.pi * area / (perimeter * perimeter)

        # More strict circularity check (was 0.6, now 0.7) - ball should be
        # very circular
        if circularity < 0.7:
            continue

        # Check aspect ratio (ball should be roughly circular, not elongated)
        (x_rect, y_rect, w_rect, h_rect) = cv2.boundingRect(c)
        aspect_ratio = float(w_rect) / h_rect if h_rect > 0 else 0

        # More strict aspect ratio (was 0.6-1.6, now 0.7-1.4) - ball should be
        # more circular
        if aspect_ratio < 0.7 or aspect_ratio > 1.4:
            continue

        # Additional check: exclude detections in upper frame (ceiling/walls)
        if y < height * 0.20:  # Top 20% of frame - likely not ball
            continue

        # Additional check: exclude detections in bottom 20% (sidelines) -
        # errant balls
        if y > height * 0.80:  # Bottom 20% of frame - likely sideline ball
            continue

        # Score candidate: prefer circular, medium-sized objects, in center field area
        # Prefer objects in middle 60% of frame (avoid sidelines and ceiling)
        # Best position is around 40-60% down the frame (center of field)
        ideal_y = height * 0.50  # Center of field
        y_distance_from_ideal = abs(y - ideal_y)
        max_y_distance = height * 0.30  # 30% tolerance
        # Up to 40% penalty for being far from center
        position_score = 1.0 - (y_distance_from_ideal / max_y_distance) * 0.4
        # Minimum 60% score even if far from ideal
        position_score = max(0.6, position_score)

        score = circularity * (1.0 - abs(1.0 - aspect_ratio)) * position_score
        ball_candidates.append((score, (x, y), radius, c))

    # Check if we have a seed ball position for this frame (from setup wizard)
    seed_position = None
    if seed_ball_positions and frame_num in seed_ball_positions:
        seed_x, seed_y = seed_ball_positions[frame_num]
        seed_position = (seed_x, seed_y)
        # Boost score for candidates near seed position
        for i, (score, (x, y), radius, c) in enumerate(ball_candidates):
            distance_to_seed = np.sqrt((x - seed_x)**2 + (y - seed_y)**2)
            # If within 50 pixels of seed position, give significant boost
            if distance_to_seed < 50:
                score_boost = 1.5  # 50% boost
            elif distance_to_seed < 100:
                score_boost = 1.2  # 20% boost
            else:
                score_boost = 1.0  # No boost
            ball_candidates[i] = (score * score_boost, (x, y), radius, c)

    # ENHANCED: If no seed for this frame but we have seed positions, use nearest seed for initialization
    # This helps establish ball tracking from the start, especially if seed is
    # at frame 0
    if seed_position is None and seed_ball_positions and len(
            seed_ball_positions) > 0:
        # Find nearest seed position (by frame number) to use as reference
        # This is especially useful for early frames (0-100) to establish
        # initial tracking
        if frame_num < 100:  # Only use this for early frames to avoid false positives later
            nearest_seed_frame = min(
                seed_ball_positions.keys(),
                key=lambda f: abs(
                    f - frame_num))
            if abs(
                    nearest_seed_frame -
                    frame_num) <= 50:  # Within 50 frames of seed
                seed_x, seed_y = seed_ball_positions[nearest_seed_frame]
                seed_position = (seed_x, seed_y)
                # Give smaller boost for nearby seed (not exact match)
                for i, (score, (x, y), radius, c) in enumerate(
                        ball_candidates):
                    distance_to_seed = np.sqrt(
                        (x - seed_x)**2 + (y - seed_y)**2)
                    if distance_to_seed < 100:
                        score_boost = 1.3  # 30% boost for nearby seed
                    elif distance_to_seed < 200:
                        score_boost = 1.1  # 10% boost
                    else:
                        score_boost = 1.0
                    ball_candidates[i] = (score * score_boost, (x, y), radius, c)

    # Select best candidate (most circular, medium-sized)
    # Iterate through candidates to find first one that passes all checks
    if len(ball_candidates) > 0:
        # Sort by score (higher is better)
        ball_candidates.sort(key=lambda x: x[0], reverse=True)

        # Check exclusion zones
        bottom_exclusion = int(height * 0.20)  # Exclude bottom 20% (sidelines)
        top_exclusion = int(height * 0.15)  # Exclude top 15% (ceiling/walls)
        side_exclusion = int(width * 0.10)  # Exclude sides 10% (walls)

        # Try each candidate until we find one that passes all checks
        valid_candidate = None
        for score, (x, y), radius, c in ball_candidates:
            # Double-check size is reasonable
            if not (min_radius <= radius <= max_radius):
                continue

            # Check if ball is in playable field area (not on sidelines or walls)
            # First check: percentage-based exclusion (fallback if no
            # calibration)
            in_bounds = (side_exclusion <= x <= width - side_exclusion and
                         top_exclusion <= y <= height - bottom_exclusion)

            # Second check: field calibration (more accurate if available)
            # Field calibration is PRIMARY - if it exists, use it strictly
            if field_calibration:
                in_field = is_point_in_field(
                    (x, y), field_calibration, strict_mode=True)
                if not in_field:
                    # Ball is outside calibrated field area - REJECT this candidate
                    # This is the most accurate check - skip to next candidate
                    continue
                # If we have calibration and ball is in field, use this candidate
                # (calibration check is sufficient, don't need percentage check)
                valid_candidate = (score, (x, y), radius, c)
                break
            else:
                # No calibration - use percentage-based exclusion as fallback
                if in_bounds:
                    valid_candidate = (score, (x, y), radius, c)
                    break

        # If we found a valid candidate, process it
        if valid_candidate is not None:
            score, (x, y), radius, c = valid_candidate

            # Proceed with motion-based filtering
            # Motion-based filtering with velocity prediction for fast balls
            # Use velocity prediction if available, otherwise use distance
            # threshold
            if len(pts) > 0:
                last_point = pts[0]  # Most recent point
                distance = np.sqrt(
                    (x - last_point[0])**2 + (y - last_point[1])**2)

                # Calculate predicted position based on velocity
                # Adjust thresholds based on frame rate:
                # At higher fps (120fps), objects move less per frame, so we can be stricter
                # At lower fps (30fps), objects move more per frame, so we need
                # to be more lenient
                fps_scale = 60.0 / max(fps, 1)  # Normalize to 60fps baseline

                predicted_distance = distance
                if ball_velocity is not None and len(pts) > 1:
                    # Predict next position based on velocity
                    vx, vy = ball_velocity
                    predicted_x = last_point[0] + vx
                    predicted_y = last_point[1] + vy
                    predicted_distance = np.sqrt(
                        (x - predicted_x)**2 + (y - predicted_y)**2)
                    # Scale thresholds based on fps: higher fps = smaller movements per frame
                    # At 120fps: fps_scale = 0.5 (stricter, allow smaller jumps)
                    # At 30fps: fps_scale = 2.0 (more lenient, allow larger
                    # jumps)
                    base_jump = min(width, height) * 0.15
                    max_jump_distance = base_jump * fps_scale  # Scale with fps
                    base_predicted = min(width, height) * 0.20
                    max_predicted_distance = base_predicted * fps_scale
                else:
                    # No velocity available - use standard distance check
                    base_jump = min(width, height) * 0.12
                    max_jump_distance = base_jump * fps_scale
                    max_predicted_distance = max_jump_distance

                # Stricter filtering: reject balls that are clearly out of bounds
                # PRIMARY CHECK: Field calibration (most accurate)
                skip_ball = False
                if field_calibration:
                    # Use field calibration as the primary filter
                    # Use stricter margin for better filtering
                    in_field = is_point_in_field(
                        (x, y), field_calibration, strict_mode=True, margin_pixels=20)
                    if not in_field:
                        skip_ball = True  # Ball is outside calibrated field - REJECT
                else:
                    # Fallback: percentage-based exclusion if no field calibration
                    # Additional check: if ball is near top of frame, likely
                    # ceiling/wall - REJECT
                    if y < height * \
                            0.10:  # Top 10% of frame - likely ceiling/wall (stricter)
                        skip_ball = True  # Skip this candidate
                    # Additional check: exclude bottom 15% (sidelines) - errant
                    # balls - REJECT (stricter)
                    # Bottom 15% of frame - likely sideline (stricter)
                    elif y > height * 0.85:
                        skip_ball = True  # Skip this candidate
                    # Also exclude left/right edges (likely walls)
                    elif x < width * 0.05 or x > width * 0.95:  # Left/right 5% - likely walls
                        skip_ball = True  # Skip this candidate

                # Motion requirement: ball must be moving (not stationary) to avoid sideline balls
                # Calculate minimum movement threshold based on FPS
                if not skip_ball and len(pts) > 2:
                    # Minimum pixels of movement per frame
                    min_movement_threshold = 3.0 * fps_scale
                    # Check if ball has been moving recently
                    recent_movement = 0
                    for i in range(min(5, len(pts) - 1)):
                        p1 = pts[i]
                        p2 = pts[i + 1]
                        recent_movement += np.sqrt((p1[0] - p2[0])
                                                   ** 2 + (p1[1] - p2[1])**2)
                    avg_movement = recent_movement / \
                        min(5, len(pts) - 1) if len(pts) > 1 else 0

                    # If ball was stationary and this detection is far, likely
                    # false positive
                    if avg_movement < min_movement_threshold and distance > max_jump_distance * 0.5:
                        skip_ball = True  # Skip stationary false positives

                # CRITICAL: Validate coordinates are within frame bounds before
                # using
                x_valid = 0 <= x < width
                y_valid = 0 <= y < height

                if not (x_valid and y_valid):
                    skip_ball = True  # Coordinates out of bounds - REJECT

                # Skip adding ball if it failed the additional checks
                if not skip_ball:
                    # Clamp coordinates to frame bounds (safety check)
                    x_clamped = max(0, min(width - 1, int(x)))
                    y_clamped = max(0, min(height - 1, int(y)))

                    # IMPROVED: Validate coordinates are not at origin (0,0) - likely a bug
                    # Skip detections at (0,0) or very close to edges (top-left
                    # corner)
                    if x_clamped >= 10 or y_clamped >= 10:
                        # CRITICAL FIX: Project ball position onto field surface using homography
                        # The detected position might be in the air, but we
                        # want to show it on the field
                        draw_x, draw_y = x_clamped, y_clamped
                        if homography_matrix is not None and homography_inv is not None:
                            # Transform detected position to field coordinates
                            field_pos = transform_point_to_field(
                                (x_clamped, y_clamped), homography_matrix)
                            if field_pos is not None:
                                # Transform back to image coordinates (this
                                # gives us ground projection)
                                ground_proj = transform_field_to_point(
                                    field_pos, homography_inv)
                                if ground_proj is not None:
                                    # CRITICAL: Validate projected position is
                                    # within field bounds
                                    projected_point = (
                                        ground_proj[0], ground_proj[1])
                                    # Check if projected position is within
                                    # field calibration bounds
                                    if field_calibration:
                                        in_field = is_point_in_field(
                                            projected_point, field_calibration, strict_mode=True, margin_pixels=30)
                                        if in_field:
                                            # Projected position is in field -
                                            # use it
                                            draw_x = max(
                                                0, min(width - 1, ground_proj[0]))
                                            draw_y = max(
                                                0, min(height - 1, ground_proj[1]))
                                        else:
                                            # Projected position is outside field - use original detection position
                                            # (This can happen if ball is detected near field edge)
                                            draw_x, draw_y = x_clamped, y_clamped
                                    else:
                                        # No field calibration - just clamp to
                                        # frame bounds
                                        draw_x = max(
                                            0, min(width - 1, ground_proj[0]))
                                        draw_y = max(
                                            0, min(height - 1, ground_proj[1]))

                    # If detection is close to predicted position (velocity-based), accept it even if far from last point
                    # This helps track fast-moving game balls while rejecting
                    # stationary sideline balls
                    if ball_velocity is not None and predicted_distance < max_predicted_distance:
                        # Valid detection near predicted position - add to
                        # trail
                        cv2.circle(
                            frame, (draw_x, draw_y), int(radius), (0, 255, 0), 2)
                        cv2.circle(
                            frame, (draw_x, draw_y), 5, (0, 255, 0), -1)
                        # Store actual detection position for tracking
                        center = (x_clamped, y_clamped)
                        pts.appendleft(center)
                        ball_detected = True
                    # If jump is reasonable (not too large), accept it
                    elif distance < max_jump_distance:
                        # Valid detection - add to trail
                        cv2.circle(
                            frame, (draw_x, draw_y), int(radius), (0, 255, 0), 2)
                        cv2.circle(
                            frame, (draw_x, draw_y), 5, (0, 255, 0), -1)
                        # Store actual detection position for tracking
                        center = (x_clamped, y_clamped)
                        pts.appendleft(center)
                        ball_detected = True
                    # If jump is too large and not near predicted position, likely a false positive
                        # (Don't add this point, but still draw trail from previous points)
            else:
                # First detection - validate bounds before adding
                x_valid = 0 <= x < width
                y_valid = 0 <= y < height

                if x_valid and y_valid:
                    # Clamp coordinates to frame bounds (safety check)
                    x_clamped = max(0, min(width - 1, int(x)))
                    y_clamped = max(0, min(height - 1, int(y)))

                    # IMPROVED: Skip detections at (0,0) or very close to edges
                    # (top-left corner)
                    if x_clamped >= 10 or y_clamped >= 10:
                        # CRITICAL FIX: Project ball position onto field
                        # surface using homography
                        draw_x, draw_y = x_clamped, y_clamped
                        if homography_matrix is not None and homography_inv is not None:
                            # Transform detected position to field coordinates
                            field_pos = transform_point_to_field(
                                (x_clamped, y_clamped), homography_matrix)
                            if field_pos is not None:
                                # Transform back to image coordinates (this
                                # gives us ground projection)
                                ground_proj = transform_field_to_point(
                                    field_pos, homography_inv)
                                if ground_proj is not None:
                                    # CRITICAL: Validate projected position is
                                    # within field bounds
                                    projected_point = (
                                        ground_proj[0], ground_proj[1])
                                    # Check if projected position is within
                                    # field calibration bounds
                                    if field_calibration:
                                        in_field = is_point_in_field(
                                            projected_point, field_calibration, strict_mode=True, margin_pixels=30)
                                        if in_field:
                                            # Projected position is in field -
                                            # use it
                                            draw_x = max(
                                                0, min(width - 1, ground_proj[0]))
                                            draw_y = max(
                                                0, min(height - 1, ground_proj[1]))
                                        else:
                                            # Projected position is outside
                                            # field - use original detection
                                            # position
                                            draw_x, draw_y = x_clamped, y_clamped
                                    else:
                                        # No field calibration - just clamp to
                                        # frame bounds
                                        draw_x = max(
                                            0, min(width - 1, ground_proj[0]))
                                        draw_y = max(
                                            0, min(height - 1, ground_proj[1]))

                        # ENHANCED: Make detected ball larger and more visible with tracking indicator
                        ball_radius = max(int(radius), 15)  # Minimum 15 pixels for better visibility
                        # Draw outer tracking indicator circle (larger, bright yellow)
                        cv2.circle(frame, (draw_x, draw_y), ball_radius + 10,
                                   (0, 255, 255), 3)  # Bright yellow outer ring for tracking indicator
                        # Draw main ball circle (larger, bright green)
                        cv2.circle(frame, (draw_x, draw_y), ball_radius,
                                   (0, 255, 0), 4)  # Bright green, thicker border
                        cv2.circle(frame, (draw_x, draw_y), ball_radius - 3, (0, 255, 0), -1)  # Filled center (bright green)
                        # Store actual detection position for tracking
                        center = (x_clamped, y_clamped)
                    pts.appendleft(center)
                    ball_detected = True

    # If no detection found, try recovery methods:
    # 1. Interpolation (for short gaps - smooth trail)
    # 2. Seed position (for initialization)
    # 3. Ball history (for out-of-bounds recovery after 10+ seconds)
    if not ball_detected:
        # ENHANCED: Try interpolation first for smooth trail during short gaps
        if len(pts) >= 2 and ball_velocity is not None:
            # We have recent ball positions and velocity - interpolate missing
            # position
            last_pos = pts[0]  # Most recent position
            prev_pos = pts[1] if len(pts) > 1 else last_pos

            # Calculate interpolated position using velocity
            # Limit interpolation to short gaps (0.5 seconds max for smooth
            # trail)
            max_interpolation_frames = int(fps * 0.5)  # 0.5 seconds max

            if ball_velocity[0] != 0 or ball_velocity[1] != 0:
                # Predict next position based on velocity
                predicted_x = last_pos[0] + ball_velocity[0]
                predicted_y = last_pos[1] + ball_velocity[1]

                # Clamp to frame bounds
                predicted_x = max(0, min(width - 1, int(predicted_x)))
                predicted_y = max(0, min(height - 1, int(predicted_y)))

                # Validate predicted position is reasonable
                distance = np.sqrt(
                    (predicted_x - last_pos[0])**2 + (predicted_y - last_pos[1])**2)
                max_jump = min(width, height) * 0.15  # Max 15% of frame size

                if distance < max_jump:
                    # Valid interpolation - add to trail for smooth
                    # visualization
                    center = (predicted_x, predicted_y)
                    pts.appendleft(center)
                    ball_detected = True  # Mark as detected for trail continuity

                    # Project interpolated position onto field surface
                    draw_x, draw_y = predicted_x, predicted_y
                    if homography_matrix is not None and homography_inv is not None:
                        field_pos = transform_point_to_field(
                            (predicted_x, predicted_y), homography_matrix)
                        if field_pos is not None:
                            ground_proj = transform_field_to_point(
                                field_pos, homography_inv)
                            if ground_proj is not None:
                                # CRITICAL: Validate projected position is
                                # within field bounds
                                projected_point = (
                                    ground_proj[0], ground_proj[1])
                                if field_calibration:
                                    in_field = is_point_in_field(
                                        projected_point, field_calibration, strict_mode=True, margin_pixels=30)
                                    if in_field:
                                        draw_x = max(
                                            0, min(width - 1, ground_proj[0]))
                                        draw_y = max(
                                            0, min(height - 1, ground_proj[1]))
                                    else:
                                        # Projected outside field - use
                                        # original predicted position
                                        draw_x, draw_y = predicted_x, predicted_y
                                else:
                                    draw_x = max(
                                        0, min(width - 1, ground_proj[0]))
                                    draw_y = max(
                                        0, min(height - 1, ground_proj[1]))

                    # Draw interpolated ball position (slightly different color
                    # to indicate interpolation)
                    # ENHANCED: Make ball larger and more visible with tracking indicator
                    ball_radius = max(int(radius), 12)  # Minimum 12 pixels for visibility
                    # Draw outer tracking indicator circle (larger, semi-transparent)
                    cv2.circle(frame, (draw_x, draw_y), ball_radius + 8,
                               (0, 255, 255), 2)  # Yellow outer ring for tracking indicator
                    # Draw main ball circle (larger)
                    cv2.circle(frame, (draw_x, draw_y), ball_radius,
                               (0, 200, 200), 3)  # Cyan-green, thicker border
                    cv2.circle(frame, (draw_x, draw_y), ball_radius - 2, (0, 200, 200), -1)  # Filled center

        # If interpolation didn't work, try seed position
        # ENHANCED: Use seed position more aggressively for initialization
        # (especially early frames)
        if not ball_detected and seed_position is not None:
            seed_x, seed_y = seed_position
            # Validate seed position is within frame bounds
            if 0 <= seed_x < width and 0 <= seed_y < height:
                # Clamp coordinates to frame bounds (safety check)
                seed_x_clamped = max(0, min(width - 1, int(seed_x)))
                seed_y_clamped = max(0, min(height - 1, int(seed_y)))

                # ENHANCED: For early frames (0-200), use seed position more aggressively
                # This establishes ball tracking from the start, creating a
                # good "origin" point
                use_seed = True
                if frame_num < 200:  # Early frames - always use seed if available
                    use_seed = True
                elif len(pts) == 0:  # No previous tracking - use seed to initialize
                    use_seed = True
                elif len(pts) > 0:  # Have previous tracking - only use seed if close to last position
                    last_pos = pts[0]
                    distance_to_last = np.sqrt(
                        (seed_x_clamped - last_pos[0])**2 + (seed_y_clamped - last_pos[1])**2)
                    max_reasonable_distance = min(
                        width, height) * 0.20  # 20% of frame size
                    if distance_to_last < max_reasonable_distance:
                        use_seed = True
                    else:
                        use_seed = False  # Seed too far from last position - likely wrong

                if use_seed:
                    # Use seed position to initialize/continue tracking
                    center = (seed_x_clamped, seed_y_clamped)
                    pts.appendleft(center)
                    ball_detected = True
                    # CRITICAL FIX: Project seed position onto field surface
                    # using homography
                    draw_x, draw_y = seed_x_clamped, seed_y_clamped
                    if homography_matrix is not None and homography_inv is not None:
                        field_pos = transform_point_to_field(
                            (seed_x_clamped, seed_y_clamped), homography_matrix)
                        if field_pos is not None:
                            ground_proj = transform_field_to_point(
                                field_pos, homography_inv)
                            if ground_proj is not None:
                                # CRITICAL: Validate projected position is
                                # within field bounds
                                projected_point = (
                                    ground_proj[0], ground_proj[1])
                                if field_calibration:
                                    in_field = is_point_in_field(
                                        projected_point, field_calibration, strict_mode=True, margin_pixels=30)
                                    if in_field:
                                        draw_x = max(
                                            0, min(width - 1, ground_proj[0]))
                                        draw_y = max(
                                            0, min(height - 1, ground_proj[1]))
                                    else:
                                        # Projected outside field - use
                                        # original seed position
                                        draw_x, draw_y = seed_x_clamped, seed_y_clamped
                                else:
                                    draw_x = max(
                                        0, min(width - 1, ground_proj[0]))
                                    draw_y = max(
                                        0, min(height - 1, ground_proj[1]))
                    # Draw seed position marker (slightly different color to
                    # indicate it's from seed)
                    # ENHANCED: Make seed ball larger and more visible
                    seed_ball_radius = 15
                    # Draw outer tracking indicator circle
                    cv2.circle(frame, (draw_x, draw_y), seed_ball_radius + 8,
                               (255, 255, 0), 2)  # Yellow outer ring for tracking indicator
                    # Draw main ball circle
                    cv2.circle(frame, (draw_x, draw_y), seed_ball_radius,
                               (255, 255, 0), 3)  # Yellow, thicker border
                    cv2.circle(frame, (draw_x, draw_y), seed_ball_radius - 2, (255, 255, 0), -1)  # Filled center
        # If no seed and ball_history available, try to recover from history
        elif not ball_detected and ball_history is not None and ball_last_seen_frame >= 0:
            frames_since_last_seen = frame_num - ball_last_seen_frame
            # If ball was seen recently (within 15 seconds), try to use last known position
            # This helps with out-of-bounds scenarios where ball disappears for
            # 10+ seconds
            if frames_since_last_seen > 0 and frames_since_last_seen < int(
                    fps * 15):
                if ball_last_seen_frame in ball_history:
                    last_pos = ball_history[ball_last_seen_frame]
                    # Use last known position as a placeholder (ball is out of bounds)
                    # Don't add to trail, but track that ball is missing
                    # This allows the system to resume tracking when ball
                    # returns
                    pass  # For now, just track that ball is missing

    # Draw trail - only draw lines between consecutive points that are close together
    # CRITICAL FIX: Respect user's trail settings - if set to 1 point/1 frame, show NO trail
    # User set trail_length to 1, so don't draw any trail at all
    if show_trail and len(pts) > 1 and trail_length > 1:
        # CRITICAL FIX: Use exact trail_length setting from GUI
        # All trail drawing code is inside this condition - if trail_length <=
        # 1, nothing is drawn
        # Use exact setting from GUI
        max_trail_points = min(trail_length, len(pts))
        trail_pts = list(pts)[:max_trail_points]  # Only use most recent points

        # Smooth trail points (simple moving average)
        smoothed_pts = []
        for i in range(len(trail_pts)):
            if trail_pts[i] is None:
                continue
            if i == 0:
                smoothed_pts.append(trail_pts[i])
            elif i == len(trail_pts) - 1:
                smoothed_pts.append(trail_pts[i])
            else:
                # Average with neighbors (smooth out jitter)
                prev_pt = trail_pts[i -
                                    1] if trail_pts[i -
                                                    1] is not None else trail_pts[i]
                next_pt = trail_pts[i +
                                    1] if (i +
                                           1 < len(trail_pts) and trail_pts[i +
                                                                            1] is not None) else trail_pts[i]
                smooth_x = int((prev_pt[0] + trail_pts[i][0] + next_pt[0]) / 3)
                smooth_y = int((prev_pt[1] + trail_pts[i][1] + next_pt[1]) / 3)
                smoothed_pts.append((smooth_x, smooth_y))

        # CRITICAL FIX: Draw every segment for very short trails (only 3-4 points)
        # Since trail is already very short, draw all segments to keep it connected
        # Draw smoothed trail
        for i in range(1, len(smoothed_pts)):
            # Draw all segments for short trails (no skipping needed - trail is
            # already minimal)
            pass  # Don't skip any segments for very short trails

            if smoothed_pts[i - 1] is not None and smoothed_pts[i] is not None:
                pt1 = smoothed_pts[i - 1]
                pt2 = smoothed_pts[i]

                # Validate both points are within frame bounds
                pt1_valid = (0 <= pt1[0] < width and 0 <= pt1[1] < height)
                pt2_valid = (0 <= pt2[0] < width and 0 <= pt2[1] < height)

                if not (pt1_valid and pt2_valid):
                    continue  # Skip invalid points

                # Clamp points to frame bounds (safety check)
                pt1_clamped = (
                    max(0, min(width - 1, int(pt1[0]))), max(0, min(height - 1, int(pt1[1]))))
                pt2_clamped = (
                    max(0, min(width - 1, int(pt2[0]))), max(0, min(height - 1, int(pt2[1]))))

                # Check distance between points
                distance = np.sqrt((pt1_clamped[0] - pt2_clamped[0])**2 +
                                   (pt1_clamped[1] - pt2_clamped[1])**2)
                # ENHANCED: Reduced to 10% (was 15%) for much tighter trails
                max_distance = min(width, height) * 0.10

                # Only draw line if points are reasonably close (prevents wild
                # lines)
                if distance < max_distance:
                    # CRITICAL FIX: Project trail points onto field surface using homography
                    # This ensures the trail appears on the field, not in the
                    # air
                    draw_pt1 = pt1_clamped
                    draw_pt2 = pt2_clamped
                    if homography_matrix is not None and homography_inv is not None:
                        # Project both points onto field surface
                        field_pt1 = transform_point_to_field(
                            pt1_clamped, homography_matrix)
                        field_pt2 = transform_point_to_field(
                            pt2_clamped, homography_matrix)
                        if field_pt1 is not None:
                            ground_proj1 = transform_field_to_point(
                                field_pt1, homography_inv)
                            if ground_proj1 is not None:
                                # Validate projected point is within field
                                # bounds
                                proj1_point = (
                                    ground_proj1[0], ground_proj1[1])
                                if field_calibration:
                                    in_field1 = is_point_in_field(
                                        proj1_point, field_calibration, strict_mode=True, margin_pixels=30)
                                    if in_field1:
                                        draw_pt1 = (max(0, min(width - 1, ground_proj1[0])),
                                                    max(0, min(height - 1, ground_proj1[1])))
                                    # If outside field, keep original
                                    # pt1_clamped
                                else:
                                    draw_pt1 = (max(0, min(width - 1, ground_proj1[0])),
                                                max(0, min(height - 1, ground_proj1[1])))
                        if field_pt2 is not None:
                            ground_proj2 = transform_field_to_point(
                                field_pt2, homography_inv)
                            if ground_proj2 is not None:
                                # Validate projected point is within field
                                # bounds
                                proj2_point = (
                                    ground_proj2[0], ground_proj2[1])
                                if field_calibration:
                                    in_field2 = is_point_in_field(
                                        proj2_point, field_calibration, strict_mode=True, margin_pixels=30)
                                    if in_field2:
                                        draw_pt2 = (max(0, min(width - 1, ground_proj2[0])),
                                                    max(0, min(height - 1, ground_proj2[1])))
                                    # If outside field, keep original
                                    # pt2_clamped
                                else:
                                    draw_pt2 = (max(0, min(width - 1, ground_proj2[0])),
                                                max(0, min(height - 1, ground_proj2[1])))

                    # ENHANCED: Make trail visible but not too distracting
                    # Use fixed thin line (2 pixels) for better visibility
                    thickness = 2  # 2 pixels for better visibility
                    # ENHANCED: Bright red trail color for visibility (BGR format)
                    # Red trail: (0, 0, 255) in BGR = bright red
                    trail_color = (0, 0, 255)  # Bright red for visibility
                    # CRITICAL FIX: Only draw if points are very close (prevents long lines across screen)
                    # This prevents trails from drawing across the entire field
                    if distance < max_distance * \
                            0.5:  # Only draw if points are very close (50% of max distance)
                        cv2.line(
                            frame,
                            draw_pt1,
                            draw_pt2,
                            trail_color,
                            thickness)

    # Calculate velocity for next frame prediction (if we have at least 2
    # points)
    velocity = None
    if len(pts) >= 2:
        # Simple velocity: difference between last two points
        p1 = pts[0]
        p2 = pts[1] if len(pts) > 1 else pts[0]
        velocity = (p1[0] - p2[0], p1[1] - p2[1])

    return frame, center, ball_detected, velocity


def calculate_possession(
        ball_center,
        player_centers,
        frame_width,
        frame_height):
    """Calculate which player is closest to the ball."""
    if ball_center is None or len(player_centers) == 0:
        return None, None

    min_distance = float('inf')
    closest_player_id = None

    for player_id, player_center in player_centers.items():
        if player_center is not None:
            distance = np.sqrt(
                (ball_center[0] - player_center[0]) ** 2 +
                (ball_center[1] - player_center[1]) ** 2
            )
            if distance < min_distance:
                min_distance = distance
                closest_player_id = player_id

    frame_diagonal = np.sqrt(frame_width ** 2 + frame_height ** 2)
    normalized_distance = min_distance / frame_diagonal if frame_diagonal > 0 else 0

    return closest_player_id, normalized_distance


def load_previous_tracking_data(input_path=None):
    """
    Load previous tracking data from CSV files to enable frame-based matching.

    Args:
        input_path: Path to video file - will search same directory for tracking CSV files

    Returns:
        Dict mapping frame_num -> list of {old_track_id, x, y, player_name}
    """
    frame_data = {}

    if not input_path:
        return frame_data

    video_dir = os.path.dirname(os.path.abspath(input_path))
    video_basename = os.path.splitext(os.path.basename(input_path))[0]

    # Look for tracking CSV files
    import glob
    csv_patterns = [
        f"{video_basename}_analyzed_tracking_data.csv",
        f"{video_basename}_tracking_data.csv",
        "*_tracking_data.csv",
        "*_analyzed_tracking_data.csv"
    ]

    # Track processed files to avoid checking the same file multiple times
    processed_files = set()

    for pattern in csv_patterns:
        csv_files = glob.glob(os.path.join(video_dir, pattern))
        for csv_file in csv_files:
            # Skip if we've already processed this file
            csv_file_abs = os.path.abspath(csv_file)
            if csv_file_abs in processed_files:
                continue
            processed_files.add(csv_file_abs)
            try:
                # Check if file is empty before reading
                if os.path.getsize(csv_file) == 0:
                    print(f"âš  Skipping empty CSV file: {os.path.basename(csv_file)}")
                    continue
                
                import pandas as pd
                # Skip comment lines (starting with '#') - these contain metadata
                df = pd.read_csv(csv_file, comment='#')
                
                # Check if DataFrame is empty
                if df.empty:
                    print(f"âš  Skipping CSV file with no data: {os.path.basename(csv_file)}")
                    continue

                # Check if CSV has required columns
                required_cols = ['frame', 'track_id']
                if not all(col in df.columns for col in required_cols):
                    continue

                # Look for position columns (x, y or center_x, center_y)
                x_col = None
                y_col = None
                if 'center_x' in df.columns and 'center_y' in df.columns:
                    x_col, y_col = 'center_x', 'center_y'
                elif 'x' in df.columns and 'y' in df.columns:
                    x_col, y_col = 'x', 'y'

                # Look for player name column
                name_col = None
                for col in ['player_name', 'name', 'player', 'tag']:
                    if col in df.columns:
                        name_col = col
                        break

                # Load frame-by-frame data
                for _, row in df.iterrows():
                    # Check for NaN in frame number
                    if pd.isna(row.get('frame')):
                        continue  # Skip rows with NaN frame numbers
                    try:
                        frame_num = int(row['frame'])
                    except (ValueError, TypeError):
                        continue  # Skip rows that can't be converted to int
                    
                    # Check for NaN in track_id
                    if pd.isna(row.get('track_id')):
                        continue  # Skip rows with NaN track IDs
                    try:
                        old_track_id = str(int(row['track_id']))
                    except (ValueError, TypeError):
                        continue  # Skip rows that can't be converted to int

                    if old_track_id is None:
                        continue

                    player_name = None
                    if name_col and pd.notna(row[name_col]):
                        # CRITICAL FIX: Handle case where row[name_col] might be a list
                        name_value = row[name_col]
                        if isinstance(name_value, list) and len(name_value) > 0:
                            player_name = str(name_value[0]).strip()
                        else:
                            player_name = str(name_value).strip()

                    x, y = None, None
                    if x_col and y_col and pd.notna(
                            row[x_col]) and pd.notna(
                            row[y_col]):
                        x = float(row[x_col])
                        y = float(row[y_col])

                    if frame_num not in frame_data:
                        frame_data[frame_num] = []

                    frame_data[frame_num].append({
                        'old_track_id': old_track_id,
                        'x': x,
                        'y': y,
                        'player_name': player_name
                    })

                print(
                    f"âœ“ Loaded previous tracking data from: {
                        os.path.basename(csv_file)} ({
                        len(frame_data)} frames)")
                break  # Only load the first matching file
            except Exception as e:
                print(
                    f"âš  Could not load tracking CSV {
                        os.path.basename(csv_file)}: {e}")
                continue

    return frame_data


def is_player_active_in_roster(player_name, video_specific_roster=None):
    """
    Check if a player is active in the roster.
    Video-specific roster (from seed config) takes precedence over global roster.
    
    Args:
        player_name: Name of the player to check
        video_specific_roster: Optional dict of video-specific roster from seed config
    
    Returns:
        True if player is active (or not in roster), False if inactive
    """
    # First check video-specific roster (from seed config) - this takes precedence
    if video_specific_roster and player_name in video_specific_roster:
        player_data = video_specific_roster[player_name]
        if isinstance(player_data, dict):
            return player_data.get('active', True)
        # If it's not a dict, assume active
        return True
    
    # Fall back to global roster
    try:
        from team_roster_manager import TeamRosterManager
        roster_manager = TeamRosterManager()
        roster = roster_manager.roster
        
        if player_name in roster:
            return roster[player_name].get('active', True)
        return True  # Default to active if player not in roster
    except Exception:
        return True  # Default to active if roster unavailable


def get_active_players_set(video_specific_roster=None, focused_players=None):
    """
    Build a set of active player names for Re-ID matching optimization.
    Combines focused players (if provided) with active players from roster.
    
    Args:
        video_specific_roster: Optional dict of video-specific roster from seed config
        focused_players: Optional list of focused player names from GUI
    
    Returns:
        Set of active player names, or None if all players should be matched
    """
    active_players = set()
    
    # If focused players are provided, use those (user explicitly selected)
    if focused_players and len(focused_players) > 0:
        active_players.update(focused_players)
    
    # Also add active players from roster (if no focused players, use all active players)
    if video_specific_roster:
        for player_name, player_data in video_specific_roster.items():
            if isinstance(player_data, dict) and player_data.get('active', True):
                active_players.add(player_name)
    
    # If no focused players but we have active players from roster, use those
    if not focused_players and active_players:
        return active_players
    
    # If focused players are provided, return intersection (only focused players that are active)
    if focused_players and len(focused_players) > 0:
        if video_specific_roster:
            # Filter focused players to only include active ones
            filtered = set()
            for player_name in focused_players:
                if is_player_active_in_roster(player_name, video_specific_roster):
                    filtered.add(player_name)
            return filtered if filtered else None
        return set(focused_players)
    
    # If we have active players from roster but no focused players, return those
    if active_players:
        return active_players
    
    # No restrictions - match against all players
    return None


def load_player_names(input_path=None, include_seed_files=True):
    """
    Load player name mappings from file.
    Also checks for PlayerTagsSeed files in the video directory.

    Args:
        input_path: Optional path to video file - will search same directory for PlayerTagsSeed files
        include_seed_files: If True, also load from PlayerTagsSeed files (default: True)
                           Set to False during periodic reloads to prevent overwriting analysis-assigned names

    Returns:
        Dict mapping track_id (as string) to player_name
    """
    player_names = {}

    # Load from player_names.json (main file)
    if os.path.exists("player_names.json"):
        try:
            with open("player_names.json", 'r') as f:
                player_names = json.load(f)
        except:
            pass

    # Also try to load from PlayerTagsSeed files in the video directory
    # CRITICAL FIX: Only load seed files if explicitly requested (not during periodic reloads)
    # This prevents seed data from overwriting correctly assigned player names
    # during analysis
    if input_path and include_seed_files:
        video_dir = os.path.dirname(os.path.abspath(input_path))
        video_basename = os.path.splitext(os.path.basename(input_path))[0]

        # Look for PlayerTagsSeed files matching the video name (STRICT: only exact matches)
        # CRITICAL FIX: Do NOT use wildcards - only load files that match THIS video
        seed_patterns = [
            f"PlayerTagsSeed-{video_basename}-Project.json",
            f"PlayerTagsSeed-{video_basename}.json",
        ]

        for pattern in seed_patterns:
            seed_file = os.path.join(video_dir, pattern)
            if os.path.exists(seed_file):
                try:
                    # Use safe JSON loading with corruption protection
                    try:
                        from json_utils import safe_json_load
                        from pathlib import Path
                        seed_data = safe_json_load(Path(seed_file), default={})
                    except ImportError:
                        # Fallback to standard JSON if json_utils not available
                        with open(seed_file, 'r', encoding='utf-8') as f:
                            seed_data = json.load(f)

                    # CRITICAL: Verify this file is for the current video (safety check)
                    file_video_path = seed_data.get("video_path", "")
                    if file_video_path:
                        # Check if this file matches the current video (exact path match)
                        file_video_normalized = os.path.normpath(os.path.abspath(file_video_path))
                        input_video_normalized = os.path.normpath(os.path.abspath(input_path))
                        if file_video_normalized != input_video_normalized:
                            logger.warning(f"Skipping {os.path.basename(seed_file)}: video path mismatch")
                            logger.debug(f"   â†’ File is for: {os.path.basename(file_video_path)}")
                            logger.debug(f"   â†’ Current video: {os.path.basename(input_path)}")
                            logger.debug(f"   â†’ This prevents wrong player assignments from different videos")
                            continue

                    # Convert PlayerTagsSeed format to player_names format
                    if "player_mappings" in seed_data:
                        for track_id_str, name_array in seed_data["player_mappings"].items(
                        ):
                            if isinstance(
                                    name_array, list) and len(name_array) > 0:
                                # First element is the player name
                                player_name_raw = name_array[0]
                                # CRITICAL FIX: Handle case where name_array[0] might be a list or other type
                                if isinstance(player_name_raw, list) and len(player_name_raw) > 0:
                                    player_name_raw = player_name_raw[0]  # Get first element if nested list
                                # Ensure it's a string
                                player_name = str(player_name_raw).strip() if player_name_raw else None
                                if player_name and player_name.strip():  # Only add non-empty names
                                    # Merge with existing names (seed file
                                    # takes precedence for new IDs)
                                    # CRITICAL FIX: Normalize player name before assignment
                                    player_name_normalized = extract_player_name(player_name)
                                    
                                    # CRITICAL FIX: Ensure existing value is string before calling strip()
                                    existing_name = player_names.get(track_id_str)
                                    if not existing_name or (isinstance(existing_name, str) and not existing_name.strip()):
                                        player_names[track_id_str] = player_name_normalized
                                    elif not isinstance(existing_name, str):
                                        # If existing is not a string (e.g., list), replace it
                                        player_names[track_id_str] = player_name_normalized

                    logger.info(f"Loaded player tags from: {os.path.basename(seed_file)}")
                    break  # Only load the first matching file
                except Exception as e:
                    logger.error(f"Could not load PlayerTagsSeed file {os.path.basename(seed_file)}: {e}", exc_info=True)
                    continue

    return player_names


def load_anchor_frames(input_path, explicit_anchor_file=None, validate_against_csv=True):
    """
    Load anchor frames from PlayerTagsSeed files.
    Anchor frames are frame-specific player tags with 1.00 confidence.
    
    Args:
        input_path: Path to video file - will search same directory for PlayerTagsSeed files
        explicit_anchor_file: Optional explicit path to PlayerTagsSeed file to load (if None, auto-selects newest)
        validate_against_csv: If True, validate anchor frames against CSV tracking data to filter out players not in video
        
    Returns:
        Tuple: (anchor_frames_dict, loaded_file_path, players_in_anchor_frames_set, players_in_current_video)
        - anchor_frames_dict: {frame_num: [{track_id, player_name, bbox, confidence: 1.00, team}]}
        - loaded_file_path: Path to the file that was loaded (or None if none loaded)
        - players_in_anchor_frames_set: Set of player names found in anchor frames (for filtering gallery matching)
        - players_in_current_video: Set of player names actually in the current video (from anchor frames + CSV)
    """
    anchor_frames = {}
    loaded_file_path = None
    players_in_anchor_frames_set = set()  # Set of player names in anchor frames
    players_in_current_video = set()  # Set of player names actually in the current video
    
    if not input_path:
        return anchor_frames, loaded_file_path, players_in_anchor_frames_set, players_in_current_video
    
    video_dir = os.path.dirname(os.path.abspath(input_path))
    video_basename = os.path.splitext(os.path.basename(input_path))[0]
    
    print(f"ðŸ” Looking for anchor frames for video: {os.path.basename(input_path)}")
    print(f"   â†’ Video directory: {video_dir}")
    print(f"   â†’ Video basename: {video_basename}")
    
    # Initialize found_files list (will be populated either from explicit file or by searching)
    found_files = []
    
    # If explicit file is provided, load it directly
    if explicit_anchor_file and os.path.exists(explicit_anchor_file):
        print(f"ðŸ” Loading explicit anchor file: {os.path.basename(explicit_anchor_file)}")
        try:
            with open(explicit_anchor_file, 'r') as f:
                seed_data = json.load(f)
            
            # CRITICAL: Validate video resolution matches (if stored in anchor frames)
            # This prevents coordinate mismatches from different video resolutions
            if "video_resolution" in seed_data:
                anchor_width = seed_data["video_resolution"].get("width")
                anchor_height = seed_data["video_resolution"].get("height")
                if anchor_width and anchor_height:
                    # Get current video resolution
                    try:
                        import cv2
                        cap = cv2.VideoCapture(input_path)
                        if cap.isOpened():
                            current_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                            current_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                            cap.release()
                            
                            if current_width != anchor_width or current_height != anchor_height:
                                print(f"  âš  WARNING: Video resolution mismatch!")
                                print(f"     â†’ Anchor frames: {anchor_width}x{anchor_height}")
                                print(f"     â†’ Current video: {current_width}x{current_height}")
                                print(f"     â†’ This will cause coordinate mismatch in metrics evaluation")
                                print(f"     â†’ Solution: Recreate anchor frames from this video")
                            else:
                                print(f"  âœ“ Video resolution matches: {current_width}x{current_height}")
                    except:
                        pass  # Skip validation if we can't read video
            
            if "anchor_frames" in seed_data:
                loaded_anchors = seed_data["anchor_frames"]
                if loaded_anchors and isinstance(loaded_anchors, dict):
                    # Process the explicit file (will continue below)
                    found_files = [(os.path.getmtime(explicit_anchor_file), explicit_anchor_file, seed_data)]
                else:
                    print(f"âš  Explicit anchor file has no valid anchor_frames data")
                    return anchor_frames, loaded_file_path, players_in_anchor_frames_set, players_in_current_video
            else:
                print(f"âš  Explicit anchor file has no 'anchor_frames' key")
                return anchor_frames, loaded_file_path
        except Exception as e:
            print(f"âš  Error loading explicit anchor file: {e}")
            return anchor_frames, loaded_file_path
    
    # Look for PlayerTagsSeed files matching the video name (STRICT: only load files for this video)
    # Priority order: Check all patterns, then prefer newest file (most recent tags)
    # NOTE: Multiple file formats exist (dash vs underscore), so check all variants
    seed_patterns = [
        f"PlayerTagsSeed_{video_basename}.json",  # Underscore format (newer, often more accurate)
        f"PlayerTagsSeed-{video_basename}.json",  # Dash format (original)
        f"PlayerTagsSeed-{video_basename}-Project.json",
        f"PlayerTagsSeed-{video_basename}_optimized.json",  # Optimized version as fallback
    ]
    
    # Also check seed_config.json in video directory, but verify it's for this video
    seed_config_file_video = os.path.join(video_dir, "seed_config.json")
    if os.path.exists(seed_config_file_video):
        try:
            with open(seed_config_file_video, 'r') as f:
                config_data = json.load(f)
            # Only use seed_config.json if it references this video (STRICT: full path match required)
            config_video_path = config_data.get("video_path", "")
            if config_video_path:
                # Normalize paths for comparison
                config_video_normalized = os.path.normpath(os.path.abspath(config_video_path))
                input_video_normalized = os.path.normpath(os.path.abspath(input_path))
                # STRICT: Require exact path match (prevents matching videos with same name in different folders)
                if config_video_normalized == input_video_normalized:
                    seed_patterns.append("seed_config.json")
        except:
            pass  # If we can't read it, skip it
    
    # ALSO check seed_config.json in project root (current working directory)
    # This is where Setup Wizard saves by default
    seed_config_file_root = "seed_config.json"
    if os.path.exists(seed_config_file_root):
        try:
            with open(seed_config_file_root, 'r') as f:
                config_data = json.load(f)
            # Only use seed_config.json if it references this video (STRICT: full path match required)
            config_video_path = config_data.get("video_path", "")
            if config_video_path:
                # Normalize paths for comparison
                config_video_normalized = os.path.normpath(os.path.abspath(config_video_path))
                input_video_normalized = os.path.normpath(os.path.abspath(input_path))
                # STRICT: Require exact path match (prevents matching videos with same name in different folders)
                if config_video_normalized == input_video_normalized:
                    # Add as absolute path so we can load it
                    seed_patterns.append(os.path.abspath(seed_config_file_root))
        except:
            pass  # If we can't read it, skip it
    
    # CRITICAL FIX: Find ALL matching files, then load the NEWEST one (most recent tags)
    # This ensures we use the most up-to-date anchor frames
    # (Skip if explicit file was provided - already set found_files above)
    if not found_files:
        for pattern in seed_patterns:
            # Handle both relative paths (in video_dir) and absolute paths (project root)
            if os.path.isabs(pattern):
                seed_file = pattern  # Already absolute
            else:
                seed_file = os.path.join(video_dir, pattern)
            
            if os.path.exists(seed_file):
                try:
                    # CRITICAL: Verify this file is for the CURRENT video using FULL PATH matching
                    # This prevents loading anchor frames from videos with the same filename in different folders
                    with open(seed_file, 'r') as f:
                        seed_data = json.load(f)
                    
                    file_video_path = seed_data.get("video_path", "")
                    if file_video_path:
                        # Normalize both paths for comparison (handles different path formats)
                        file_video_normalized = os.path.normpath(os.path.abspath(file_video_path))
                        input_video_normalized = os.path.normpath(os.path.abspath(input_path))
                        
                        # STRICT: Require exact path match (not just filename)
                        if file_video_normalized != input_video_normalized:
                            # Filenames match but paths differ - this is a different video!
                            print(f"  âš  SKIPPING {os.path.basename(seed_file)}: Video path mismatch")
                            print(f"     â†’ Anchor file is for: {file_video_path}")
                            print(f"     â†’ Current video is: {input_path}")
                            print(f"     â†’ These are DIFFERENT videos (same filename, different folder)")
                            continue  # Skip files for different videos
                        else:
                            # Paths match exactly - this is the correct video
                            print(f"  âœ“ VERIFIED {os.path.basename(seed_file)}: Video path matches exactly")
                    else:
                        # No video_path in anchor file - fallback to filename matching (less safe)
                        print(f"  âš  WARNING: {os.path.basename(seed_file)} has no 'video_path' - using filename match only")
                        print(f"     â†’ This is less safe if you have videos with same name in different folders")
                    
                    # File is valid - add to list with modification time
                    mod_time = os.path.getmtime(seed_file)
                    found_files.append((mod_time, seed_file, seed_data))
                except:
                    continue  # Skip files that can't be read
    
    # Sort by modification time (newest first) and load the most recent
    # (If explicit file was provided, found_files already has one entry)
    if found_files:
        found_files.sort(key=lambda x: x[0], reverse=True)  # Newest first
        mod_time, seed_file, seed_data = found_files[0]
        
        # DIAGNOSTIC: Show which file was selected
        file_keys = list(seed_data.keys())
        print(f"ðŸ” DEBUG: Selected {os.path.basename(seed_file)} (modified: {mod_time}, newest of {len(found_files)} matching files) with keys: {file_keys}")
        
        # CRITICAL: Validate video resolution matches (if stored in anchor frames)
        # This prevents coordinate mismatches from different video resolutions
        if "video_resolution" in seed_data:
            anchor_width = seed_data["video_resolution"].get("width")
            anchor_height = seed_data["video_resolution"].get("height")
            if anchor_width and anchor_height:
                # Get current video resolution
                try:
                    import cv2
                    cap = cv2.VideoCapture(input_path)
                    if cap.isOpened():
                        current_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                        current_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                        cap.release()
                        
                        if current_width != anchor_width or current_height != anchor_height:
                            print(f"  âš  WARNING: Video resolution mismatch!")
                            print(f"     â†’ Anchor frames: {anchor_width}x{anchor_height}")
                            print(f"     â†’ Current video: {current_width}x{current_height}")
                            print(f"     â†’ This will cause coordinate mismatch in metrics evaluation")
                            print(f"     â†’ Solution: Recreate anchor frames from this video")
                        else:
                            print(f"  âœ“ Video resolution matches: {current_width}x{current_height}")
                except:
                    pass  # Skip validation if we can't read video
        
        # Load anchor_frames from seed data
        if "anchor_frames" in seed_data:
            try:
                loaded_anchors = seed_data["anchor_frames"]
                
                # Check if anchor_frames is None or empty
                if loaded_anchors is None:
                    print(f"âš  {os.path.basename(seed_file)} has 'anchor_frames' key but value is None")
                    loaded_anchors = {}
                
                if not isinstance(loaded_anchors, dict):
                    print(f"âš  {os.path.basename(seed_file)} has 'anchor_frames' but it's not a dictionary (type: {type(loaded_anchors)})")
                    loaded_anchors = {}
                
                # Safe len() check
                num_frames = len(loaded_anchors) if loaded_anchors is not None else 0
                print(f"ðŸ” DEBUG: Found 'anchor_frames' key with {num_frames} frame entries")
                
                # Track which players are in the anchor frames for validation
                players_in_anchor_frames = {}
                player_frame_counts = {}  # player_name -> count of frames they appear in
                
                # Merge with existing (later files take precedence for same frame)
                for frame_num_str, anchors in loaded_anchors.items():
                    # Check if anchors is None or not a list
                    if anchors is None:
                        continue
                    if not isinstance(anchors, list):
                        print(f"âš  Frame {frame_num_str} has non-list anchors: {type(anchors)}")
                        continue
                    frame_num = int(frame_num_str)
                    if frame_num not in anchor_frames:
                        anchor_frames[frame_num] = []
                    # Add anchors (avoid duplicates)
                    for anchor in anchors:
                        # VALIDATION: Ensure anchor is a dictionary
                        if not isinstance(anchor, dict):
                            print(f"  âš  WARNING: Skipping invalid anchor frame at frame {frame_num}: not a dictionary (type: {type(anchor)})")
                            continue
                        
                        # VALIDATION: Ensure player_name is a string (not a list or other type)
                        player_name = anchor.get('player_name', '')
                        if isinstance(player_name, list):
                            # Try to extract the first element if it's a list
                            if len(player_name) > 0 and isinstance(player_name[0], str):
                                player_name = player_name[0]
                                anchor['player_name'] = player_name  # Fix the anchor
                                print(f"  âš  WARNING: Fixed malformed anchor frame at frame {frame_num}: player_name was a list, extracted '{player_name}'")
                            else:
                                print(f"  âš  WARNING: Skipping invalid anchor frame at frame {frame_num}: player_name is a list with no valid string: {player_name}")
                                continue
                        elif not isinstance(player_name, str):
                            print(f"  âš  WARNING: Skipping invalid anchor frame at frame {frame_num}: player_name is not a string (type: {type(player_name)}, value: {player_name})")
                            continue
                        
                        # VALIDATION: Ensure player_name is not empty
                        player_name = player_name.strip() if isinstance(player_name, str) else ''
                        if not player_name:
                            print(f"  âš  WARNING: Skipping anchor frame at frame {frame_num}: player_name is empty")
                            continue
                        
                        # Update anchor with cleaned player_name
                        anchor['player_name'] = player_name
                        
                        if anchor not in anchor_frames[frame_num]:
                            anchor_frames[frame_num].append(anchor)
                            if player_name not in players_in_anchor_frames:
                                players_in_anchor_frames[player_name] = []
                                player_frame_counts[player_name] = 0
                            players_in_anchor_frames[player_name].append((frame_num, anchor))
                            player_frame_counts[player_name] += 1
                        
                        # CRITICAL: Check if anchor has bbox - if not, warn (bbox is needed for matching when track_id changes)
                        if anchor.get('bbox') is None and anchor.get('track_id') is not None:
                            # This is OK - track_id matching will work, and we'll recover bbox from detection
                            # But log for awareness
                            if len(anchor_frames) < 10:  # Only log for first few frames to avoid spam
                                print(f"  âš  Anchor frame loaded without bbox: Frame {frame_num}, Track {anchor.get('track_id')}, Player '{player_name}' - will use track_id matching")
                        
                        # Validate confidence is 1.00 (anchor frames are ground truth)
                        anchor_conf = anchor.get('confidence', 0.0)
                        if anchor_conf != 1.00:
                            print(f"  âš  WARNING: Anchor frame has incorrect confidence: Frame {frame_num}, Track {anchor.get('track_id')}, confidence={anchor_conf} (expected 1.00)")
                            # Force to 1.00 - anchor frames must be ground truth
                            anchor['confidence'] = 1.00
                        
                # Log which players are in the anchor frames with frame counts
                if players_in_anchor_frames:
                    # VALIDATION: Clean up any remaining malformed player names in the dictionary keys
                    cleaned_players_in_anchor_frames = {}
                    cleaned_player_frame_counts = {}
                    for player_name, anchor_list in players_in_anchor_frames.items():
                        # Clean the player name
                        cleaned_name = player_name
                        if isinstance(player_name, list):
                            if len(player_name) > 0 and isinstance(player_name[0], str):
                                cleaned_name = player_name[0]
                            else:
                                print(f"  âš  WARNING: Skipping malformed player name in anchor frames: {player_name}")
                                continue
                        elif not isinstance(player_name, str):
                            cleaned_name = str(player_name).strip() if player_name else None
                            if not cleaned_name:
                                print(f"  âš  WARNING: Skipping empty player name in anchor frames")
                                continue
                        else:
                            cleaned_name = player_name.strip() if player_name else None
                            if not cleaned_name:
                                print(f"  âš  WARNING: Skipping empty player name in anchor frames")
                                continue
                        
                        # Merge counts if multiple variations of the same name exist
                        if cleaned_name in cleaned_players_in_anchor_frames:
                            cleaned_players_in_anchor_frames[cleaned_name].extend(anchor_list)
                            cleaned_player_frame_counts[cleaned_name] += player_frame_counts[player_name]
                        else:
                            cleaned_players_in_anchor_frames[cleaned_name] = anchor_list
                            cleaned_player_frame_counts[cleaned_name] = player_frame_counts[player_name]
                    
                    # Update the dictionaries with cleaned names
                    players_in_anchor_frames = cleaned_players_in_anchor_frames
                    player_frame_counts = cleaned_player_frame_counts
                    
                    players_list = sorted(players_in_anchor_frames.keys(), key=lambda p: player_frame_counts[p], reverse=True)
                    players_in_anchor_frames_set = set(players_list)  # Create set for filtering
                    print(f"  ðŸ“‹ Players found in anchor frames ({len(players_list)} total):")
                    for player_name in players_list:
                        frame_count = player_frame_counts[player_name]
                        print(f"     â†’ {player_name}: {frame_count} frame(s)")
                else:
                    players_in_anchor_frames_set = set()
                
                # CRITICAL: Build set of players actually in the current video
                # This will be used to restrict gallery matching to ONLY players in the video
                # NOTE: We include anchor-protected players here for tracking, but they will be excluded from matching via exclude_players
                players_in_current_video = set(players_in_anchor_frames_set)  # Start with anchor frame players
                
                # CRITICAL: Build set of players available for Re-ID matching (players in video but NOT anchor-protected)
                # This ensures Re-ID only matches untagged players against non-anchor-protected gallery players
                # Example: 5 players on field, 3 have anchors â†’ Re-ID matches remaining 2 untagged players
                # Example: Gallery has 9 players, 3 have anchors â†’ Re-ID only considers 6 non-anchor players
                players_available_for_reid = set()  # Will be populated from CSV or other sources
                
                # VALIDATION: Filter out players that don't appear in CSV tracking data (if available)
                if validate_against_csv and players_in_anchor_frames:
                    # Look for CSV files in video directory
                    # CRITICAL: Prefer CSV file matching the current video name to avoid using CSV from different videos
                    csv_files = []
                    csv_file_matching_video = None
                    try:
                        for filename in os.listdir(video_dir):
                            if filename.endswith('_tracking_data.csv') or filename.endswith('_analyzed_tracking_data.csv'):
                                csv_path = os.path.join(video_dir, filename)
                                csv_files.append(csv_path)
                                # Check if this CSV matches the current video name
                                csv_basename = os.path.splitext(filename)[0]
                                # Remove suffixes to get video name
                                csv_video_name = csv_basename.replace('_tracking_data', '').replace('_analyzed_tracking_data', '').replace('_analyzed', '')
                                if csv_video_name == video_basename:
                                    csv_file_matching_video = csv_path
                    except:
                        pass
                    
                    # Prefer CSV matching current video, fallback to most recent
                    if csv_file_matching_video:
                        csv_file = csv_file_matching_video
                        print(f"  ðŸ” Using CSV matching current video: {os.path.basename(csv_file)}")
                    elif csv_files:
                        # Use the most recent CSV file as fallback
                        csv_file = max(csv_files, key=os.path.getmtime) if csv_files else None
                        if csv_file:
                            print(f"  âš  WARNING: Using most recent CSV (may be from different video): {os.path.basename(csv_file)}")
                            print(f"     â†’ Current video: {video_basename}")
                            print(f"     â†’ CSV video: {os.path.splitext(os.path.basename(csv_file))[0].replace('_tracking_data', '').replace('_analyzed_tracking_data', '').replace('_analyzed', '')}")
                    else:
                        csv_file = None
                        if csv_file:
                            try:
                                import csv as csv_module
                                players_in_csv = set()
                                
                                with open(csv_file, 'r') as f:
                                    # Skip comment lines (starting with '#') - these contain metadata
                                    lines = [line for line in f if not line.strip().startswith('#')]
                                    reader = csv_module.DictReader(lines)
                                    for row in reader:
                                        player_name_raw = row.get('player_name', '').strip()
                                        # Handle list-formatted names (e.g., "['Rocco Piazza', '', '']")
                                        if player_name_raw:
                                            # Extract actual name from list format if needed
                                            if player_name_raw.startswith('[') and player_name_raw.endswith(']'):
                                                # Parse list format: ['Name', 'Team', 'Jersey']
                                                try:
                                                    import ast
                                                    name_list = ast.literal_eval(player_name_raw)
                                                    if isinstance(name_list, list) and len(name_list) > 0:
                                                        player_name = str(name_list[0]).strip()
                                                    else:
                                                        player_name = player_name_raw
                                                except:
                                                    player_name = player_name_raw
                                            else:
                                                player_name = player_name_raw
                                            
                                            if player_name and player_name not in ['', 'Unknown', 'None']:
                                                players_in_csv.add(player_name)
                                
                                if players_in_csv:
                                    print(f"  ðŸ” Validating anchor frames against CSV: {os.path.basename(csv_file)}")
                                    print(f"     â†’ Players in CSV: {', '.join(sorted(players_in_csv))}")
                                    
                                    # Show anchor frame players before filtering
                                    print(f"     â†’ Players in anchor frames: {', '.join(sorted(players_list))}")
                                    
                                    # Add CSV players to the set of players in current video
                                    players_in_current_video.update(players_in_csv)
                                    
                                    # Filter out anchor frames for players not in CSV
                                    # Use case-insensitive matching to handle name variations
                                    players_not_in_csv = []
                                    csv_names_lower = {name.lower(): name for name in players_in_csv}
                                    anchor_names_lower = {name.lower(): name for name in players_list}
                                    
                                    for anchor_name in players_list:
                                        # Handle list-formatted anchor names (e.g., ['Rocco Piazza', '', ''])
                                        anchor_name_clean = anchor_name
                                        if isinstance(anchor_name, list):
                                            anchor_name_clean = str(anchor_name[0]).strip() if len(anchor_name) > 0 else ''
                                        elif isinstance(anchor_name, str) and anchor_name.startswith('['):
                                            # Parse list format string
                                            try:
                                                import ast
                                                name_list = ast.literal_eval(anchor_name)
                                                if isinstance(name_list, list) and len(name_list) > 0:
                                                    anchor_name_clean = str(name_list[0]).strip()
                                            except:
                                                anchor_name_clean = anchor_name
                                        
                                        if not anchor_name_clean or anchor_name_clean in ['', 'Unknown', 'None']:
                                            players_not_in_csv.append(anchor_name)
                                            continue
                                        
                                        anchor_lower = anchor_name_clean.lower()
                                        # Check exact match first
                                        if anchor_name_clean not in players_in_csv:
                                            # Check case-insensitive match
                                            if anchor_lower not in csv_names_lower:
                                                players_not_in_csv.append(anchor_name)
                                            else:
                                                # Case-insensitive match found - keep it but warn about case mismatch
                                                matched_csv_name = csv_names_lower[anchor_lower]
                                                if anchor_name_clean != matched_csv_name:
                                                    print(f"     â†’ Note: '{anchor_name}' matches CSV player '{matched_csv_name}' (case-insensitive)")
                                    
                                    if players_not_in_csv:
                                        print(f"  âš  WARNING: Filtering out anchor frames for players NOT in video CSV:")
                                        for player_name in sorted(players_not_in_csv):
                                            frame_count = player_frame_counts[player_name]
                                            print(f"     â†’ {player_name}: {frame_count} frame(s) - REMOVED")
                                            print(f"        â†’ This player may not be tracked yet, or name doesn't match CSV")
                                            print(f"        â†’ Solution: Ensure player name in anchor frames matches CSV exactly")
                                        
                                        # Remove anchor frames for these players
                                        filtered_count = 0
                                        for frame_num in list(anchor_frames.keys()):
                                            original_anchors = anchor_frames[frame_num]
                                            filtered_anchors = [a for a in original_anchors if a.get('player_name', '') not in players_not_in_csv]
                                            if len(filtered_anchors) < len(original_anchors):
                                                filtered_count += len(original_anchors) - len(filtered_anchors)
                                                if filtered_anchors:
                                                    anchor_frames[frame_num] = filtered_anchors
                                                else:
                                                    del anchor_frames[frame_num]
                                        
                                        if filtered_count > 0:
                                            print(f"  âœ“ Removed {filtered_count} anchor frame(s) for players not in video")
                                            total_anchors = sum(len(anchors) for anchors in anchor_frames.values() if anchors is not None and isinstance(anchors, list))
                                            print(f"  âœ“ Remaining anchor frames: {total_anchors} tag(s) from {len(anchor_frames)} frames")
                                    else:
                                        print(f"  âœ“ All players in anchor frames are present in CSV")
                            except Exception as e:
                                print(f"  âš  Could not validate against CSV: {e}")
                
                        total_anchors = sum(len(anchors) for anchors in anchor_frames.values() if anchors is not None and isinstance(anchors, list))
                        
                        # CRITICAL: Try to recover bbox from CSV files if anchor frames are missing bbox
                        anchors_without_bbox = 0
                        bbox_recovered = 0
                        if total_anchors > 0:
                            # Look for CSV files in video directory that might have tracking data
                            csv_files = []
                            try:
                                for filename in os.listdir(video_dir):
                                    if filename.endswith('_tracking_data.csv') or filename.endswith('_analyzed_tracking_data.csv'):
                                        csv_files.append(os.path.join(video_dir, filename))
                            except:
                                pass
                            
                            # Try to load CSV and recover bbox
                            if csv_files:
                                # Use the most recent CSV file
                                csv_file = max(csv_files, key=os.path.getmtime) if csv_files else None
                                if csv_file:
                                    try:
                                        import csv as csv_module
                                        csv_tracks = {}  # {track_id: {frame: (x1, y1, x2, y2)}}
                                        
                                        with open(csv_file, 'r') as f:
                                            # Skip comment lines (starting with '#') - these contain metadata
                                            lines = [line for line in f if not line.strip().startswith('#')]
                                            reader = csv_module.DictReader(lines)
                                            for row in reader:
                                                try:
                                                    frame = int(float(row.get('frame', 0)))
                                                    track_id = int(float(row.get('player_id', -1)))
                                                    player_x = float(row.get('player_x', 0))
                                                    player_y = float(row.get('player_y', 0))
                                                    
                                                    # Try to get bbox from CSV if available
                                                    bbox_x1 = row.get('bbox_x1', '')
                                                    bbox_y1 = row.get('bbox_y1', '')
                                                    bbox_x2 = row.get('bbox_x2', '')
                                                    bbox_y2 = row.get('bbox_y2', '')
                                                    
                                                    if bbox_x1 and bbox_y1 and bbox_x2 and bbox_y2:
                                                        # CSV has bbox - use it
                                                        x1, y1, x2, y2 = float(bbox_x1), float(bbox_y1), float(bbox_x2), float(bbox_y2)
                                                    else:
                                                        # Create bbox from center point (default size: 80x160)
                                                        x1 = player_x - 40
                                                        y1 = player_y - 80
                                                        x2 = player_x + 40
                                                        y2 = player_y + 80
                                                    
                                                    if track_id not in csv_tracks:
                                                        csv_tracks[track_id] = {}
                                                    csv_tracks[track_id][frame] = (x1, y1, x2, y2)
                                                except (ValueError, TypeError):
                                                    continue
                                        
                                        # Now try to recover bbox for anchor frames
                                        for frame_num, anchors in anchor_frames.items():
                                            for anchor in anchors:
                                                if anchor.get('bbox') is None and anchor.get('track_id') is not None:
                                                    anchors_without_bbox += 1
                                                    track_id = int(anchor.get('track_id'))
                                                    
                                                    # Try to find bbox in CSV
                                                    if track_id in csv_tracks:
                                                        # Try exact frame match first
                                                        if frame_num in csv_tracks[track_id]:
                                                            anchor['bbox'] = list(csv_tracks[track_id][frame_num])
                                                            bbox_recovered += 1
                                                        else:
                                                            # Try nearby frames (within 10 frames)
                                                            best_frame = None
                                                            best_diff = float('inf')
                                                            for csv_frame in csv_tracks[track_id].keys():
                                                                diff = abs(csv_frame - frame_num)
                                                                if diff < best_diff and diff <= 10:
                                                                    best_frame = csv_frame
                                                                    best_diff = diff
                                                            
                                                            if best_frame is not None:
                                                                anchor['bbox'] = list(csv_tracks[track_id][best_frame])
                                                                bbox_recovered += 1
                                        
                                        if bbox_recovered > 0:
                                            print(f"  ðŸ”§ Recovered bbox for {bbox_recovered}/{anchors_without_bbox} anchor frames from CSV: {os.path.basename(csv_file)}")
                                        elif anchors_without_bbox > 0:
                                            print(f"  âš  {anchors_without_bbox} anchor frames missing bbox - could not recover from CSV")
                                            print(f"     â†’ Tip: Recreate anchor frames using 'Convert Tracks â†’ Anchor Frames' to include bbox")
                                    except Exception as e:
                                        if anchors_without_bbox > 0:
                                            print(f"  âš  Could not recover bbox from CSV: {e}")
                        
                        print(f"âœ“ Loaded {total_anchors} anchor frame tag(s) from {len(anchor_frames)} frames: {os.path.basename(seed_file)}")
                        loaded_file_path = seed_file  # Store the file path that was loaded
                        
                        # Warn if using optimized file (may have fewer frames)
                        if "_optimized" in os.path.basename(seed_file):
                            print(f"  âš  Using optimized anchor file - this may have fewer anchor frames than the original")
                            print(f"     â†’ If you need all anchor frames, ensure PlayerTagsSeed-{video_basename}.json exists")
                        
                        # Show sample of what was loaded
                        if total_anchors > 0:
                            sample_frame = sorted(anchor_frames.keys())[0]
                            sample_anchors = anchor_frames.get(sample_frame)
                            if sample_anchors and isinstance(sample_anchors, list) and len(sample_anchors) > 0:
                                sample_anchor = sample_anchors[0]
                                bbox_info = f", bbox={sample_anchor.get('bbox')}" if sample_anchor.get('bbox') else ", bbox=None"
                                print(f"  ðŸ“‹ Sample: Frame {sample_frame} = {sample_anchor.get('player_name', 'Unknown')} (track_id={sample_anchor.get('track_id', 'None')}{bbox_info})")
            except Exception as e:
                print(f"âš  Error accessing anchor_frames in {os.path.basename(seed_file)}: {e}")
                import traceback
                traceback.print_exc()
        else:
            # No anchor_frames key in seed data
            print(f"  âš  {os.path.basename(seed_file)} does not contain 'anchor_frames' key")
    
    # If no file matched the patterns, try searching for any PlayerTagsSeed-*.json files (including part*.json)
    if not loaded_file_path:
        print(f"ðŸ” Searching for PlayerTagsSeed files in: {video_dir}")
        try:
            found_files = []
            for filename in os.listdir(video_dir):
                if filename.startswith("PlayerTagsSeed-") and filename.endswith(".json"):
                    found_files.append(filename)
                    candidate_path = os.path.join(video_dir, filename)
                    try:
                        with open(candidate_path, 'r') as f:
                            seed_data = json.load(f)
                        
                        file_video_path = seed_data.get("video_path", "")
                        has_anchor_frames = "anchor_frames" in seed_data
                        anchor_frames_count = 0
                        if has_anchor_frames:
                            af = seed_data.get("anchor_frames")
                            if isinstance(af, dict):
                                anchor_frames_count = len(af)
                        
                        print(f"  ðŸ“„ Found: {filename}")
                        print(f"     â†’ Video path in file: {os.path.basename(file_video_path) if file_video_path else 'None'}")
                        print(f"     â†’ Has anchor_frames key: {has_anchor_frames}")
                        if has_anchor_frames:
                            print(f"     â†’ Anchor frames count: {anchor_frames_count}")
                        
                        # CRITICAL: Check if it's for this video using STRICT full path matching
                        # This prevents loading anchor frames from videos with same filename in different folders
                        video_match = False
                        if file_video_path:
                            # Normalize paths for comparison (handles different path formats)
                            file_video_normalized = os.path.normpath(os.path.abspath(file_video_path))
                            input_video_normalized = os.path.normpath(os.path.abspath(input_path))
                            
                            # STRICT: Require exact path match (not just filename)
                            if file_video_normalized == input_video_normalized:
                                video_match = True
                                print(f"     â†’ âœ“ Matches current video (exact path match)")
                            else:
                                # Filenames might match but paths differ - this is a different video!
                                file_basename = os.path.basename(file_video_path)
                                input_basename = os.path.basename(input_path)
                                if file_basename == input_basename:
                                    print(f"     â†’ âœ— DIFFERENT VIDEO (same filename '{file_basename}' but different folder)")
                                    print(f"        Anchor file video: {file_video_path}")
                                    print(f"        Current video: {input_path}")
                                    print(f"        â†’ Skipping to avoid wrong player assignments")
                                else:
                                    print(f"     â†’ âœ— Different video (filename mismatch: '{file_basename}' vs '{input_basename}')")
                        
                        # FALLBACK: If no video_path specified, only match if filename exactly matches video basename
                        # WARNING: This is less safe if you have videos with same name in different folders
                        if not file_video_path and has_anchor_frames and not video_match:
                            # Extract potential video name from filename
                            # PlayerTagsSeed-part001.json -> part001
                            name_part = filename.replace("PlayerTagsSeed-", "").replace("-Project.json", "").replace("_optimized.json", "").replace(".json", "")
                            input_name_part = os.path.splitext(os.path.basename(input_path))[0]
                            # Only match if names are exactly the same (not partial match)
                            if name_part == input_name_part:
                                video_match = True
                                print(f"     â†’ âš  Matched by filename only (no video_path in anchor file)")
                                print(f"        â†’ WARNING: This is less safe if you have videos with same name in different folders")
                                print(f"        â†’ Recommendation: Re-save anchor frames to include video_path")
                            else:
                                print(f"     â†’ âœ— Filename doesn't match (skipping to avoid wrong player assignments)")
                        
                        if video_match and has_anchor_frames:
                            loaded_anchors = seed_data["anchor_frames"]
                            if loaded_anchors and isinstance(loaded_anchors, dict) and len(loaded_anchors) > 0:
                                # Merge with existing
                                for frame_num_str, anchors in loaded_anchors.items():
                                    if anchors is None or not isinstance(anchors, list):
                                        continue
                                    frame_num = int(frame_num_str)
                                    if frame_num not in anchor_frames:
                                        anchor_frames[frame_num] = []
                                    for anchor in anchors:
                                        if anchor not in anchor_frames[frame_num]:
                                            anchor_frames[frame_num].append(anchor)
                                loaded_file_path = candidate_path
                                total_anchors = sum(len(anchors) for anchors in anchor_frames.values() if anchors is not None and isinstance(anchors, list))
                                print(f"âœ“ Loaded {total_anchors} anchor frame tag(s) from {len(anchor_frames)} frames: {os.path.basename(candidate_path)}")
                                break
                            elif has_anchor_frames:
                                print(f"     âš  File has anchor_frames key but it's empty or invalid")
                        elif has_anchor_frames:
                            print(f"     âš  Video path mismatch - skipping")
                    except Exception as e:
                        print(f"     âš  Error reading {filename}: {e}")
                        continue
            
            if not found_files:
                print(f"  â„¹ No PlayerTagsSeed-*.json files found in {video_dir}")
            elif not loaded_file_path:
                print(f"  âš  Found {len(found_files)} PlayerTagsSeed file(s) but none matched this video")
                print(f"     â†’ Files were skipped to prevent wrong player assignments from different videos")
                print(f"     â†’ To use anchor frames, create a PlayerTagsSeed file for this video using Setup Wizard")
        except Exception as e:
            print(f"âš  Error searching for PlayerTagsSeed files: {e}")
            import traceback
            traceback.print_exc()
    
    # Ensure players_in_current_video is defined even if no anchor frames were loaded
    if 'players_in_current_video' not in locals():
        players_in_current_video = set(players_in_anchor_frames_set)
    
    return anchor_frames, loaded_file_path, players_in_anchor_frames_set, players_in_current_video


def preprocess_frame_worker(args):
    """
    Worker function for parallel frame preprocessing (dewarping, net removal).
    This function is designed to be called by ProcessPoolExecutor.
    
    NOTE: Net removal is applied to YOLO detection frames, but original frames
    are preserved for Re-ID feature extraction and gallery learning to maintain
    image quality.
    """
    frame, frame_num, dewarp, dewarp_maps, remove_net = args

    # Store original frame for learning (Re-ID and gallery need sharp images)
    original_frame = frame.copy()

    # Apply dewarping if requested
    if dewarp and dewarp_maps is not None:
        map1, map2 = dewarp_maps
        frame = cv2.remap(
            frame,
            map1,
            map2,
            interpolation=cv2.INTER_LINEAR,
            borderMode=cv2.BORDER_CONSTANT)
        # Also dewarp original frame for consistency
        original_frame = cv2.remap(
            original_frame,
            map1,
            map2,
            interpolation=cv2.INTER_LINEAR,
            borderMode=cv2.BORDER_CONSTANT)

    # Remove net if requested - using improved battle-tested algorithm
    # NOTE: Only apply to YOLO detection frame, keep original for learning
    yolo_frame = frame.copy()
    if remove_net:
        yolo_frame = remove_net_pattern(yolo_frame, kernel_size=21, sigma=7)

    # Return both: processed frame for YOLO, original for learning
    return yolo_frame, original_frame, frame_num


def match_players_by_frame(
        previous_frame_data,
        current_frame_num,
        current_detections,
        player_names,
        max_distance_pixels=50):
    """
    Match players from previous analysis to current detections based on frame number and position.

    Args:
        previous_frame_data: Dict from load_previous_tracking_data()
        current_frame_num: Current frame number
        current_detections: sv.Detections object with current detections
        player_names: Dict to update with matched player names
        max_distance_pixels: Maximum distance in pixels to consider a match

    Returns:
        Number of matches made
    """
    if current_frame_num not in previous_frame_data:
        return 0

    if len(current_detections) == 0:
        return 0

    matches_made = 0
    previous_players = previous_frame_data[current_frame_num]

    # Get current detection centers
    tracker_ids = current_detections.tracker_id if current_detections.tracker_id is not None else [
        None] * len(current_detections.xyxy)

    for prev_player in previous_players:
        prev_name_raw = prev_player.get('player_name')
        prev_x = prev_player.get('x')
        prev_y = prev_player.get('y')

        # CRITICAL FIX: Handle case where prev_name might be a list or other type
        if isinstance(prev_name_raw, list) and len(prev_name_raw) > 0:
            prev_name = str(prev_name_raw[0]).strip()
        elif prev_name_raw:
            prev_name = str(prev_name_raw).strip()
        else:
            prev_name = None

        if not prev_name or not prev_name.strip():
            continue

        if prev_x is None or prev_y is None:
            continue

        # Find closest current detection
        best_match_idx = None
        best_distance = float('inf')

        for i, (xyxy, track_id) in enumerate(
                zip(current_detections.xyxy, tracker_ids)):
            if track_id is None:
                continue

            # Calculate center of current detection
            center_x = (xyxy[0] + xyxy[2]) / 2
            center_y = (xyxy[1] + xyxy[3]) / 2

            # Calculate distance
            distance = np.sqrt((center_x - prev_x)**2 + (center_y - prev_y)**2)

            if distance < best_distance and distance <= max_distance_pixels:
                best_distance = distance
                best_match_idx = i

        # Assign player name if we found a good match
        if best_match_idx is not None:
            matched_track_id = str(tracker_ids[best_match_idx])
            # CRITICAL FIX: Ensure existing value is string before calling strip()
            existing_name = player_names.get(matched_track_id)
            if not existing_name or (isinstance(existing_name, str) and not existing_name.strip()):
                player_names[matched_track_id] = prev_name
                matches_made += 1
            elif not isinstance(existing_name, str):
                # If existing is not a string (e.g., list), replace it
                player_names[matched_track_id] = prev_name
                matches_made += 1

    return matches_made


def combined_analysis_optimized(input_path, output_path,
                                dewarp=False, track_ball_flag=True, track_players_flag=True,
                                export_csv=True, buffer=64, batch_size=8,
                                ball_min_radius=5, ball_max_radius=50, remove_net=False,
                                show_ball_trail=True,
                                track_thresh=0.25, match_thresh=0.6, track_buffer=30, track_buffer_seconds=5.0, min_track_length=5,
                                min_bbox_area=200, min_bbox_width=10, min_bbox_height=15,  # Minimum detection size (configurable from GUI)
                                tracker_type="bytetrack",  # "bytetrack" or "ocsort"
                                viz_style="box", viz_color_mode="team",
                                preserve_audio=True, video_fps=None, output_fps=None,
                                process_every_nth_frame=1, temporal_smoothing=True,
                                yolo_resolution="full", foot_based_tracking=True,
                                use_reid=True, reid_similarity_threshold=0.6,
                                osnet_variant="osnet_x1_0", use_boxmot_backend=True,
                                use_gsi=False, gsi_interval=20, gsi_tau=10.0,
                                occlusion_recovery_seconds=3.0, occlusion_recovery_distance=250,
                                reid_check_interval=30, reid_confidence_threshold=0.75,
                                use_harmonic_mean=True, use_expansion_iou=True, enable_soccer_reid_training=False,
                                use_enhanced_kalman=True, use_ema_smoothing=True,
                                confidence_filtering=True, adaptive_confidence=True,
                                use_optical_flow=False,  # Use optical flow for motion prediction to reduce tracking blinking
                                track_referees=False, max_players=12, enable_substitutions=True,
                                enable_velocity_constraints=True,  # Enable velocity constraints to prevent impossible jumps
                                # Adjustable ellipse size for foot-based
                                # tracking
                                ellipse_width=20, ellipse_height=12,
                                ellipse_outline_thickness=3,  # White border thickness around ellipse (pixels)
                                show_ball_possession=True,  # Show triangle indicator when player has ball
                                trail_length=20,
                                use_yolo_streaming=False,  # Use YOLO's native streaming mode for memory efficiency
                                # Number of trail points to display (shorter =
                                # less visual clutter)
                                trail_buffer=20,
                                # Trail buffer size (max trail history)
                                box_shrink_factor=0.10,
                                # Box shrink factor (0.0 = no shrink, 0.10 = 10% shrink on each side for
                                # tighter boxes)
                                show_player_labels=True,
                                # Show player name/ID labels (False = hide all
                                # labels to reduce clutter)
                                show_yolo_boxes=False,  # Show raw YOLO detection boxes (before tracking) in orange
                                label_font_scale=0.7,
                                # Label font size (smaller = less clutter,
                                # default: 0.7)
                                label_type="full_name",
                                # Label type: "full_name", "last_name", "jersey", "team", "custom"
                                label_custom_text="Player",
                                # Custom text for all labels (used when label_type="custom")
                                label_font_face="FONT_HERSHEY_SIMPLEX",
                                # Font face: "FONT_HERSHEY_SIMPLEX", "FONT_HERSHEY_PLAIN", etc.
                                show_predicted_boxes=True,
                                # Show predicted boxes for lost tracks (track
                                # ID decay visualization)
                                prediction_duration=0.3,
                                # Prediction duration in seconds (how long to show predicted boxes after
                                # track is lost)
                                prediction_style="dot",
                                # Style for predicted boxes: "dot", "box", "cross", "x", "arrow", "diamond"
                                prediction_size=5,
                                # Size of predicted markers in pixels
                                prediction_color=None,
                                # Color of predicted markers as (B, G, R) tuple or None for default yellow
                                preview_mode=False,  # Preview mode: process only a small sample of frames
                                # Maximum frames to process in preview mode
                                # (default: 360 frames = 15 seconds at 24fps)
                                preview_max_frames=360,
                                box_thickness=2,  # Box border thickness in pixels (default: 2)
                                box_color=None,  # Box color as (B, G, R) tuple or None for team colors
                                label_color=None,  # Label color as (B, G, R) tuple or None for team colors
                                player_viz_alpha=255,  # Opacity for player boxes/ellipses (0-255, 255 = fully opaque)
                                show_bounding_boxes=True,  # Show bounding boxes around players (separate from circles)
                                show_circles_at_feet=True,  # Show team-colored circles at player feet (always uses team colors)
                                # Enhanced feet marker visualization (high-quality graphics)
                                feet_marker_style="circle",  # Style: "circle", "ellipse", "diamond", "star", "hexagon", "ring", "glow", "pulse"
                                feet_marker_opacity=255,  # Opacity for feet markers (0-255, separate from box opacity)
                                feet_marker_enable_glow=False,  # Enable glow effect
                                feet_marker_glow_intensity=50,  # Glow intensity (0-100)
                                feet_marker_enable_shadow=False,  # Enable shadow effect
                                feet_marker_shadow_offset=3,  # Shadow offset in pixels
                                feet_marker_shadow_opacity=128,  # Shadow opacity (0-255)
                                feet_marker_enable_gradient=False,  # Enable gradient fill
                                feet_marker_enable_pulse=False,  # Enable pulse animation
                                feet_marker_pulse_speed=2.0,  # Pulse animation speed (cycles per second)
                                feet_marker_enable_particles=False,  # Enable particle effects
                                feet_marker_particle_count=5,  # Number of particles
                                feet_marker_vertical_offset=50,  # Vertical offset in pixels (negative = above feet, positive = below feet)
                                show_direction_arrow=False,  # Show direction arrow under feet pointing in direction of travel
                                show_player_trail=False,  # Show player trail/breadcrumb behind player
                                direction_arrow_color=None,  # Arrow color (None = white default, tuple of (B, G, R))
                                watch_only=False,  # Watch-only mode: learn from video without saving output
                                show_live_viewer=False,  # Show live viewer window during watch-only mode
                                viewer_downscale="720p",  # Downscale live viewer for performance: "none", "720p", "1080p"
                                viewer_threaded=False,  # Use threaded viewer (non-blocking, better for high-res videos)
                                focused_players=None,  # List of player names to focus on (None = all players)
                                use_imperial_units=True,  # Convert distances to feet and speeds to mph (American units)
                                save_base_video=False,  # Save base video without overlays
                                export_overlay_metadata=True,  # Export overlay metadata for separate rendering
                                enable_video_encoding=True,  # Enable video encoding (can disable for faster analysis)
                                overlay_quality="hd",  # Overlay quality: "sd", "hd", "4k" (affects rendering resolution)
                                render_scale=1.0,  # Render scale multiplier for HD overlays (1.0 = original, 2.0 = 2x resolution)
                                enable_advanced_blending=True,  # Enable advanced blending modes (glow, screen, additive)
                                enable_motion_blur=False,  # Enable motion blur for fast-moving objects
                                motion_blur_amount=1.0,  # Motion blur intensity (0.0-2.0)
                                use_professional_text=True,  # Use PIL-based professional text rendering
                                enable_text_gradient=False,  # Enable gradient text effects
                                enable_text_glow=False,  # Enable text glow effects
                                enable_text_pulse=False,  # Enable pulsing text animation
                                enable_glow_pulse=False,  # Enable pulsing glow effects
                                enable_color_shift=False,  # Enable color-shifting glow
                                enable_gradient_boxes=False,  # Enable gradient-filled boxes
                                enable_particle_trails=False,  # Enable particle motion trails
                                graphics_quality_preset="hd",  # Quality preset: "sd", "hd", "4k"
                                gallery_similarity_threshold=0.40,  # Gallery similarity threshold (default: 0.40, adjustable via GUI)
                                # Broadcast-level graphics settings
                                trajectory_smoothness="bezier",  # Trajectory smoothness: "linear", "bezier", "spline"
                                player_graphics_style="standard",  # Player graphics style: "minimal", "standard", "broadcast"
                                use_rounded_corners=True,  # Use rounded corners for bounding boxes
                                use_gradient_fill=False,  # Use gradient fill for bounding boxes
                                corner_radius=5,  # Corner radius for rounded rectangles (pixels)
                                show_jersey_badge=False,  # Show circular jersey number badge
                                ball_graphics_style="standard",  # Ball graphics style: "standard", "broadcast"
                                show_statistics=False,  # Show broadcast-style statistics overlay
                                statistics_position="top_left",  # Statistics panel position: "top_left", "top_right", "bottom_left", "bottom_right"
                                show_heat_map=False,  # Show player position heat map
                                heat_map_alpha=0.4,  # Heat map opacity (0.0 to 1.0)
                                heat_map_color_scheme="hot",  # Heat map color scheme: "hot", "cool", "green"
                                overlay_quality_preset="hd",  # Overlay quality preset: "sd", "hd", "4k", "broadcast"
                                progress_callback=None,  # Progress callback function(frame_count, total_frames, progress_pct)
                                video_type="practice",  # "practice" or "game" - controls team locking behavior (practice: flexible, game: strict)
                                viz_settings_override=None,  # Optional dict of visualization settings to override metadata settings (e.g., statistics panel customization)
                                explicit_anchor_file=None,  # Optional explicit path to PlayerTagsSeed file to load (if None, auto-selects newest)
                                seed_frame_interval=None):  # Force gallery mapping every N frames (None = disabled, 0 = every 100 frames)
    """
    Optimized combined analysis with batch processing for better GPU utilization.

    Args:
        input_path: Path to input video
        output_path: Path to output video
        dewarp: Whether to apply dewarping
        track_ball_flag: Whether to track ball
        track_players_flag: Whether to track players
        export_csv: Whether to export tracking data to CSV
        buffer: Ball trail buffer length
        batch_size: Number of frames to process in YOLO batch (default: 8, higher = more GPU usage)
        ball_min_radius: Minimum ball radius in pixels (default: 5)
        ball_max_radius: Maximum ball radius in pixels (default: 50)
        remove_net: Whether to attempt to reduce net visibility (default: False)
        show_ball_trail: Whether to draw red trail lines (default: True)
        track_thresh: ByteTrack detection confidence threshold (default: 0.25, lower = more detections)
        match_thresh: ByteTrack matching threshold (default: 0.6, higher = stricter matching, better ID consistency)
        track_buffer: ByteTrack frames to keep lost tracks (default: 30, higher = more persistent for fast movement)
        track_buffer_seconds: Buffer time in seconds (default: 1.5, optimized for Re-ID, faster processing, auto-calculates frames based on FPS)
        min_track_length: Minimum frames before track is activated (default: 3, lower = faster activation, less blinking)
        output_fps: Output video frame rate (default: None = same as input). Lower FPS = smaller file, slower playback
        process_every_nth_frame: Process every Nth frame for tracking (default: 1 = all frames). Higher = faster but less accurate
        temporal_smoothing: Apply temporal smoothing to player positions (default: True). Better tracking stability
        yolo_resolution: YOLO processing resolution - "full" (original), "1080p", or "720p" (default: "full"). Lower = faster processing
        foot_based_tracking: Use foot position as tracking anchor for more stable tracking (default: True)
        use_reid: Enable Re-ID (Re-identification) for better ID persistence during occlusions (default: True)
        reid_similarity_threshold: Minimum cosine similarity for Re-ID matching (default: 0.5, higher = stricter)
        gallery_similarity_threshold: Minimum cosine similarity for gallery matching (default: 0.40, adjustable via GUI, for cross-video player identification)
        osnet_variant: OSNet variant to use ('osnet_x1_0', 'osnet_ain_x1_0', 'osnet_ibn_x1_0', etc.) (default: 'osnet_x1_0')
        use_boxmot_backend: Use BoxMOT optimized backends (ONNX/TensorRT) for faster inference (default: True)
        use_gsi: Enable Gaussian Smoothed Interpolation for smoother tracks (default: False)
        gsi_interval: Maximum frame gap to interpolate for GSI (default: 20)
        gsi_tau: Time constant for Gaussian smoothing in GSI (default: 10.0, higher = more smoothing)
        use_harmonic_mean: Use Harmonic Mean for association (default: True, based on Deep HM-SORT)
        use_expansion_iou: Use Expansion IOU with motion prediction (default: True, based on Deep HM-SORT)
        use_optical_flow: Use optical flow for motion prediction to reduce tracking blinking (default: False).
                         When enabled, predicts detection positions based on motion between frames, helping
                         maintain track continuity during occlusions. Adds ~5-10% processing overhead.
        enable_soccer_reid_training: Enable soccer-specific Re-ID training data collection (default: False)
        use_enhanced_kalman: Enable enhanced Kalman filtering for additional smoothing (default: True)
        use_ema_smoothing: Use EMA (Exponential Moving Average) instead of simple average for temporal smoothing (default: True)
        confidence_filtering: Filter low-confidence detections before tracking (default: True)
        adaptive_confidence: Use adaptive confidence threshold based on detection consistency (default: True)
        ellipse_width: Width of ellipse for foot-based tracking visualization (default: 20 pixels)
        ellipse_height: Height of ellipse for foot-based tracking visualization (default: 12 pixels)
        ellipse_outline_thickness: White border thickness around ellipse (default: 3 pixels)
        show_ball_possession: Show triangle indicator when player has ball (default: True)
        track_referees: Track referees and bench players in addition to field players (default: False)
        max_players: Maximum number of field players to track (default: 12, for 11 players + coach)
        enable_substitutions: Enable substitution detection and handling (default: True)
        use_yolo_streaming: Use YOLO's native streaming mode for video processing (default: False). 
                           When True, uses memory-efficient generator instead of loading all frames.
        occlusion_recovery_seconds: How long to search for disappeared players in seconds (default: 3.0s).
                                   Scales with FPS automatically. Higher = longer search = better recovery.
        occlusion_recovery_distance: Maximum pixel distance for spatial recovery of disappeared players (default: 250px).
                                    Higher = search wider area = better recovery but more false positives.
        reid_check_interval: How often to check Re-ID for tracks with existing assignments in frames (default: 30).
                            Lower = more frequent checks = better reconnection but slower.
        reid_confidence_threshold: Skip Re-ID checks if track confidence above this threshold (default: 0.75).
                                  Lower = check more tracks = better but slower.
                           Note: Requires video file path, not applicable to frame-by-frame processing.
    """
    
    # NOTE: Many variables below are flagged as "unused" by static analyzers, but they ARE used
    # throughout the function. Static analyzers can't track variables through complex control flow,
    # conditional branches, or when passed to functions. These warnings are false positives.
    
    cap = cv2.VideoCapture(input_path)  # Used later: cap.read(), cap.release(), cap.set()
    if not cap.isOpened():
        print(f"Error: Could not open video file {input_path}")
        return

    # Get video properties
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    # CRITICAL FIX: Preview mode - limit frames processed
    if preview_mode:
        original_total_frames = total_frames
        total_frames = min(preview_max_frames, total_frames)
        print(
            f"PREVIEW MODE: Processing {total_frames} frames (of {original_total_frames} total)")
        print(f"  â†’ This is a quick preview to test your settings")
        print(f"  â†’ PREVIEW SETTINGS (from GUI): match_thresh={match_thresh:.2f}, track_buffer_seconds={track_buffer_seconds:.1f}s, min_track_length={min_track_length}, tracker_type={tracker_type}")
        print(f"  â†’ PREVIEW SETTINGS (from GUI): use_reid={use_reid}, use_harmonic_mean={use_harmonic_mean}, use_expansion_iou={use_expansion_iou}")
    else:
        print(f"Total frames: {total_frames}")

    # Get FPS from video or use manual override
    detected_fps = cap.get(cv2.CAP_PROP_FPS)  # Keep as float for calculations
    if video_fps is not None and video_fps > 0:
        # Use manual FPS override if provided
        fps = float(video_fps)
        print(
            f"Video: {width}x{height} @ {
                fps:.1f}fps (manual override, detected: {
                detected_fps:.1f}fps)")
    else:
        # Use detected FPS
        fps = detected_fps
        print(f"Video: {width}x{height} @ {fps:.1f}fps")
    
    # Initialize progress tracking
    try:
        import shared_state
        shared_state.update_analysis_progress(
            current=0,
            total=total_frames,
            status="Initializing analysis",
            details="Loading models and preparing video processing",
            phase="Initialization"
        )
    except (ImportError, Exception):
        pass  # shared_state not available, continue normally
    
    # Print detection settings
    print(f"âš™ï¸ Detection Settings:")
    print(f"   â†’ Confidence threshold: {track_thresh:.2f} (lower = more detections, recommended: 0.20-0.30 for better player coverage)")
    print(f"   â†’ Max field players: {max_players} (tracks beyond this will be filtered per frame)")
    print(f"   â†’ Track referees: {'Yes' if track_referees else 'No'}")

    # Load seed config (ball positions and rejected IDs from setup wizard) if
    # available
    seed_ball_positions, rejected_ids, seed_player_roster = load_seed_config()
    
    # Store seed player roster for video-specific active/inactive settings
    # This takes precedence over global roster for this video
    video_specific_roster = seed_player_roster if seed_player_roster else {}

    # Adjust ByteTrack buffer based on frame rate for consistent time-based tracking
    # Reference: https://datature.io/blog/introduction-to-bytetrack-multi-object-tracking-by-associating-every-detection-box
    # ByteTrack uses track_buffer to keep lost tracklets alive for association with low-confidence detections
    # Use track_buffer_seconds from GUI if provided, otherwise use default 1.5 seconds
    # This allows user to control buffer time directly (more intuitive than
    # frames)
    if fps > 0:
        # CRITICAL FIX: Use GUI setting for buffer time, or default to 1.5 seconds
        # Optimized for Re-ID: Shorter buffer reduces false reconnections and improves performance
        # Use GUI setting directly - no hardcoded auto-adjustments
        # User controls buffer time via GUI - respect their choice
        time_buffer_seconds = track_buffer_seconds if track_buffer_seconds > 0 else 5.0  # Default to 5.0 if not set

        # Sanity check - cap buffer time at reasonable maximum (10 seconds)
        # Prevents accidentally setting buffer to very long times
        if time_buffer_seconds > 10.0:
            print(
                f"âš  Warning: Track buffer time ({
                    time_buffer_seconds:.1f}s) is too high! Capping at 10.0 seconds.")
            print(
                f"   â†’ For 120fps video, 10s = {int(120 * 10)} frames (recommended: 4-6 seconds)")
            time_buffer_seconds = 10.0

        # Time-based buffer: scales with FPS to maintain consistent time duration
        # SUGGESTED: For better track stability and less player reassignment:
        #   - track_buffer_seconds: 8.0-10.0 seconds (longer = tracks stay alive longer = less new track creation)
        #   - For 120fps: 8.0s = 960 frames, 10.0s = 1200 frames
        track_buffer_scaled = int(fps * time_buffer_seconds)
        print(
            f"ByteTrack buffer: {track_buffer_scaled} frames (~{
                track_buffer_scaled /
                fps:.2f}s at {
                fps:.1f}fps, from GUI setting: {
                track_buffer_seconds:.1f}s)")
    else:
        # Fallback: use track_buffer frames directly if no FPS info
        # SUGGESTED: For better track stability, use track_buffer_seconds instead (scales with FPS)
        #   - If using track_buffer directly: 720-1200 frames for 120fps (6-10 seconds)
        track_buffer_scaled = track_buffer

    print(
        f"Batch processing: {batch_size} frames per batch (better GPU utilization)")
    if use_yolo_streaming:
        print(f"ðŸ“¡ YOLO streaming mode: Enabled (for direct video paths, not used with batch processing)")
        print(f"   â†’ Note: Streaming mode is most effective when processing video files directly")
        print(f"   â†’ Current batch processing uses frame arrays, streaming provides limited benefit")

    # Determine YOLO processing resolution
    yolo_scale_factor = 1.0
    yolo_width = width
    yolo_height = height
    if yolo_resolution == "1080p":
        yolo_width = 1920
        yolo_height = 1080
        yolo_scale_factor = width / 1920.0
        print(
            f"YOLO processing: 1080p (1920x1080) - {yolo_scale_factor:.2f}x speed boost")
    elif yolo_resolution == "720p":
        yolo_width = 1280
        yolo_height = 720
        yolo_scale_factor = width / 1280.0
        print(
            f"YOLO processing: 720p (1280x720) - {yolo_scale_factor:.2f}x speed boost")
    else:
        # For 4K videos, auto-downscale to 1080p to save memory (4K frames are
        # ~25MB each)
        if width >= 3840 or height >= 2160:
            yolo_width = 1920
            yolo_height = 1080
            yolo_scale_factor = width / 1920.0
            print(
                f"YOLO processing: Auto-downscaled 4K to 1080p (1920x1080) for memory efficiency")
            print(f"  â†’ Saves ~75% memory while maintaining good detection quality")
        else:
            print(f"YOLO processing: Full resolution ({width}x{height})")

    if process_every_nth_frame > 1:
        print(
            f"Frame skipping: Processing every {process_every_nth_frame} frames ({
                fps / process_every_nth_frame:.1f} effective FPS)")

    if foot_based_tracking:
        print("Foot-based tracking: Enabled (using foot position as stable anchor)")

    # Initialize dewarping if requested
    dewarp_maps = None
    if dewarp:
        center_x, center_y = width / 2, height / 2
        ifov_rad = np.radians(120)
        ofov_rad = np.radians(90)
        focal_length = width / (2 * np.tan(ifov_rad / 2))

        K = np.array([[focal_length, 0, center_x],
                      [0, focal_length, center_y],
                      [0, 0, 1]], dtype=np.float32)

        D = np.array([0.0, 0.0, 0.0, 0.0], dtype=np.float32)

        new_focal = width / (2 * np.tan(ofov_rad / 2))
        new_K, roi = cv2.getOptimalNewCameraMatrix(
            K, D, (width, height), 1, (width, height))

        dewarp_maps = cv2.fisheye.initUndistortRectifyMap(
            K, D, np.eye(3), new_K, (width, height), cv2.CV_16SC2)

    # Initialize YOLO model if player tracking is enabled
    model = None
    byte_tracker = None
    reid_tracker = None
    box_annotator = None
    label_annotator = None
    # CRITICAL: Initialize player_gallery early to prevent UnboundLocalError
    player_gallery = None
    if track_players_flag and YOLO_AVAILABLE:
        # Check for GPU availability - prefer NVIDIA GPU over Intel integrated
        # GPU
        import torch
        device = 'cpu'
        cuda_device_id = None

        if torch.cuda.is_available():
            # Find NVIDIA GPU (not Intel)
            for i in range(torch.cuda.device_count()):
                gpu_name = torch.cuda.get_device_name(i)
                if 'nvidia' in gpu_name.lower() or 'geforce' in gpu_name.lower(
                ) or 'rtx' in gpu_name.lower() or 'gtx' in gpu_name.lower():
                    device = 'cuda'
                    cuda_device_id = i
                    print(
                        f"âœ“ NVIDIA GPU detected: {gpu_name} ({
                            torch.cuda.get_device_properties(i).total_memory /
                            1024**3:.1f} GB)")
                    break

            # If no NVIDIA GPU found but CUDA is available, use first CUDA
            # device
            if device == 'cpu' and torch.cuda.device_count() > 0:
                device = 'cuda'
                cuda_device_id = 0
                print(
                    f"âš  Using CUDA device 0: {
                        torch.cuda.get_device_name(0)} (may be Intel integrated GPU)")
        else:
            print("âš  No CUDA GPU detected - using CPU (will be much slower)")

        # Use YOLO pose model if foot-based tracking is enabled (for foot keypoint detection)
        # Otherwise use detection model
        if foot_based_tracking:
            print("Foot-based tracking enabled: Loading YOLO pose model for foot keypoint detection...")
            # Try YOLOv11 pose first (faster and more efficient), fallback to YOLOv8 pose
            try:
                print("Loading YOLOv11 pose model (faster, more efficient)...")
                model = YOLO('yolo11n-pose.pt')
                # Move model to specific GPU if available
                if device == 'cuda' and cuda_device_id is not None:
                    torch.cuda.set_device(cuda_device_id)
                    model.to(f'cuda:{cuda_device_id}')
                    torch.backends.cudnn.benchmark = True
                    torch.backends.cudnn.deterministic = False
                    print(f"âœ“ YOLOv11 pose loaded on NVIDIA GPU: {torch.cuda.get_device_name(cuda_device_id)}")
                elif device == 'cuda':
                    model.to('cuda')
                    torch.backends.cudnn.benchmark = True
                    torch.backends.cudnn.deterministic = False
                    print(f"âœ“ YOLOv11 pose loaded on GPU: {torch.cuda.get_device_name(0)}")
                else:
                    print(f"âœ“ YOLOv11 pose loaded on CPU")
                print("  â†’ Pose model loaded: Foot keypoints will be extracted for tracking")
            except Exception as e:
                print(f"YOLOv11 pose not available ({e}), using YOLOv8 pose...")
                try:
                    model = YOLO('yolov8n-pose.pt')
                    # Move model to specific GPU if available
                    if device == 'cuda' and cuda_device_id is not None:
                        torch.cuda.set_device(cuda_device_id)
                        model.to(f'cuda:{cuda_device_id}')
                        torch.backends.cudnn.benchmark = True
                        torch.backends.cudnn.deterministic = False
                        print(f"âœ“ YOLOv8 pose loaded on NVIDIA GPU: {torch.cuda.get_device_name(cuda_device_id)}")
                    elif device == 'cuda':
                        model.to('cuda')
                        torch.backends.cudnn.benchmark = True
                        torch.backends.cudnn.deterministic = False
                        print(f"âœ“ YOLOv8 pose loaded on GPU: {torch.cuda.get_device_name(0)}")
                    else:
                        print(f"âœ“ YOLOv8 pose loaded on CPU")
                    print("  â†’ Pose model loaded: Foot keypoints will be extracted for tracking")
                except Exception as e2:
                    print(f"âš  YOLO pose models not available ({e2}), falling back to detection model")
                    print("  Foot-based tracking will use bounding box bottom instead of keypoints")
                    foot_based_tracking = False  # Disable if pose models unavailable
                    model = YOLO('yolo11m.pt')  # Medium model for better accuracy
        else:
            # Use detection model when foot-based tracking is disabled
            try:
                print("Loading YOLOv11 medium model (better accuracy, balanced speed)...")
                model = YOLO('yolo11m.pt')  # Medium model for better accuracy
                # Move model to specific GPU if available
                if device == 'cuda' and cuda_device_id is not None:
                    # Set CUDA device before moving model
                    torch.cuda.set_device(cuda_device_id)
                    model.to(f'cuda:{cuda_device_id}')
                    # Verify model is on GPU
                    try:
                        next(model.model.parameters()).device
                        print(
                            f"âœ“ YOLOv11 loaded on NVIDIA GPU: {
                                torch.cuda.get_device_name(cuda_device_id)}")
                        # Enable optimizations
                        torch.backends.cudnn.benchmark = True  # Optimize for consistent input sizes
                        torch.backends.cudnn.deterministic = False  # Allow non-deterministic for speed
                    except:
                        print(
                            f"âœ“ YOLOv11 loaded on NVIDIA GPU: {
                                torch.cuda.get_device_name(cuda_device_id)}")
                elif device == 'cuda':
                    model.to('cuda')
                    torch.backends.cudnn.benchmark = True
                    torch.backends.cudnn.deterministic = False
                    print(
                        f"âœ“ YOLOv11 loaded on GPU: {
                            torch.cuda.get_device_name(0)}")
                else:
                    print(f"âœ“ YOLOv11 loaded on CPU")
            except Exception as e:
                # Only fallback to YOLOv8 if we're not using pose models
                print(f"YOLOv11 not available ({e}), using YOLOv8...")
                model = YOLO('yolov8n.pt')
            # Move model to specific GPU if available
            if device == 'cuda' and cuda_device_id is not None:
                # Set CUDA device before moving model
                torch.cuda.set_device(cuda_device_id)
                model.to(f'cuda:{cuda_device_id}')
                # Enable optimizations
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
                print(
                    f"âœ“ YOLOv8 loaded on NVIDIA GPU: {
                        torch.cuda.get_device_name(cuda_device_id)}")
            elif device == 'cuda':
                model.to('cuda')
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
                print(
                    f"âœ“ YOLOv8 loaded on GPU: {
                        torch.cuda.get_device_name(0)}")
            else:
                print(f"âœ“ YOLOv8 loaded on CPU")
        # Initialize tracker (ByteTrack or OC-SORT) with optimized parameters for fast-moving players
        # For 11 players + coach + ball, we need:
        # - Lower activation threshold to catch all players (especially when partially occluded)
        # - Higher lost_track_buffer to maintain IDs during fast movement
        # - Slightly lower matching threshold for high FPS to prevent rapid ID loss
        # Use GUI match_thresh directly (no hardcoded adjustments)
        # SUGGESTED: For better track stability and less player reassignment:
        #   - match_thresh: 0.4-0.5 for high FPS (120fps) - more lenient = tracks reconnect more easily
        #   - match_thresh: 0.5-0.6 for medium FPS (30-60fps)
        #   - match_thresh: 0.6-0.7 for low FPS (<30fps)
        #   Lower threshold = more lenient matching = less track loss = less new track creation
        adjusted_match_thresh = match_thresh
        # Don't multiply buffer - use the time-based buffer directly (already scaled by FPS)
        # track_buffer_scaled is already set correctly above based on
        # time_buffer_seconds

        # Choose tracker type
        tracker = None
        tracker_name = tracker_type.lower()
        
        # Check if BoxMOT tracker is requested
        if BOXMOT_AVAILABLE and tracker_name in ['deepocsort', 'strongsort', 'botsort', 'boxmot_ocsort', 'boxmot_bytetrack']:
            # Use BoxMOT tracker
            # Map tracker names to BoxMOT types
            boxmot_type = tracker_name
            if tracker_name == 'boxmot_ocsort':
                boxmot_type = 'ocsort'
            elif tracker_name == 'boxmot_bytetrack':
                boxmot_type = 'bytetrack'
            
            # Try to get Re-ID model path from reid_tracker if available
            # Note: reid_tracker may not be initialized yet, so we'll use default weights
            model_weights = None
            if REID_AVAILABLE and boxmot_type in ['deepocsort', 'strongsort', 'botsort']:
                # BoxMOT will download default OSNet weights if not provided
                # We can optionally try to use the same model path later if needed
                # For now, let BoxMOT use its default weights
                pass
            
            tracker = create_boxmot_tracker(
                tracker_type=boxmot_type,
                device=device,
                track_thresh=track_thresh,
                match_thresh=adjusted_match_thresh,
                track_buffer=track_buffer_scaled,
                min_track_length=min_track_length,
                model_weights=model_weights,
                fp16=True,
                osnet_variant=osnet_variant if 'osnet_variant' in locals() else 'osnet_x1_0'
            )
            
            if tracker:
                print(f"âœ“ Using BoxMOT {boxmot_type} tracker")
                if boxmot_type in ['deepocsort', 'strongsort', 'botsort']:
                    print(f"  â†’ Appearance-based tracking enabled (better occlusion handling)")
            else:
                print(f"âš  BoxMOT {boxmot_type} failed to initialize, falling back to ByteTrack")
                tracker_name = "bytetrack"  # Fallback
        
        # Fallback to standard trackers if BoxMOT not available or failed
        if tracker is None:
            if tracker_name == "ocsort" and OCSORT_AVAILABLE:
                # Use OC-SORT for better occlusion handling
                # CRITICAL FIX: Lower activation threshold to prevent blinking
                # BASIC SETUP: Use track_thresh directly (no complex adjustments)
                activation_thresh = track_thresh if track_thresh > 0 else 0.25  # BASIC: Default 0.25
                # Lower minimum threshold to catch more players (especially small/distant ones)
                # Minimum 0.15 allows detection of lower-confidence detections
                activation_thresh = max(0.15, activation_thresh)
                # Use GUI min_track_length directly (no hardcoded enforcement)
                # SUGGESTED: For better track stability and less player reassignment:
                #   - min_track_length: 3-5 frames (higher = new tracks must persist longer = less opportunity to replace existing players)
                #   - For 120fps: 3 frames = 0.025s, 5 frames = 0.042s
                tracker = OCSortTracker(
                    track_activation_threshold=activation_thresh,  # Balanced for persistence
                    minimum_matching_threshold=adjusted_match_thresh,
                    lost_track_buffer=track_buffer_scaled,
                    min_track_length=min_track_length,  # Use GUI value directly
                    max_age=track_buffer_scaled * 3,
                    # IMPROVED: Longer max age (was *2) for better persistence
                    iou_threshold=adjusted_match_thresh
                )
                # Store tracker parameters for reinitialization if needed (for OC-SORT error recovery)
                ocsort_tracker_params = {
                    'activation_thresh': activation_thresh,
                    'adjusted_match_thresh': adjusted_match_thresh,
                    'track_buffer_scaled': track_buffer_scaled,
                    'min_track_length': min_track_length
                }
                print(f"âœ“ Using OC-SORT tracker (better occlusion handling)")
            else:
                # Use ByteTrack (default or fallback)
                if tracker_name == "ocsort" and not OCSORT_AVAILABLE:
                    print("âš  OC-SORT requested but not available, falling back to ByteTrack")
                # BASIC SETUP: Use track_thresh directly (no complex adjustments)
                activation_thresh = track_thresh if track_thresh > 0 else 0.25  # BASIC: Default 0.25
                # Lower minimum threshold to catch more players (especially small/distant ones)
                # Minimum 0.15 allows detection of lower-confidence detections
                activation_thresh = max(0.15, activation_thresh)
                tracker = sv.ByteTrack(
                    track_activation_threshold=activation_thresh,  # Balanced for persistence
                    minimum_matching_threshold=adjusted_match_thresh,     # Adjusted for persistence
                    # FPS-scaled buffer for consistent tracking
                    lost_track_buffer=track_buffer_scaled
                )
                print(f"âœ“ Using ByteTrack tracker")

        # Initialize track post-processor (interpolation, NMS, lifecycle
        # management, enhanced occlusion recovery)
        post_processor = None
        if POSTPROCESSING_AVAILABLE:
            post_processor = TrackPostProcessor(
                # Allow interpolation for up to 1/3 of buffer
                max_gap_frames=max(5, int(track_buffer_scaled // 3)),
                nms_iou_threshold=0.5,  # Lower threshold to catch more duplicate detections (0.5 = 50% overlap)
                occlusion_recovery_seconds=occlusion_recovery_seconds,
                occlusion_recovery_distance=occlusion_recovery_distance,
                fps=fps
            )

        # BASIC SETUP: Simple logging (no complex FPS-based adjustments)
        print(
            f"BASIC TRACKING SETUP: Match threshold: {
                match_thresh:.2f} (from GUI), Buffer: {track_buffer_scaled} frames (~{
                track_buffer_scaled /
                fps:.1f}s at {
                fps:.1f}fps)")

        # Load team and ball colors for Re-ID integration
        team_colors = load_team_color_config()
        ball_colors = load_ball_color_config()
        if team_colors:
            team1_name = team_colors.get(
                'team_colors',
                {}).get(
                'team1',
                {}).get(
                'name',
                'Team 1')
            team2_name = team_colors.get(
                'team_colors',
                {}).get(
                'team2',
                {}).get(
                'name',
                'Team 2')
            print(f"âœ“ Loaded team colors: {team1_name} vs {team2_name}")
            # Verify team colors have HSV ranges configured
            team1_has_ranges = team_colors.get(
                'team_colors', {}).get(
                'team1', {}).get('hsv_ranges') is not None
            team2_has_ranges = team_colors.get(
                'team_colors', {}).get(
                'team2', {}).get('hsv_ranges') is not None
            if not team1_has_ranges or not team2_has_ranges:
                print(
                    f"âš  Warning: Team colors loaded but HSV ranges missing. Team classification may not work.")
                print(
                    f"  Team1 has ranges: {team1_has_ranges}, Team2 has ranges: {team2_has_ranges}")
                print(
                    f"  Use 'Team Color Helper' to configure HSV ranges for team classification.")
        if ball_colors:
            print(f"âœ“ Loaded ball colors for Re-ID integration")

        # HARMONIC MEAN AUTO-ENABLE: If Harmonic Mean is selected, automatically enable Re-ID
        # Harmonic Mean requires Re-ID similarity scores to function properly
        if use_harmonic_mean and not use_reid:
            print("  â„¹ Harmonic Mean selected - automatically enabling Re-ID (required for Harmonic Mean association)")
            use_reid = True
        
        # Initialize Re-ID tracker for better ID persistence
        if use_reid and REID_AVAILABLE:
            try:
                # Let ReIDTracker auto-detect the best device (it will test CUDA availability)
                # CRITICAL FIX: Lower similarity threshold for better reconnection
                # For young players who look similar, 0.5 might be too strict
                # Lower threshold = more lenient matching = better reconnection
                # ENHANCED: Lower similarity threshold more aggressively for better reconnection
                # Lower by 0.15 instead of 0.1, with minimum of 0.25 (was 0.3)
                adaptive_similarity_threshold = max(
                    0.25, reid_similarity_threshold - 0.15)  # Lower by 0.15, minimum 0.25
                # Store initial threshold for HOTA-guided adjustments
                initial_reid_threshold = adaptive_similarity_threshold
                reid_tracker = ReIDTracker(
                    enable_adaptive_thresholds=True,
                    enable_multi_frame_verification=True,
                    enable_quality_weighting=True,
                    enable_negative_filtering=True,
                    enable_position_verification=True,
                    feature_dim=128,
                    # More lenient for better reconnection
                    similarity_threshold=adaptive_similarity_threshold,
                    max_features_per_track=50,
                    use_torchreid=True,
                    osnet_variant=osnet_variant if 'osnet_variant' in locals() else 'osnet_x1_0',
                    use_boxmot_backend=use_boxmot_backend if 'use_boxmot_backend' in locals() else True,
                    # Auto-detect (will test CUDA and fallback to CPU if
                    # needed)
                    device=None
                )
                if adaptive_similarity_threshold < reid_similarity_threshold:
                    print(
                        f"  â†’ Using adaptive Re-ID similarity threshold: {
                            adaptive_similarity_threshold:.2f} (lowered from {
                            reid_similarity_threshold:.2f} for better reconnection)")
                print(
                    "âœ“ Re-ID tracker enabled - better ID persistence during occlusions")
            except Exception as e:
                print(f"âš  Could not initialize Re-ID tracker: {e}")
                print("  Continuing without Re-ID features...")
                reid_tracker = None
        else:
            if use_reid and not REID_AVAILABLE:
                logger.warning("âš  Re-ID requested but not available. Install torch: pip install torch")
            reid_tracker = None

        # Initialize HOTA-guided tracker for real-time quality monitoring and adjustments
        # Advanced features are passed as function parameters (use_harmonic_mean, use_expansion_iou)
        
        hota_guided_tracker = None
        if HOTA_GUIDED_AVAILABLE and use_reid and reid_tracker is not None:
            try:
                hota_guided_tracker = HOTAGuidedTracker(
                    window_size=100,  # Monitor last 100 frames
                    min_hota_threshold=0.5,  # Trigger adjustments if HOTA < 0.5
                    use_harmonic_mean=use_harmonic_mean,  # Harmonic Mean association (Deep HM-SORT)
                    use_expansion_iou=use_expansion_iou  # Expansion IOU with motion prediction (Deep HM-SORT)
                )
                print("âœ“ HOTA-guided tracking enabled - will monitor quality and adjust Re-ID thresholds")
            except Exception as e:
                print(f"âš  Could not initialize HOTA-guided tracker: {e}")
                hota_guided_tracker = None
        
        # Enable soccer-specific Re-ID training if requested
        if enable_soccer_reid_training and reid_tracker is not None:
            try:
                reid_tracker.enable_soccer_specific_training()
                print("âœ“ Soccer-specific Re-ID training enabled - training data will be collected")
            except Exception as e:
                print(f"âš  Could not enable soccer Re-ID training: {e}")
        
        # Initialize Player Gallery for cross-video identification
        # Note: player_gallery was initialized earlier (line 3563) to prevent UnboundLocalError
        print(f"ðŸ” DEBUG: Checking Player Gallery initialization...")
        print(f"   use_reid={use_reid}, GALLERY_AVAILABLE={GALLERY_AVAILABLE}, reid_tracker={reid_tracker is not None}")
        
        if use_reid and GALLERY_AVAILABLE and reid_tracker is not None:
            try:
                print("   Attempting to load Player Gallery...")
                player_gallery = PlayerGallery()
                stats = player_gallery.get_stats()
                if stats['total_players'] > 0:
                    print(f"âœ“ Player Gallery loaded: {stats['total_players']} players (cross-video ID enabled)")
                    print(f"  Players with features: {stats['players_with_features']}")
                    # List player names for visibility
                    all_players = player_gallery.list_players()
                    # list_players() returns tuples: (player_id, player_name)
                    player_list = ", ".join([p[1] for p in all_players[:5]])  # p[1] is the name
                    if len(all_players) > 5:
                        player_list += f" (and {len(all_players) - 5} more)"
                    print(f"  Gallery players: {player_list}")
                    
                    # CRITICAL: Remove incorrect gallery references for players not in this video
                    # Ellie Hill and Cameron Hill were likely incorrectly matched in this video
                    excluded_players = ['Ellie Hill', 'Cameron Hill']
                    if input_path and excluded_players:
                        try:
                            frames_removed, players_cleaned = player_gallery.remove_reference_frames_for_players_from_video(
                                video_path=input_path,
                                player_names=excluded_players
                            )
                            if frames_removed > 0:
                                print(f"  ðŸ§¹ Cleaned gallery: Removed {frames_removed} incorrect reference frames for {players_cleaned} player(s) from this video")
                        except Exception as cleanup_error:
                            print(f"  âš  Could not clean gallery references: {cleanup_error}")
                    
                    # PERFORMANCE FIX: If gallery has no features, skip gallery matching to improve speed
                    if stats['players_with_features'] == 0:
                        print("â„¹ Gallery has no learned features yet - disabling gallery matching for faster processing")
                        print("  â†’ Re-ID tracking will still work for occlusion handling")
                        print("  â†’ Tag players with anchor frames to build the gallery")
                        player_gallery = None  # Disable gallery matching until features are learned
                else:
                    if watch_only:
                        print("â„¹ Player Gallery is empty. Will learn new players during watch-only mode.")
                    else:
                        print("â„¹ Player Gallery is empty. Use 'player_gallery_seeder.py' to add players.")
                        player_gallery = None  # Don't use empty gallery (unless in watch-only mode)
            except Exception as e:
                print(f"âš  Could not initialize Player Gallery: {e}")
                import traceback
                traceback.print_exc()
                player_gallery = None
        else:
            if use_reid and not GALLERY_AVAILABLE:
                print("â„¹ Player Gallery not available (player_gallery.py not found)")
            elif not use_reid:
                print("â„¹ Player Gallery disabled (Re-ID not enabled)")
            elif reid_tracker is None:
                print("â„¹ Player Gallery disabled (Re-ID tracker failed to initialize)")

        # Initialize advanced recognition modules
        jersey_ocr = None
        # ENHANCED: Jersey OCR works independently of Re-ID - it can identify players by jersey numbers
        # even when Re-ID features are weak (e.g., small/far players). Enable whenever player tracking is on.
        if JERSEY_OCR_AVAILABLE and track_players_flag:
            try:
                jersey_ocr = JerseyNumberOCR(ocr_backend="auto", confidence_threshold=0.5, preprocess=True)
                logger.info("âœ“ Jersey Number OCR initialized (independent of Re-ID)")
            except Exception as e:
                logger.warning(f"âš  Could not initialize Jersey OCR: {e}")
                jersey_ocr = None
        elif track_players_flag and not JERSEY_OCR_AVAILABLE:
            logger.debug("â„¹ Jersey Number OCR not available - jersey number recognition disabled")

        gait_analyzer = None
        if GAIT_ANALYZER_AVAILABLE and use_reid and foot_based_tracking and track_players_flag:
            try:
                gait_analyzer = GaitAnalyzer(history_length=30, min_samples_for_gait=10)
                logger.info("âœ“ Gait Analyzer initialized")
            except Exception as e:
                logger.warning(f"âš  Could not initialize Gait Analyzer: {e}")
                gait_analyzer = None
        elif use_reid and foot_based_tracking and track_players_flag and not GAIT_ANALYZER_AVAILABLE:
            logger.debug("â„¹ Gait Analyzer not available - gait analysis features disabled")

        hard_negative_miner = None
        if HARD_NEGATIVE_AVAILABLE and use_reid and player_gallery is not None and track_players_flag:
            try:
                hard_negative_miner = HardNegativeMiner(
                    max_hard_negatives=50,
                    similarity_threshold=0.4,
                    max_similarity=0.7,
                    min_confidence=0.3
                )
                logger.info("âœ“ Hard Negative Miner initialized")
            except Exception as e:
                logger.warning(f"âš  Could not initialize Hard Negative Miner: {e}")
                hard_negative_miner = None
        elif use_reid and player_gallery is not None and track_players_flag and not HARD_NEGATIVE_AVAILABLE:
            logger.debug("â„¹ Hard Negative Mining not available - hard negative mining features disabled")

        graph_tracker = None
        if GRAPH_TRACKER_AVAILABLE and use_reid and track_players_flag:
            try:
                graph_tracker = GraphTracker(
                    position_grid_size=(10, 10),
                    max_nodes_per_type=1000,
                    edge_decay_rate=0.95,
                    min_edge_weight=0.1
                )
                logger.info("âœ“ Graph Tracker initialized")
            except Exception as e:
                logger.warning(f"âš  Could not initialize Graph Tracker: {e}")
                graph_tracker = None
        elif use_reid and track_players_flag and not GRAPH_TRACKER_AVAILABLE:
            logger.debug("â„¹ Graph Tracker not available - graph-based tracking features disabled")

        # Initialize annotators based on visualization style
        # Configure box thickness and color
        box_annotator_kwargs = {
            'thickness': box_thickness
        }
        # If box_color is specified, use it; otherwise use team colors (handled in annotate())
        if box_color is not None:
            box_annotator_kwargs['color'] = sv.Color(r=box_color[2], g=box_color[1], b=box_color[0])  # BGR to RGB
        
        if viz_style == "circle":
            box_annotator = None  # Don't use boxes
            circle_annotator = sv.CircleAnnotator()  # radius is set in annotate() call
        elif viz_style == "both":
            box_annotator = sv.BoxAnnotator(**box_annotator_kwargs)
            circle_annotator = sv.CircleAnnotator()  # radius is set in annotate() call
        else:  # "box" or default
            box_annotator = sv.BoxAnnotator(**box_annotator_kwargs)
            circle_annotator = None

        # Initialize label annotator with TRACK color lookup to avoid class_id issues
        # CRITICAL FIX: Use INDEX color lookup instead of TRACK to prevent supervision from assigning random colors
        # We'll manually set colors based on team or use a single subtle color
        label_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)

        # Team colors are already loaded above for Re-ID integration
        # If not loaded (Re-ID disabled), load for visualization only
        if team_colors is None and viz_color_mode == "team":
            team_colors = load_team_color_config()
            if team_colors:
                team1_name = team_colors.get(
                    'team_colors',
                    {}).get(
                    'team1',
                    {}).get(
                    'name',
                    'Team 1')
                team2_name = team_colors.get(
                    'team_colors',
                    {}).get(
                    'team2',
                    {}).get(
                    'name',
                    'Team 2')
                print(f"âœ“ Loaded team colors: {team1_name} vs {team2_name}")
            else:
                print(
                    "âš  Team color mode selected but no team colors configured. Using default colors.")
                print("  Use 'Team Color Helper' to configure team colors.")

    # Create video writer
    # Determine output frame rate
    # If output_fps is None, use input fps. Otherwise, use specified output_fps
    # CRITICAL FIX: Ensure output FPS exactly matches input FPS to prevent slow-motion playback
    output_fps_value = output_fps if output_fps is not None else fps
    # Ensure FPS is a float and matches input exactly when not specified
    if output_fps is None:
        output_fps_value = float(fps)  # Use exact input FPS
    else:
        output_fps_value = float(output_fps)  # Use specified output FPS
    
    if abs(output_fps_value - fps) > 0.01:  # Allow small floating point differences
        print(
            f"Output frame rate: {
                output_fps_value:.3f}fps (input: {
                fps:.3f}fps)")
        print(
            f"  â†’ Video will play at {
                output_fps_value:.3f}fps, but tracking uses all {
                fps:.3f}fps frames")
    else:
        print(f"Output frame rate: {output_fps_value:.3f}fps (same as input: {fps:.3f}fps)")

    # Frame skipping for output (but still process all frames for tracking)
    output_frame_skip = max(1, int(fps / output_fps_value)
                            ) if output_fps_value < fps else 1
    frames_to_write = []  # Queue frames to write at output rate

    # WATCH-ONLY MODE: Skip video writing, only learn from video
    live_viewer_window = None
    
    # DYNAMIC SETTINGS: Shared state for pause/resume and real-time parameter updates
    # This allows settings to be changed during processing
    class DynamicSettings:
        def __init__(self):
            self.paused = False
            self.should_resume = False
            self.settings_updated = False
            # Store original settings
            self.track_ball_flag = track_ball_flag
            self.show_ball_trail = show_ball_trail
            self.trail_length = trail_length
            self.trail_buffer = trail_buffer
            self.viz_style = viz_style
            self.viz_color_mode = viz_color_mode
            self.ellipse_width = ellipse_width
            self.ellipse_height = ellipse_height
            self.ellipse_outline_thickness = ellipse_outline_thickness
            self.box_thickness = box_thickness
            self.box_color = box_color
            self.label_color = label_color if 'label_color' in locals() else None
            self.show_player_labels = show_player_labels
            self.label_font_scale = label_font_scale
            self.label_type = label_type if 'label_type' in locals() else "full_name"
            self.label_custom_text = label_custom_text if 'label_custom_text' in locals() else "Player"
            self.label_font_face = label_font_face if 'label_font_face' in locals() else "FONT_HERSHEY_SIMPLEX"
            self.ball_min_radius = ball_min_radius
            self.ball_max_radius = ball_max_radius
            self.player_viz_alpha = player_viz_alpha if 'player_viz_alpha' in locals() else 255
            # Get prediction settings from function parameters
            self.show_predicted_boxes = show_predicted_boxes if 'show_predicted_boxes' in locals() else True
            self.prediction_duration = prediction_duration if 'prediction_duration' in locals() else 0.3
            self.prediction_style = prediction_style
            self.prediction_size = prediction_size
            self.prediction_color = prediction_color
            # SEPARATE CONTROLS: Bounding boxes vs Circles at feet (user requested)
            self.show_bounding_boxes = show_bounding_boxes if 'show_bounding_boxes' in locals() else True
            self.show_circles_at_feet = show_circles_at_feet if 'show_circles_at_feet' in locals() else True
            # Enhanced feet marker visualization
            self.feet_marker_style = feet_marker_style if 'feet_marker_style' in locals() else "circle"
            self.feet_marker_opacity = feet_marker_opacity if 'feet_marker_opacity' in locals() else 255
            self.feet_marker_enable_glow = feet_marker_enable_glow if 'feet_marker_enable_glow' in locals() else False
            self.feet_marker_glow_intensity = feet_marker_glow_intensity if 'feet_marker_glow_intensity' in locals() else 50
            self.feet_marker_enable_shadow = feet_marker_enable_shadow if 'feet_marker_enable_shadow' in locals() else False
            self.feet_marker_shadow_offset = feet_marker_shadow_offset if 'feet_marker_shadow_offset' in locals() else 3
            self.feet_marker_shadow_opacity = feet_marker_shadow_opacity if 'feet_marker_shadow_opacity' in locals() else 128
            self.feet_marker_enable_gradient = feet_marker_enable_gradient if 'feet_marker_enable_gradient' in locals() else False
            self.feet_marker_enable_pulse = feet_marker_enable_pulse if 'feet_marker_enable_pulse' in locals() else False
            self.feet_marker_pulse_speed = feet_marker_pulse_speed if 'feet_marker_pulse_speed' in locals() else 2.0
            self.feet_marker_enable_particles = feet_marker_enable_particles if 'feet_marker_enable_particles' in locals() else False
            self.feet_marker_particle_count = feet_marker_particle_count if 'feet_marker_particle_count' in locals() else 5
        
        def update_settings(self, **kwargs):
            """Update settings dynamically"""
            for key, value in kwargs.items():
                if hasattr(self, key):
                    setattr(self, key, value)
                    self.settings_updated = True
        
        def get_current_settings(self):
            """Get current settings as dict"""
            return {
                'track_ball_flag': self.track_ball_flag,
                'show_ball_trail': self.show_ball_trail,
                'trail_length': self.trail_length,
                'trail_buffer': self.trail_buffer,
                'viz_style': self.viz_style,
                'viz_color_mode': self.viz_color_mode,
                'ellipse_width': self.ellipse_width,
                'ellipse_height': self.ellipse_height,
                'ellipse_outline_thickness': self.ellipse_outline_thickness,
                'box_thickness': self.box_thickness,
                'box_color': self.box_color,
                'label_color': self.label_color,
                'show_player_labels': self.show_player_labels,
                'label_font_scale': self.label_font_scale,
                'label_type': self.label_type,
                'label_custom_text': self.label_custom_text,
                'label_font_face': self.label_font_face,
                'ball_min_radius': self.ball_min_radius,
                'ball_max_radius': self.ball_max_radius,
                'player_viz_alpha': self.player_viz_alpha,
                'show_predicted_boxes': self.show_predicted_boxes,
                'prediction_duration': self.prediction_duration,
                'prediction_style': self.prediction_style,
                'prediction_size': self.prediction_size,
                'prediction_color': self.prediction_color,
                'show_bounding_boxes': self.show_bounding_boxes,
                'show_circles_at_feet': self.show_circles_at_feet,
                'feet_marker_style': self.feet_marker_style,
                'feet_marker_opacity': self.feet_marker_opacity,
                'feet_marker_enable_glow': self.feet_marker_enable_glow,
                'feet_marker_glow_intensity': self.feet_marker_glow_intensity,
                'feet_marker_enable_shadow': self.feet_marker_enable_shadow,
                'feet_marker_shadow_offset': self.feet_marker_shadow_offset,
                'feet_marker_shadow_opacity': self.feet_marker_shadow_opacity,
                'feet_marker_enable_gradient': self.feet_marker_enable_gradient,
                'feet_marker_enable_pulse': self.feet_marker_enable_pulse,
                'feet_marker_pulse_speed': self.feet_marker_pulse_speed,
                'feet_marker_enable_particles': self.feet_marker_enable_particles,
                'feet_marker_particle_count': self.feet_marker_particle_count,
            }
    
    # Initialize dynamic_settings for watch-only mode with live viewer OR for conflict resolution during normal tracking
    # ENHANCED: Always create dynamic_settings if player tracking is enabled (for conflict resolution)
    # This allows live viewer/conflict resolution to work during normal analysis, not just watch-only mode
    dynamic_settings = DynamicSettings() if (watch_only and show_live_viewer) or track_players_flag or (show_live_viewer and not watch_only) else None
    
    # Store dynamic_settings globally so GUI can access it
    # This allows the GUI control window to modify settings during processing
    if dynamic_settings:
        # Use shared_state module for thread-safe access
        try:
            import shared_state
            shared_state.set_dynamic_settings(dynamic_settings)
            if watch_only and show_live_viewer:
                print("ðŸ“º Live viewer controls: dynamic_settings shared with GUI via shared_state")
                print(f"   â†’ Watch-only: {watch_only}, Show live viewer: {show_live_viewer}")
                print("   â†’ GUI should open Live Viewer Controls window automatically")
            elif track_players_flag and not preview_mode:
                print("ðŸ“º Conflict resolution: dynamic_settings shared with GUI via shared_state")
                print(f"   â†’ Player tracking enabled (watch_only={watch_only}) - conflict resolution available via GUI button")
                print("   â†’ Use 'Open Conflict Resolution' button in GUI to manually open controls window")
            elif track_players_flag and preview_mode:
                print("â„¹ Conflict resolution: Disabled during preview mode")
            print("   â†’ If window doesn't open, check GUI log for error messages")
        except ImportError as e:
            print(f"âš  Could not import shared_state: {e}")
            # Fallback to module-level variable
            try:
                import combined_analysis_optimized as analysis_module
                analysis_module._current_dynamic_settings = dynamic_settings
                print("ðŸ“º Live viewer controls: dynamic_settings stored (fallback method)")
                print("   â†’ GUI should open Live Viewer Controls window automatically")
            except Exception as e2:
                print(f"âš  Could not store dynamic_settings: {e2}")
    else:
        if watch_only and not show_live_viewer:
            print("âš  Live viewer controls: watch_only=True but show_live_viewer=False - controls window will not open")
        elif track_players_flag and watch_only:
            print("â„¹ Conflict resolution: Available in watch-only mode if 'Show Live Viewer' is enabled")
    
    # Helper functions to get current settings (dynamic if available, otherwise original)
    def get_setting(name, default_value):
        """
        Get current setting value, using dynamic_settings if available.
        
        BOUNDING BOX COLOR FLOW:
        1. GUI: User sets box_color_r, box_color_g, box_color_b (0-255) and use_custom_box_color checkbox
        2. Analysis Start: box_color computed as (B, G, R) tuple if use_custom_box_color=True, else None
        3. DynamicSettings: box_color stored in DynamicSettings.box_color (updated via shared_state from GUI)
        4. During Processing: get_setting('box_color', box_color) retrieves current value:
           - If dynamic_settings exists and has box_color: use that (allows live updates)
           - Otherwise: use original box_color parameter (static from analysis start)
        5. Drawing: box_color_to_use = current_box_color if current_box_color else color (team color)
        
        POTENTIAL CONFLICTS:
        - Multiple drawing paths: preview/batch (line ~11651), main video (line ~12671), legacy style
        - DynamicSettings updates via shared_state but only checked when get_setting() is called
        - For smoother playback: playback viewer should check settings every frame (currently ~30fps)
        """
        if dynamic_settings and hasattr(dynamic_settings, name):
            return getattr(dynamic_settings, name)
        return default_value
    
    def get_label_text(player_name, track_id, team, label_type, label_custom_text, player_names_dict=None):
        """
        Generate label text based on label type
        
        Args:
            player_name: Full player name (e.g., "John Smith" or "#123")
            track_id: Track ID number
            team: Team name (e.g., "Gray", "Blue") or None
            label_type: "full_name", "last_name", "jersey", "team", "custom"
            label_custom_text: Custom text when label_type="custom"
            player_names_dict: Optional dict to look up jersey numbers {track_id: player_name}
        
        Returns:
            Label text string
        """
        # CRITICAL FIX: Ensure player_name is always a string
        if isinstance(player_name, list) and len(player_name) > 0:
            player_name = str(player_name[0])
        elif player_name:
            player_name = str(player_name)
        else:
            player_name = ""
        
        if label_type == "custom":
            return label_custom_text or "Player"
        
        elif label_type == "team":
            return team or "Unknown"
        
        elif label_type == "jersey":
            # Try to extract jersey number from player_name
            # Format might be "John Smith #10" or just "#10" or "Player #10"
            if "#" in player_name:
                # Extract jersey number
                parts = player_name.split("#")
                if len(parts) > 1:
                    jersey_part = parts[-1].strip()
                    # Get just the number part
                    jersey_num = ""
                    for char in jersey_part:
                        if char.isdigit():
                            jersey_num += char
                        elif jersey_num:  # Stop at first non-digit after digits
                            break
                    if jersey_num:
                        return f"#{jersey_num}"
            # Fallback: use track_id as jersey
            display_id = int(track_id) % 100
            return f"#{display_id}"
        
        elif label_type == "last_name":
            # Extract last name from full name
            if player_name.startswith("#"):
                # No name, use ID
                display_id = int(track_id) % 100
                return f"#{display_id}"
            # Remove any ID suffix and get last name
            name_part = player_name.split("(")[0].strip()
            name_parts = name_part.split()
            if len(name_parts) > 1:
                return name_parts[-1]  # Last name
            elif len(name_parts) == 1:
                return name_parts[0]  # Only one name, use it
            else:
                display_id = int(track_id) % 100
                return f"#{display_id}"
        
        else:  # full_name (default)
            if player_name.startswith("#"):
                # No name, just ID
                display_id = int(track_id) % 100
                return f"#{display_id}"
            else:
                # Show full name (remove any ID suffix that might be there)
                return player_name.split("(")[0].strip()
    
    # Convert focused_players to set for fast lookup
    focused_players_set = None
    if focused_players and len(focused_players) > 0:
        focused_players_set = set(focused_players)
    
    # SAFETY CHECK: If user wants CSV export or video output, disable watch-only mode
    # This prevents accidental watch-only mode when user expects output
    # NOTE: Player tracking is allowed in watch-only mode (for learning), but CSV/video export is not
    # NOTE: Live viewer can still be enabled even when watch-only is disabled
    if watch_only and export_csv:
        print("âš  WARNING: Watch-only mode was enabled but CSV export is requested.")
        print("   â†’ Disabling watch-only mode to allow CSV export.")
        print("   â†’ If you want watch-only mode, uncheck 'Export CSV'.")
        print("   â†’ Live viewer window will still open if 'Show Live Viewer' is enabled.")
        watch_only = False
    
    # Initialize live viewer window if requested (works in both watch-only and normal mode)
    # DISABLED: Auto-opening conflict resolution window during analysis
    # Users can manually open conflict resolution via GUI button when needed
    # BUT: Don't open conflict resolution window during previews
    live_viewer_window = None
    should_show_video = show_live_viewer  # Only show video if live viewer is explicitly enabled (conflict resolution must be opened manually)
    
    # OPTIMIZATION: Viewer downscaling for performance
    viewer_downscale_target = None
    if viewer_downscale == "720p":
        viewer_downscale_target = (1280, 720)
    elif viewer_downscale == "1080p":
        viewer_downscale_target = (1920, 1080)
    # "none" means no downscaling (full resolution)
    
    # OPTIMIZATION: Threaded viewer for non-blocking display
    viewer_queue = None
    viewer_thread = None
    viewer_thread_running = False
    if viewer_threaded and should_show_video:
        try:
            from queue import Queue
            import threading
            viewer_queue = Queue(maxsize=10)  # Buffer up to 10 frames
            viewer_thread_running = True
            
            def viewer_thread_func(q, window_name):
                """Non-blocking viewer thread"""
                while viewer_thread_running:
                    try:
                        frame_data = q.get(timeout=0.1)
                        if frame_data is None:  # Shutdown signal
                            break
                        frame, key_callback = frame_data
                        if frame is not None:
                            cv2.imshow(window_name, frame)
                            key = cv2.waitKey(1) & 0xFF
                            if key_callback:
                                key_callback(key)
                    except:
                        continue
            
            logger.info("ðŸ“º Threaded viewer enabled - non-blocking display")
        except ImportError:
            logger.warning("âš  Threading not available - falling back to blocking viewer")
            viewer_threaded = False
    
    if should_show_video:
        try:
            # Use appropriate window name based on mode
            if watch_only:
                window_name = "Watch-Only Learning Mode"
            else:
                window_name = "Live Video Analysis"
            cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
            # Set initial size but allow user to resize
            # Scale down for initial display (4K is too large for most screens)
            display_width = min(width, 1920)  # Cap at 1920px width
            display_height = int(height * (display_width / width))  # Maintain aspect ratio
            cv2.resizeWindow(window_name, display_width, display_height)
            # Enable window to be resizable and maintain aspect ratio
            cv2.setWindowProperty(window_name, cv2.WND_PROP_ASPECT_RATIO, cv2.WINDOW_KEEPRATIO)
            # Try to bring window to front (Windows-specific)
            try:
                import platform
                if platform.system() == "Windows":
                    import ctypes
                    hwnd = ctypes.windll.user32.FindWindowW(None, window_name)
                    if hwnd:
                        ctypes.windll.user32.SetForegroundWindow(hwnd)
                        ctypes.windll.user32.ShowWindow(hwnd, 9)  # SW_RESTORE
            except:
                pass  # Silently fail if can't bring to front
            live_viewer_window = window_name
            
            # Start threaded viewer if enabled
            if viewer_threaded and viewer_queue is not None:
                viewer_thread = threading.Thread(target=viewer_thread_func, args=(viewer_queue, window_name), daemon=True)
                viewer_thread.start()
                logger.info("âœ“ Viewer thread started")
            
            if watch_only:
                mode_text = "watch-only learning"
            else:
                mode_text = "analysis"
            print(f"ðŸ“º Live viewer window opened - watching {mode_text} in real-time!")
            print(f"   â†’ Window: '{window_name}'")
            print(f"   â†’ Window size: {display_width}x{display_height} (resizable)")
            if viewer_downscale_target:
                print(f"   â†’ Downscaling: {viewer_downscale} ({viewer_downscale_target[0]}x{viewer_downscale_target[1]}) for performance")
            if viewer_threaded:
                print(f"   â†’ Threaded viewer: Enabled (non-blocking)")
            print("   â†’ Press SPACEBAR to pause/resume")
            print("   â†’ Press 'r' to resume (when paused)")
            print("   â†’ Press 'q' to close viewer")
            print("   â†’ Settings can be updated while paused")
            print("   â†’ Window is resizable - drag corners to resize")
            
            # Helper function for optimized viewer display
            def display_viewer_frame(frame, key_callback=None):
                """Display frame in viewer with downscaling and threading optimizations"""
                if frame is None or live_viewer_window is None:
                    return None
                
                display_frame = frame.copy()
                
                # Apply downscaling for performance
                if viewer_downscale_target:
                    frame_h, frame_w = display_frame.shape[:2]
                    target_w, target_h = viewer_downscale_target
                    if frame_w > target_w or frame_h > target_h:
                        scale_w = target_w / frame_w
                        scale_h = target_h / frame_h
                        scale = min(scale_w, scale_h)
                        new_w = int(frame_w * scale)
                        new_h = int(frame_h * scale)
                        display_frame = cv2.resize(display_frame, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
                
                # Use threaded or blocking display
                if viewer_threaded and viewer_queue is not None:
                    try:
                        if not viewer_queue.full():
                            viewer_queue.put((display_frame, key_callback), block=False)
                        return None  # No key returned in threaded mode
                    except:
                        return None
                else:
                    # Blocking display
                    cv2.imshow(live_viewer_window, display_frame)
                    if key_callback:
                        key = cv2.waitKey(1) & 0xFF
                        return key
                    return None
            
            if track_players_flag and not watch_only and not preview_mode:
                print("   â†’ Use the Live Viewer Controls window to resolve player conflicts")
            print("   â†’ If window is not visible, check if it's behind other windows or try Alt+Tab")
        except Exception as e:
            print(f"âš  Could not create live viewer window: {e}")
            print("   â†’ Live viewer controls window will still work, but video display is unavailable")
            live_viewer_window = None
    
    if watch_only:
        print("=" * 60)
        print("ðŸ‘€ WATCH-ONLY MODE: Learning from video (no output will be saved)")
        print("=" * 60)
        print("   â†’ Processing video to learn:")
        print("      â€¢ Player features (shape, movement, position, behavior)")
        print("      â€¢ Team colors (HSV ranges)")
        print("      â€¢ Player gallery updates")
        if focused_players_set:
            print(f"   â†’ FOCUS MODE: Only learning from {len(focused_players_set)} selected player(s)")
            print(f"      â€¢ Focused players: {', '.join(sorted(focused_players_set))}")
            print("      â€¢ 5-10x faster processing (skipping non-focused players)")
        print("   â†’ No video output will be created")
        if show_live_viewer:
            print("   â†’ Live viewer window enabled (press 'q' to close)")
        print("   â†’ Learned data will be saved to:")
        print("      â€¢ player_gallery.json (auto-saved every 1000 frames)")
        print("      â€¢ team_color_config.json (auto-saved every 100 frames)")
        print("   â†’ Learning accumulates throughout the entire video")
        print("   â†’ Progress reports every 500 frames")
        print("=" * 60)
        out = None
        fourcc = None
        # Live viewer window already initialized above (works for both watch-only and normal mode)
    elif not enable_video_encoding:
        # Video encoding disabled - only export metadata/CSV
        print("â„¹ Video encoding disabled - will only export tracking data and overlay metadata")
        out = None
        fourcc = None
    else:
        # Use H.264 codec for better frame rate handling (fallback to mp4v if not available)
        # H.264 properly encodes frame rate metadata, preventing fast/flashing playback
        # CRITICAL FIX: Preserve original FPS to prevent fast playback in VLC
        # Don't cap FPS - let the video player handle high FPS videos correctly
        # Note: OpenH264 library warnings are harmless - codec selection will fallback automatically
        original_output_fps = output_fps_value
        # Log FPS for diagnostics
        if output_fps_value > 60:
            print(f"â„¹ Output FPS {output_fps_value:.1f} is high - video will play at original speed in VLC")
        # Keep original FPS - don't cap it
        # output_fps_value remains unchanged to preserve original playback speed
        
        # Try better codecs first (H.264/avc1 handle frame rate metadata better)
        # Note: OpenH264 library errors are expected and harmless - we fallback automatically
        # Note: os and sys are already imported at the top of the file
        
        # Ensure output directory exists
        output_dir = os.path.dirname(output_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
        
        # Suppress OpenH264 warnings by temporarily redirecting stderr
        # (These are just library loading messages, not actual errors)
        # Note: OpenH264 DLL errors are harmless - we fallback to avc1 which works without it
        old_stderr = sys.stderr
        devnull = None
        try:
            # Redirect stderr to suppress OpenH264 library loading messages
            devnull = open(os.devnull, 'w')
            sys.stderr = devnull
        except:
            pass
        
        fourcc = None
        out = None
        codec_options = ['avc1', 'H264', 'XVID', 'mp4v']  # avc1 is H.264, better for frame rate
        openh264_warning_shown = False
        selected_codec = None  # Track which codec was selected
        for codec in codec_options:
            try:
                test_fourcc = cv2.VideoWriter_fourcc(*codec)
                test_out = cv2.VideoWriter(output_path, test_fourcc, output_fps_value, (width, height))
                if test_out.isOpened():
                    fourcc = test_fourcc
                    out = test_out
                    selected_codec = codec  # Store selected codec for logging
                    print(f"âœ“ Using codec: {codec} @ {output_fps_value:.1f} fps")
                    break
                else:
                    test_out.release()
            except Exception as e:
                # Try next codec
                continue
        
        # Restore stderr after codec selection
        sys.stderr = old_stderr
        if devnull:
            try:
                devnull.close()
            except:
                pass
        
        # Note about OpenH264 (informational only - avc1 works fine without it)
        if not openh264_warning_shown and out is not None and out.isOpened():
            # OpenH264 errors are harmless - avc1 codec works without it
            # If you want to use OpenH264, place openh264-2.x.x-win64.dll in:
            # 1. Same directory as Python executable
            # 2. Or in a directory in your PATH
            # Note: Version 1.8.0 is outdated - use 2.4.0 or newer from:
            # https://github.com/cisco/openh264/releases
            pass
        
        # Fallback to mp4v if no codec worked
        if out is None or not out.isOpened():
            if out is not None:
                out.release()
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, output_fps_value, (width, height))
            print(f"Using fallback codec: mp4v @ {output_fps_value:.1f} fps")
        
        # Verify the writer is properly initialized
        if not out.isOpened():
            raise RuntimeError(f"Failed to open video writer for {output_path}. Tried codecs: {codec_options}")
        
        # CRITICAL: Log the actual FPS being used for video export
        print(f"âœ“ Video export: Input FPS={fps:.3f}, Output FPS={output_fps_value:.3f}, Codec={selected_codec if selected_codec else 'mp4v (fallback)'}")
        if abs(output_fps_value - fps) > 0.01:
            print(f"  âš  FPS mismatch: Output ({output_fps_value:.3f}) differs from input ({fps:.3f})")
            print(f"  â†’ This may cause playback speed issues in VLC")
        else:
            print(f"  âœ“ FPS matches input - video should play at correct speed in VLC")
        
        print(f"âœ“ Video writer initialized: {output_path}")
        print(f"   â†’ Resolution: {width}x{height}")
        print(f"   â†’ FPS: {output_fps_value:.3f} (input: {fps:.3f})")
        print(f"   â†’ Codec: {codec if 'codec' in locals() else 'mp4v'}")
        if abs(output_fps_value - fps) > 0.01:
            print(f"   âš  WARNING: Output FPS ({output_fps_value:.3f}) differs from input FPS ({fps:.3f})")
            print(f"   â†’ This may cause slow-motion or fast-motion playback")
        
        # Note about OpenH264 errors (shown earlier - these are harmless)
        # The "Failed to load OpenH264" errors are harmless - avc1 codec works fine without OpenH264
        # If you want to suppress these errors:
        # 1. Download openh264-2.4.0-win64.dll (or newer) from https://github.com/cisco/openh264/releases
        # 2. Place it in the same directory as your Python executable (NOT system32)
        #    To find Python location: python -c "import sys; print(sys.executable)"
        # 3. Note: Version 1.8.0 in system32 is outdated and causes "Incorrect library version" errors
        #    You need version 2.4.0 or newer, and it should be in Python's directory, not system32
        else:
            print(f"   âœ“ FPS matches input - video should play at correct speed")

    # Tracking data storage
    # Ball tracking: short-term trail (for visualization) and long-term history (for out-of-bounds recovery)
    # ENHANCED: Use GUI-specified trail buffer (default: 20 points for shorter trails)
    # CRITICAL FIX: Much shorter trail buffer to reduce clutter (cap at 8 points max)
    # This prevents trails from accumulating and creating visual mess
    trail_buffer_capped = min(8, trail_buffer)  # Cap at 8 points maximum
    # Short-term trail for visualization
    ball_pts = deque(maxlen=trail_buffer_capped) if track_ball_flag else None
    # Long-term ball history: {frame_num: (x, y)} for out-of-bounds recovery
    ball_history = {}
    ball_last_seen_frame = -1  # Track when ball was last detected
    # Keep 15 seconds of history (for 10+ second out-of-bounds recovery)
    ball_history_max_age = int(fps * 15)
    heatmap_data = []

    # Store last processed detections for frames that aren't processed
    last_detections = None
    last_annotated_frame = None
    # Store detections for all frames (processed and unprocessed)
    frame_detections = {}
    
    # TEMPORAL CONFIDENCE: Frame look-ahead and look-back for player identity reaffirmation
    # This helps maintain player identity across occlusions and track changes
    # Format: frame_num -> {track_id: {'player_name': name, 'confidence': conf, 'bbox': bbox, 'team': team, 'jersey': jersey}}
    temporal_player_history = {}  # Store player identity from previous frames
    TEMPORAL_LOOK_BACK_FRAMES = int(fps * 1.0) if fps > 0 else 30  # Look back 1 second
    TEMPORAL_LOOK_AHEAD_FRAMES = int(fps * 1.0) if fps > 0 else 30  # Look ahead 1 second (if frames are buffered)
    MAX_TEMPORAL_HISTORY = int(fps * 5.0) if fps > 0 else 150  # Keep 5 seconds of history

    # Overlay metadata setup
    overlay_metadata = None
    player_trajectories = {}  # track_id -> deque of recent positions for trajectory
    # ENHANCEMENT: Track player positions for direction arrow and trail visualization
    player_position_history = {}  # track_id -> deque of (x, y, frame) positions
    player_velocity_cache = {}  # track_id -> (vx, vy, angle) for direction arrow
    MAX_POSITION_HISTORY = 30  # Keep last 30 positions for trail and direction calculation
    
    if export_overlay_metadata and not watch_only:
        try:
            from overlay_metadata import OverlayMetadata, create_player_overlay_data, create_ball_overlay_data, create_trajectory_data, create_predicted_box_data
            overlay_metadata = OverlayMetadata(input_path, fps, total_frames)
            # Build base visualization settings
            viz_settings = {
                "viz_style": viz_style,
                "viz_color_mode": viz_color_mode,
                "show_player_labels": show_player_labels,
                "label_type": label_type,
                "box_thickness": box_thickness,
                "ellipse_width": ellipse_width,
                "ellipse_height": ellipse_height,
                "ellipse_outline_thickness": ellipse_outline_thickness if 'ellipse_outline_thickness' in locals() else 3,
                "overlay_quality": overlay_quality,
                "render_scale": render_scale,
                "enable_advanced_blending": enable_advanced_blending,
                "enable_motion_blur": enable_motion_blur,
                "motion_blur_amount": motion_blur_amount,
                "use_professional_text": use_professional_text,
                # SEPARATE CONTROLS: Bounding boxes vs Circles at feet (user requested)
                "show_bounding_boxes": show_bounding_boxes if 'show_bounding_boxes' in locals() else True,
                "show_circles_at_feet": show_circles_at_feet if 'show_circles_at_feet' in locals() else True,
                # Broadcast-level graphics settings
                "trajectory_smoothness": trajectory_smoothness if 'trajectory_smoothness' in locals() else "bezier",
                "player_graphics_style": player_graphics_style if 'player_graphics_style' in locals() else "standard",
                "use_rounded_corners": use_rounded_corners if 'use_rounded_corners' in locals() else True,
                "use_gradient_fill": use_gradient_fill if 'use_gradient_fill' in locals() else False,
                "corner_radius": corner_radius if 'corner_radius' in locals() else 5,
                "show_jersey_badge": show_jersey_badge if 'show_jersey_badge' in locals() else False,
                "ball_graphics_style": ball_graphics_style if 'ball_graphics_style' in locals() else "standard",
                "show_statistics": show_statistics if 'show_statistics' in locals() else False,
                "statistics_position": statistics_position if 'statistics_position' in locals() else "top_left",
                "show_heat_map": show_heat_map if 'show_heat_map' in locals() else False,
                "heat_map_alpha": heat_map_alpha if 'heat_map_alpha' in locals() else 0.4,
                "heat_map_color_scheme": heat_map_color_scheme if 'heat_map_color_scheme' in locals() else "hot",
                "overlay_quality_preset": overlay_quality_preset if 'overlay_quality_preset' in locals() else "hd",
                # Analytics font settings
                "analytics_font_scale": analytics_font_scale if 'analytics_font_scale' in locals() else 0.5,
                "analytics_position": analytics_position if 'analytics_position' in locals() else "with_player",
                "analytics_font_face": analytics_font_face if 'analytics_font_face' in locals() else "FONT_HERSHEY_SIMPLEX",
                # ENHANCEMENT: Direction arrow and player trail settings
                "show_direction_arrow": show_direction_arrow if 'show_direction_arrow' in locals() else False,
                "show_player_trail": show_player_trail if 'show_player_trail' in locals() else False,
                "direction_arrow_color": direction_arrow_color if 'direction_arrow_color' in locals() else None
            }
            
            # Merge viz_settings_override if provided (allows GUI to override specific settings like statistics panel)
            if viz_settings_override:
                viz_settings.update(viz_settings_override)
            
            overlay_metadata.set_visualization_settings(viz_settings)
            # Initialize trajectory tracking (keep last 30 positions per player)
            # Note: deque is already imported at the top of the file
            print("âœ“ Overlay metadata system initialized (with trajectory tracking)")
        except ImportError as e:
            print(f"âš  Could not initialize overlay metadata: {e}")
            overlay_metadata = None
    
    # Base video writer setup (for saving clean video without overlays)
    # NOTE: Usually not needed - the original input video is already the "base" video
    # This is only useful if:
    # 1. Original video was modified (dewarping, cropping, etc.)
    # 2. You want a guaranteed clean copy in the output directory
    # 3. Original file might be in a different location
    base_video_writer = None
    base_video_path = None
    if save_base_video and not watch_only and enable_video_encoding:
        base_video_path = output_path.replace('.mp4', '_base.mp4')
        try:
            # Use same codec settings as main video
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            base_video_writer = cv2.VideoWriter(base_video_path, fourcc, output_fps_value, (width, height))
            if base_video_writer.isOpened():
                print(f"âœ“ Base video writer initialized: {base_video_path}")
                print(f"   â„¹ Note: Original video at {input_path} is already the base video")
                print(f"   â†’ Base video export is only needed if original was modified (dewarping, etc.)")
            else:
                print(f"âš  Could not initialize base video writer")
                base_video_writer = None
        except Exception as e:
            print(f"âš  Error initializing base video writer: {e}")
            base_video_writer = None
    elif save_base_video:
        print(f"â„¹ Base video export requested but skipped:")
        if watch_only:
            print(f"   â†’ Watch-only mode: no video output")
        elif not enable_video_encoding:
            print(f"   â†’ Video encoding disabled")

    # CSV file setup
    csv_file = None
    csv_writer = None
    csv_file = None
    if export_csv and not watch_only:  # Skip CSV export in watch-only mode
        csv_filename = output_path.replace('.mp4', '_tracking_data.csv')
        # Ensure output directory exists
        csv_dir = os.path.dirname(csv_filename)
        if csv_dir and not os.path.exists(csv_dir):
            os.makedirs(csv_dir, exist_ok=True)
        
        # Check disk space before opening CSV file
        try:
            import shutil
            csv_drive = os.path.splitdrive(csv_filename)[0] + '\\'
            disk_usage = shutil.disk_usage(csv_drive)
            free_gb = disk_usage.free / (1024**3)
            
            # Estimate CSV size: ~500 bytes per frame per player (conservative)
            # Assume max 30 players, so ~15KB per frame
            estimated_csv_size_gb = (total_frames * 15 * 1024) / (1024**3)
            
            if free_gb < 1.0:  # Less than 1GB free
                print(f"âš  WARNING: Low disk space on {csv_drive}: {free_gb:.2f}GB free")
                print(f"   Estimated CSV size: {estimated_csv_size_gb:.2f}GB")
                print(f"   Consider freeing up space or changing output location")
            elif free_gb < estimated_csv_size_gb * 2:  # Less than 2x estimated size
                print(f"âš  WARNING: Disk space may be tight: {free_gb:.2f}GB free, estimated CSV: {estimated_csv_size_gb:.2f}GB")
        except Exception as e:
            print(f"âš  Could not check disk space: {e}")
        
        # Open CSV with buffering to reduce write operations
        # Use line buffering (1) to flush after each write, preventing buffer buildup
        csv_file = open(csv_filename, 'w', newline='', buffering=1)
        csv_writer = csv.writer(csv_file)
        # Determine unit labels based on conversion setting
        dist_unit = 'ft' if use_imperial_units else 'm'
        speed_unit = 'mph' if use_imperial_units else 'mps'
        accel_unit = 'fts2' if use_imperial_units else 'mps2'
        
        csv_writer.writerow(['frame', 'timestamp', 'ball_x', 'ball_y', 'ball_detected',
                            f'ball_x_{dist_unit}', f'ball_y_{dist_unit}', 'ball_trajectory_angle', f'ball_speed_{speed_unit}',
                             'player_id', 'player_name', 'player_x', 'player_y', f'player_x_{dist_unit}', f'player_y_{dist_unit}',
                             f'player_speed_{speed_unit}', f'player_acceleration_{accel_unit}', 'player_movement_angle', 
                             f'distance_to_ball_px', f'distance_traveled_{dist_unit}', f'max_speed_{speed_unit}', 'sprint_count',
                             'possession_time_s', f'distance_from_center_{dist_unit}', f'distance_from_goal_{dist_unit}',
                             'field_zone', 'field_position_x_pct', 'field_position_y_pct',
                             'direction_changes', f'avg_speed_{speed_unit}', f'distance_walking_{dist_unit}', f'distance_jogging_{dist_unit}',
                             f'distance_running_{dist_unit}', f'distance_sprinting_{dist_unit}', 'time_stationary_s',
                             'acceleration_events', f'nearest_teammate_dist_{dist_unit}', f'nearest_opponent_dist_{dist_unit}',
                             'confidence', 'possession_player_id', 'team', 'is_anchor', 'x1', 'y1', 'x2', 'y2'])
        
        # CRITICAL: Write video resolution metadata as a comment at the top of CSV
        # This ensures we can validate that anchor frames match the video resolution
        csv_file.write(f"# Video Resolution: {width}x{height} pixels\n")
        csv_file.write(f"# YOLO Resolution: {yolo_resolution} ({yolo_width}x{yolo_height})\n")
        csv_file.write(f"# YOLO Scale Factor: {yolo_scale_factor:.6f}\n")
        csv_file.write(f"# Video FPS: {fps:.3f}\n")
        csv_file.write(f"# All bbox coordinates (x1, y1, x2, y2) are in video native resolution ({width}x{height})\n")
        
        print(f"âœ“ CSV export enabled: {csv_filename}")
        print(f"   â†’ Video resolution: {width}x{height} (locked)")
        print(f"   â†’ YOLO processing: {yolo_resolution} ({yolo_width}x{yolo_height})")
        print(f"   â†’ All bboxes scaled to video resolution: {width}x{height}")
    elif export_csv and watch_only:
        print("âš  CSV export requested but watch-only mode is enabled - CSV will not be created")

    # Load field calibration for ball tracking (to exclude sideline balls)
    field_calibration = load_field_calibration()
    if field_calibration:
        print("âœ“ Field calibration loaded - will exclude balls outside playable field area")
    else:
        print("âš  No field calibration found - using percentage-based exclusion (less accurate)")
        print("  Tip: Run 'Calibrate Field' before analysis for better sideline ball filtering")
    
    # Global flag to disable field boundary filter if it's too aggressive (prevents removing all detections)
    field_boundary_filter_disabled = False
    
    # Statistics for CSV export diagnostics
    csv_export_stats = {
        'frames_with_players': 0,
        'total_player_rows': 0,
        'frames_with_empty_centers': 0,
        'tracker_stats': {
            'frames_processed': 0,
            'frames_with_detections': 0,
            'frames_with_tracker_ids': 0,
            'total_detections': 0,
            'total_tracker_ids_assigned': 0,
            'frames_tracker_failed': 0
        }
    }
    
    # Load anchor frames for player identity protection
    anchor_frames, anchor_frames_file_path, players_in_anchor_frames_set, players_in_current_video = load_anchor_frames(input_path, explicit_anchor_file=explicit_anchor_file, validate_against_csv=True)
    if anchor_frames and isinstance(anchor_frames, dict) and len(anchor_frames) > 0:
        total_anchor_frames = len(anchor_frames)
        total_anchor_tags = sum(len(anchors) for anchors in anchor_frames.values() if anchors is not None and isinstance(anchors, list))
        print(f"âœ“ Loaded {total_anchor_tags} anchor tag(s) from {total_anchor_frames} frame(s)")
        if len(players_in_current_video) > 0:
            print(f"  ðŸŽ¯ Players in this video: {len(players_in_current_video)} ({', '.join(sorted(players_in_current_video))})")
            print(f"  ðŸ” Re-ID will match untagged tracks against ALL gallery players (cross-video identification enabled)")
            print(f"  ðŸŽ¯ Higher confidence threshold (0.75+) required for new assignments to prevent false matches")
        else:
            print(f"  âš  No players identified in current video - Re-ID will match against all gallery players")
            print(f"  ðŸŽ¯ Higher confidence threshold (0.75+) required for new assignments to prevent false matches")
    else:
        print("âš  No anchor frames found - player identity will rely on Re-ID and gallery matching")
        players_in_current_video = set()  # Ensure it's defined even if no anchor frames
        print("   â†’ Anchor frames provide ground truth player tags for better tracking accuracy")
        print("   â†’ To create anchor frames:")
        print("     1. Use the 'Setup Wizard' to tag players at specific frames")
        print("     2. Or use the 'Gallery Seeder' to create anchor frames from gallery images")
        print("     3. Anchor frames should be saved as PlayerTagsSeed-{video_name}.json")
        print(f"   â†’ Expected file location: {os.path.dirname(os.path.abspath(input_path))}")
        print(f"   â†’ Expected filename pattern: PlayerTagsSeed-{os.path.splitext(os.path.basename(input_path))[0]}.json")
    
    # Load analytics preferences for display
    analytics_preferences = {}
    analytics_prefs_file = "analytics_preferences.json"
    if os.path.exists(analytics_prefs_file):
        try:
            import json as json_module  # Ensure json is in scope
            with open(analytics_prefs_file, 'r') as f:
                analytics_preferences = json_module.load(f)
            selected_count = len([k for k, v in analytics_preferences.items() if v])
            if selected_count > 0:
                print(f"âœ“ Analytics preferences loaded: {selected_count} metrics selected for display")
                # Show which metrics require field calibration
                field_calibration_metrics = ['distance_from_center', 'distance_from_goal', 'field_zone', 
                                            'field_position', 'nearest_teammate', 'nearest_opponent']
                selected_field_metrics = [k for k in field_calibration_metrics if analytics_preferences.get(k, False)]
                if selected_field_metrics:
                    print(f"  â„¹ Note: {len(selected_field_metrics)} selected metric(s) require field calibration: {', '.join(selected_field_metrics)}")
                    print(f"     â†’ These will show 'N/A' if field calibration is not set up")
        except Exception as e:
            print(f"âš  Could not load analytics preferences: {e}")
    
    # QUICK WIN #1: ROI Cropping - Calculate field bounds for faster YOLO processing
    # NOTE: ROI cropping can cause issues if field calibration is incorrect or only covers part of the field
    # If players are only detected in certain areas, consider disabling ROI cropping
    roi_bounds = None
    if field_calibration is not None and track_players_flag:
        # Use larger padding (100px) to ensure we don't miss players near field edges
        # Also check that field calibration covers at least 30% of frame (prevents partial calibrations)
        roi_bounds = get_field_roi_bounds(field_calibration, width, height, padding=100, min_coverage_ratio=0.3)
        if roi_bounds:
            x1, y1, x2, y2 = roi_bounds
            roi_width = x2 - x1
            roi_height = y2 - y1
            roi_area = roi_width * roi_height
            full_area = width * height
            
            # CRITICAL FIX: Validate ROI size - if too small, disable ROI cropping
            # ROI should be at least 20% of frame width and 20% of frame height
            min_width_ratio = 0.20
            min_height_ratio = 0.20
            
            if roi_width < width * min_width_ratio or roi_height < height * min_height_ratio:
                print(f"âš  ROI bounds appear incorrect: ({x1},{y1}) to ({x2},{y2})")
                print(f"   â†’ ROI size: {roi_width}x{roi_height} pixels (too small - {roi_width/width*100:.1f}% width, {roi_height/height*100:.1f}% height)")
                print(f"   â†’ Disabling ROI cropping to prevent excluding players")
                print(f"   â†’ Tip: Re-calibrate field if this is incorrect, or field may extend beyond calibration points")
                roi_bounds = None
            else:
                speedup_estimate = full_area / roi_area if roi_area > 0 else 1.0
                print(f"âœ“ ROI Cropping enabled: Field bounds ({x1},{y1}) to ({x2},{y2})")
                print(f"   â†’ Processing {roi_area/full_area*100:.1f}% of frame area (~{speedup_estimate:.1f}x speedup)")
                print(f"   â†’ If players are missed or boxes are misaligned, field calibration may need adjustment")
        else:
            print("âš  ROI cropping unavailable: Field calibration points insufficient or coverage too small")
            print("   â†’ Processing full frame (slower but more reliable)")
    elif track_players_flag:
        print("âš  ROI cropping unavailable: No field calibration found")
        print("   â†’ Processing full frame (slower but more reliable)")
    
    # VELOCITY CONSTRAINTS: Estimate pixels per meter for velocity validation
    pixels_per_meter = None
    if enable_velocity_constraints and field_calibration:
        pixels_per_meter = estimate_pixels_per_meter(field_calibration, width, height)
        if pixels_per_meter:
            max_velocity_px = pixels_per_meter * 10.0  # 10 m/s max
            print(f"âœ“ Velocity constraints enabled: Max velocity 10.0 m/s (~{max_velocity_px:.1f} px/s)")
        else:
            print("âš  Velocity constraints: Could not estimate pixels/meter (will use fallback estimation)")
    elif enable_velocity_constraints:
        print("âš  Velocity constraints enabled but no field calibration - using fallback estimation")

    # Compute homography matrix for perspective transformation (trajectory
    # analysis)
    homography_matrix = None
    homography_inv = None  # Inverse homography for projecting ball onto field surface
    field_dims = None
    if field_calibration:
        homography_matrix, field_dims = compute_homography_matrix(
            field_calibration, width, height)
        if homography_matrix is not None:
            # CRITICAL FIX: Compute inverse homography for projecting ball
            # positions onto field
            try:
                homography_inv = np.linalg.inv(homography_matrix)
                print(f"âœ“ Homography matrix computed - trajectory analysis enabled")
                print(
                    f"âœ“ Inverse homography computed - ball will be projected onto field surface")
                print(
                    f"  Field dimensions: {
                        field_dims[0]:.1f}m x {
                        field_dims[1]:.1f}m")
            except:
                print(
                    "âš  Could not compute inverse homography - ball projection disabled")
                homography_inv = None
        else:
            print("âš  Could not compute homography matrix - trajectory analysis disabled")

    # Calculate field center and goal positions for analytics
    field_center_m = None  # (x_m, y_m) center of field
    goal1_pos_m = None  # (x_m, y_m) position of goal 1 (typically at x=0)
    goal2_pos_m = None  # (x_m, y_m) position of goal 2 (typically at x=field_length)
    if field_dims is not None:
        field_length, field_width = field_dims
        field_center_m = (field_length / 2.0, field_width / 2.0)
        # Goals are typically at the ends of the field, centered vertically
        goal1_pos_m = (0.0, field_width / 2.0)  # Left goal
        goal2_pos_m = (field_length, field_width / 2.0)  # Right goal

    # Track previous ball position in real-world coordinates for trajectory
    # calculation
    prev_ball_pos_m = None  # (x_m, y_m) in meters
    prev_ball_frame = None

    # Track player positions in real-world space for better smoothing and
    # movement validation
    # track_id -> deque of (x_m, y_m) positions in meters (for per-track position smoothing)
    player_position_history_m = {}
    # player_name -> (x_m, y_m) last known position in meters (for analytics speed calculation, aggregates across track_id changes)
    player_last_pos_m = {}
    # Maximum realistic player speed (m/s) - ~18 mph for soccer
    max_player_speed_mps = 8.0
    
    # Player analytics tracking
    # player_name -> cumulative distance traveled in meters (aggregated across all track_ids for same player)
    player_distance_traveled_m = {}
    # player_name -> maximum speed reached (m/s) (max across all track_ids for same player)
    player_max_speed_mps = {}
    # player_name -> previous speed for acceleration calculation
    player_prev_speed_mps = {}
    # player_name -> previous acceleration for acceleration event detection
    player_prev_acceleration_mps2 = {}
    # player_name -> number of sprints (speed > sprint_threshold) (sum across all track_ids for same player)
    player_sprint_count = {}
    sprint_threshold_mps = 5.5  # ~12.3 mph, typical sprint threshold
    
    # Additional analytics tracking
    # player_name -> time in possession (seconds) (sum across all track_ids for same player)
    player_possession_time = {}
    # player_name -> previous movement angle for direction change detection
    player_prev_movement_angle = {}
    # player_name -> count of direction changes (>45Â°) (sum across all track_ids for same player)
    player_direction_changes = {}
    # player_name -> rolling average speed (deque of recent speeds)
    player_speed_history = {}
    speed_average_window = 10  # Frames for rolling average
    # player_name -> distance in each speed zone (walking, jogging, running, sprinting) (sum across all track_ids)
    player_speed_zone_distances = {}  # {player_name: {'walking': 0, 'jogging': 0, 'running': 0, 'sprinting': 0}}
    # player_name -> time spent stationary (seconds) (sum across all track_ids for same player)
    player_stationary_time = {}  # {player_name: seconds}
    # player_name -> acceleration events count (sum across all track_ids for same player)
    player_acceleration_events = {}  # {player_name: count}
    # Speed zone thresholds (m/s)
    speed_zone_thresholds = {
        'walking': 0.0,  # 0-2 m/s
        'jogging': 2.0,   # 2-4 m/s
        'running': 4.0,  # 4-5.5 m/s
        'sprinting': 5.5  # >5.5 m/s
    }
    # Extract threshold values for easier access
    walking_threshold = speed_zone_thresholds['jogging']  # 2.0 m/s (below this is walking)
    jogging_threshold = speed_zone_thresholds['running']   # 4.0 m/s (below this is jogging)
    running_threshold = speed_zone_thresholds['sprinting']  # 5.5 m/s (below this is running)
    stationary_threshold_mps = 0.5  # Speed below this is considered stationary (m/s)
    acceleration_event_threshold = 2.0  # Acceleration above this triggers an event (m/sÂ²)
    
    # JERSEY UNIQUENESS TRACKING: Global mapping of jersey numbers to track IDs
    # This ensures only ONE track can have a given jersey number across the ENTIRE video
    jersey_to_track_global = {}  # jersey_number -> track_id (persistent across all frames)
    
    # TEAM UNIQUENESS TRACKING: Global mapping of player names to teams
    # This ensures a player can only be on ONE team across the ENTIRE video
    player_to_team_global = {}  # player_name -> (team, track_id) (persistent across all frames)
    
    # PLAYER UNIQUENESS TRACKING: Global mapping of player names to track IDs
    # This ensures a player can only be on ONE track at a time (prevents duplicates like Rocco on tracks #1, #6, #7)
    player_to_track_global = {}  # player_name -> track_id (persistent across all frames)
    
    # CRITICAL FIX: Filter player_to_track_global to only include players in current video
    # This prevents track merging for players from previous videos
    if players_in_current_video and len(players_in_current_video) > 0:
        # Clear any mappings for players not in current video
        players_to_remove = [p for p in player_to_track_global.keys() if p not in players_in_current_video]
        for player_to_remove in players_to_remove:
            del player_to_track_global[player_to_remove]
        if players_to_remove:
            print(f"  âœ“ Cleared {len(players_to_remove)} player(s) from player_to_track_global (not in current video): {', '.join(players_to_remove)}")
    
    # TEAM PERSISTENCE TRACKING: Global mapping of track IDs to teams
    # This ensures a track stays on the same team once assigned (e.g., Gray track cannot switch to Blue)
    track_to_team_global = {}  # track_id -> team (persistent across all frames)
    
    # GLOBAL MERGED TRACKS TRACKING: Track which tracks have been merged (persistent across all frames)
    # This prevents the same merge from being attempted repeatedly on different frames
    global_merged_tracks = set()  # Set of track_ids that have been merged into other tracks (persistent)
    
    # COACH LIST: Players who should NOT be assigned to any team
    # Coaches are not on either team and should remain unassigned
    coach_names = {"Kevin Hill", "Coach", "coach"}  # Case-insensitive matching
    
    # TRACK CONFIDENCE: Store similarity scores for each track's name assignment
    # This allows us to update names when we get significantly better evidence
    track_name_confidence = {}  # track_id -> (player_name, similarity_score, frame_assigned)
    
    # RE-ID OPTIMIZATION: Track when each track was last checked with Re-ID
    # This allows us to skip Re-ID for tracks that already have high-confidence assignments
    track_last_reid_check = {}  # track_id -> frame_num (last frame Re-ID was checked for this track)
    REID_CHECK_INTERVAL = reid_check_interval  # Only check Re-ID every N frames for tracks with existing assignments
    REID_CONFIDENCE_THRESHOLD = reid_confidence_threshold  # If confidence above this, skip Re-ID checks
    
    # BREADCRUMB-BASED RE-ID: Store Re-ID features before occlusions for long-term recovery
    # Format: track_id -> deque of (reid_feature, frame_num, bbox) - stores last N features before occlusion
    track_reid_breadcrumbs = {}  # track_id -> deque(maxlen=BREADCRUMB_HISTORY)
    BREADCRUMB_HISTORY = 5  # Number of Re-ID features to store before occlusion (breadcrumbs)
    BREADCRUMB_MAX_AGE_FRAMES = int(fps * 10) if fps > 0 else 300  # Max age for breadcrumbs (10 seconds)
    # Track when tracks disappear (for occlusion detection)
    track_last_seen = {}  # track_id -> frame_num (last frame track was visible)
    OCCLUSION_THRESHOLD_FRAMES = int(fps * 2) if fps > 0 else 60  # Consider occluded after 2 seconds
    
    # Track drift history to identify problematic tracks
    track_drift_count = {}  # track_id -> count of drift corrections
    track_drift_frames = {}  # track_id -> list of frames where drift occurred
    track_drift_bypass = {}  # track_id -> bool (True = bypass temporal smoothing for this track)
    MAX_DRIFT_CORRECTIONS_BEFORE_WARNING = 5  # Warn if track drifts more than 5 times
    
    # ðŸŽ“ ANCHOR LEARNING STATISTICS: Track how many frames we've learned from anchor-protected players
    # This helps monitor learning progress for Rocco, Cameron, and Ellie
    anchor_learning_stats = {}  # player_name -> {'frames_learned': int, 'total_features': int, 'first_frame': int, 'last_frame': int}
    
    # ðŸŽ¯ UNTAGGED PLAYER IDENTIFICATION STATISTICS: Track untagged players identified by Re-ID
    # Format: player_name -> {'first_identified_frame': int, 'last_identified_frame': int, 'total_frames': int, 
    #                         'avg_confidence': float, 'max_confidence': float, 'track_ids': set, 'frames_learned': int}
    untagged_player_stats = {}  # Track which untagged players were identified and their confidence scores
    
    # ðŸŽ“ UNTAGGED LEARNING STATISTICS: Track learning from high-confidence untagged matches
    # Format: player_name -> {'frames_learned': int, 'total_features': int, 'first_frame': int, 'last_frame': int, 'avg_confidence': float}
    untagged_learning_stats = {}  # Track learning from untagged players (similarity >0.75)
    
    # ANCHOR FRAME PROTECTION: Tracks tagged in anchor frames are protected from Re-ID reassignment
    # Format: track_id -> (player_name, protection_start_frame, protection_end_frame)
    # Protected tracks cannot have their IDs changed by Re-ID within the protection window
    
    # Anchor protection constants (frames)
    # HARD_PROTECTION: Complete block (within 50 frames of anchor center) - Re-ID cannot override
    # SOFT_PROTECTION: Full 3x evidence multiplier (within 100 frames of anchor)
    # DECAY: Gradual decay from 3x to 1x (100-150 frames from anchor)
    # Beyond DECAY: No special protection
    ANCHOR_HARD_PROTECTION_FRAMES = 50  # Frames within this distance get hard protection (complete block)
    ANCHOR_SOFT_PROTECTION_FRAMES = 100  # Frames within this distance get full 3x protection
    ANCHOR_DECAY_FRAMES = 150  # Frames beyond this distance lose protection (decay zone: 100-150 frames)
    
    track_anchor_protection = {}  # track_id -> (player_name, start_frame, end_frame)
    
    # PLAYER-NAME-BASED ANCHOR PROTECTION: Works even when track IDs change
    # Format: player_name -> [(protection_start_frame, protection_end_frame, anchor_bbox, anchor_track_id), ...]
    # This allows us to recover protected players even if their track ID changes
    player_anchor_protection = {}  # player_name -> list of (start_frame, end_frame, bbox, original_track_id)
    
    # OPTIMIZATION: Set of frames that need protection checks (pre-computed at startup)
    # This allows skipping expensive protection checks for frames far from anchor frames
    frames_needing_protection_check = set()  # Will be populated during anchor frame initialization
    
    # ðŸ›¡ï¸ PRE-INITIALIZE ALL PROTECTION WINDOWS AT STARTUP
    # This ensures protection is active from frame 0, not just when anchor frames are encountered
    if anchor_frames and isinstance(anchor_frames, dict) and len(anchor_frames) > 0:
        print(f"  ðŸ›¡ï¸ Initializing protection windows for all anchor frames...")
        protection_windows_initialized = 0
        
        # Group anchors by player/track combination to calculate full protection windows
        player_track_anchors = {}  # (player_name, track_id) -> [frame_numbers]
        anchor_frame_ranges = {}  # (player_name, track_id) -> (first_frame, last_frame) - OPTIMIZATION: Pre-computed for fast lookup
        
        # Track anchor frames by player (even if track_id is None - will be matched during processing)
        player_anchors_by_frame = {}  # player_name -> [frame_numbers] for anchors without track_id
        
        for frame_num, anchors in anchor_frames.items():
            if anchors is None or not isinstance(anchors, list):
                continue
            frame_num_int = int(frame_num) if isinstance(frame_num, str) else frame_num
            
            for anchor in anchors:
                anchor_player_name = anchor.get('player_name')
                anchor_track_id = anchor.get('track_id')
                
                if anchor_player_name:
                    if anchor_track_id is not None:
                        # Anchor has track_id - use existing logic
                        key = (anchor_player_name, anchor_track_id)
                        if key not in player_track_anchors:
                            player_track_anchors[key] = []
                        player_track_anchors[key].append(frame_num_int)
                    else:
                        # Anchor without track_id - will be matched by bbox during processing
                        # Still create player-based protection window
                        if anchor_player_name not in player_anchors_by_frame:
                            player_anchors_by_frame[anchor_player_name] = []
                        player_anchors_by_frame[anchor_player_name].append(frame_num_int)
        
        # OPTIMIZATION: Pre-compute anchor frame ranges for fast lookup during processing
        # This avoids expensive nested loops during video processing
        for (player_name, track_id), frame_numbers in player_track_anchors.items():
            if frame_numbers:
                first_frame = min(frame_numbers)
                last_frame = max(frame_numbers)
                anchor_frame_ranges[(player_name, track_id)] = (first_frame, last_frame)
        
        # Calculate full protection windows for each player/track combination
        for (player_name, track_id), frame_numbers in player_track_anchors.items():
            if not frame_numbers:
                continue
            
            # Find first and last anchor frames for this player/track
            first_anchor_frame = min(frame_numbers)
            last_anchor_frame = max(frame_numbers)
            
            # CRITICAL: Extend protection to END of video (not just 150 frames)
            # Anchor-protected players (Rocco, Cameron, Ellie) should maintain protection for entire video
            # This ensures their identity is preserved even if track ID changes (e.g., Rocco Track #3 â†’ Track #15)
            full_protection_start = max(0, first_anchor_frame - ANCHOR_DECAY_FRAMES)
            # Extend protection to end of video - anchor protection should last until video ends
            full_protection_end = total_frames if 'total_frames' in globals() and total_frames > 0 else last_anchor_frame + 10000
            
            # Initialize track_anchor_protection
            track_anchor_protection[track_id] = (
                player_name, full_protection_start, full_protection_end
            )
            
            # Initialize player_anchor_protection
            if player_name not in player_anchor_protection:
                player_anchor_protection[player_name] = []
            
            # Check if we already have a protection window for this track
            track_protection_exists = False
            for i, (ps, pe, pb, pt) in enumerate(player_anchor_protection[player_name]):
                if pt == track_id:
                    # Update existing protection window to full range
                    player_anchor_protection[player_name][i] = (
                        full_protection_start, full_protection_end, pb, track_id
                    )
                    track_protection_exists = True
                    break
            
            if not track_protection_exists:
                # Add new protection window
                player_anchor_protection[player_name].append((
                    full_protection_start, full_protection_end, None, track_id
                ))
            
            protection_windows_initialized += 1
        
        # Handle anchor frames without track_id - create player-based protection windows
        for player_name, frame_numbers in player_anchors_by_frame.items():
            if not frame_numbers:
                continue
            
            first_anchor_frame = min(frame_numbers)
            last_anchor_frame = max(frame_numbers)
            
            # CRITICAL: Extend protection to END of video (not just 150 frames)
            # Anchor-protected players should maintain protection for entire video
            full_protection_start = max(0, first_anchor_frame - ANCHOR_DECAY_FRAMES)
            # Extend protection to end of video - anchor protection should last until video ends
            full_protection_end = total_frames if 'total_frames' in globals() and total_frames > 0 else last_anchor_frame + 10000
            
            # Initialize player_anchor_protection (track_id will be None, matched during processing)
            if player_name not in player_anchor_protection:
                player_anchor_protection[player_name] = []
            
            # Add protection window with None track_id (will be matched by bbox during processing)
            player_anchor_protection[player_name].append((
                full_protection_start, full_protection_end, None, None  # bbox=None, track_id=None
            ))
            
            protection_windows_initialized += 1
        
        # OPTIMIZATION: Pre-compute set of frames that need protection checks
        # This allows us to skip the entire protection block for frames far from anchor frames
        frames_needing_protection_check = set()
        
        # Add frames from track-based protection
        for track_id, (player_name, protection_start, protection_end) in track_anchor_protection.items():
            for frame_num in range(protection_start, protection_end + 1):
                frames_needing_protection_check.add(frame_num)
        
        # Add frames from player-based protection (anchors without track_id)
        for player_name, windows in player_anchor_protection.items():
            for ps, pe, pb, pt in windows:
                for frame_num in range(ps, pe + 1):
                    frames_needing_protection_check.add(frame_num)
        
        print(f"  âš¡ Performance optimization: Protection checks will only run for {len(frames_needing_protection_check)} frames (out of total video frames)")
        
        # Calculate protection range for summary
        all_protection_starts = []
        all_protection_ends = []
        for player_name, windows in player_anchor_protection.items():
            for ps, pe, pb, pt in windows:
                all_protection_starts.append(ps)
                all_protection_ends.append(pe)
        
        if all_protection_starts and all_protection_ends:
            min_frame = min(all_protection_starts)
            max_frame = max(all_protection_ends)
            print(f"  âœ“ Initialized {protection_windows_initialized} protection window(s) covering frames {min_frame} to {max_frame}")
        else:
            print(f"  âœ“ Initialized {protection_windows_initialized} protection window(s)")
    else:
        # No anchor frames - initialize empty ranges dict and empty protection check set
        anchor_frame_ranges = {}
        frames_needing_protection_check = set()
    
    # Note: ANCHOR protection constants are defined earlier (before anchor_frames loading)
    
    # AUTOMATIC LEARNING: Accumulate data to learn team colors and improve player features
    learned_colors_by_team = {}  # team_name -> list of HSV colors from high-confidence matches
    learning_update_interval = 100  # Update learned colors every N frames
    
    # MOVEMENT LEARNING: Track previous positions for velocity/acceleration calculation
    track_previous_positions = {}  # track_id -> (x, y, frame_num) for velocity calculation
    
    # AUTO-CREATE PLAYERS FROM TRACKS (watch-only mode): Track unmatched tracks for auto-creation
    unmatched_track_frames = {}  # track_id -> frame_count (how many frames this track has been unmatched)
    auto_created_players = {}  # track_id -> player_id (maps tracks to auto-created anonymous players)
    min_frames_for_auto_create = 30  # Minimum frames before auto-creating a player from a track

    # Ball velocity tracking (for motion prediction)
    last_ball_velocity = None

    # Temporal smoothing for player positions (better tracking stability)
    player_position_history = {}  # track_id -> deque of recent positions
    # IMPROVED: Increased smoothing window for smoother tracking (was 10, now
    # 15 for less blinking)
    smoothing_window = 15 if temporal_smoothing else 1  # Number of frames to average
    
    # Label position smoothing to prevent flickering
    # ENHANCED: Scale smoothing window with FPS to prevent flickering at high frame rates
    # At 120fps, 5 frames = 0.04s (too short, causes flickering)
    # At 24fps, 5 frames = 0.21s (reasonable)
    # OPTIMIZED: Reduced smoothing time for more responsive labels that keep up with fast movement
    # Scale to maintain ~0.2-0.25 seconds of smoothing regardless of FPS
    if fps > 0:
        target_smoothing_time = 0.25  # OPTIMIZED: 0.25s provides smooth labels without lag (was 0.75s)
        label_smoothing_window = max(5, int(fps * target_smoothing_time))  # At least 5 frames, scale with FPS
        # Cap at reasonable maximum (30 frames = ~0.25s at 120fps)
        label_smoothing_window = min(30, label_smoothing_window)
    else:
        label_smoothing_window = 5  # Fallback (reduced from 10)
    
    label_position_history = {}  # track_id -> deque of recent label positions (x, y)
    label_side_history = {}  # track_id -> deque of recent side choices ('left' or 'right')
    print(f"   â†’ Label smoothing: {label_smoothing_window} frames (~{label_smoothing_window/fps:.2f}s at {fps:.1f}fps) for stable labels")
    
    def get_smoothed_label_position(track_id, raw_x, raw_y, text_width, frame_width, bbox_x1, bbox_x2):
        """Get smoothed label position to prevent flickering
        
        Args:
            track_id: Track ID
            raw_x: Raw label X position
            raw_y: Raw label Y position
            text_width: Width of label text
            frame_width: Frame width for boundary checking
            bbox_x1: Bounding box left edge
            bbox_x2: Bounding box right edge
        """
        if track_id is None:
            # Simple case - no smoothing
            if raw_x + text_width > frame_width:
                return bbox_x1 - text_width - 5, raw_y
            return raw_x, raw_y
        
        if track_id not in label_position_history:
            label_position_history[track_id] = deque(maxlen=label_smoothing_window)
            label_side_history[track_id] = deque(maxlen=label_smoothing_window)
        
        # Determine which side to use (with hysteresis to prevent rapid switching)
        use_left_side = raw_x + text_width > frame_width
        
        # Track side choice history
        label_side_history[track_id].append('left' if use_left_side else 'right')
        
        # Use majority vote for side to prevent rapid switching
        if len(label_side_history[track_id]) >= 3:
            side_votes = list(label_side_history[track_id])
            left_count = side_votes.count('left')
            right_count = side_votes.count('right')
            # Use majority side, with preference for current side if tied
            if left_count > right_count:
                use_left_side = True
            elif right_count > left_count:
                use_left_side = False
            # If tied, keep current side
        
        # Calculate final position based on side choice
        if use_left_side:
            final_raw_x = bbox_x1 - text_width - 5
        else:
            final_raw_x = raw_x
        
        # Smooth the position with exponential weighted average
        label_position_history[track_id].append((final_raw_x, raw_y))
        
        # Use smoothed position if we have enough history
        if len(label_position_history[track_id]) >= 3:
            # ENHANCED: Exponential weighted average (recent positions have slightly more weight)
            # This makes labels move smoothly but still track the player
            positions = list(label_position_history[track_id])
            
            # Create weights that increase exponentially (most recent = highest weight)
            # But not too aggressive - we want labels to "stick" to a position
            n = len(positions)
            # Exponential weights: [0.5, 0.6, 0.7, ..., 1.0] (normalized)
            weights = np.linspace(0.5, 1.0, n)
            weights = weights / weights.sum()  # Normalize to sum to 1
            
            # Weighted average
            smoothed_x = int(sum(p[0] * w for p, w in zip(positions, weights)))
            smoothed_y = int(sum(p[1] * w for p, w in zip(positions, weights)))
            
            return smoothed_x, smoothed_y
        else:
            return final_raw_x, raw_y

    # Enhanced tracking: Kalman filters and EMA smoothers per track
    enhanced_kalman_filters = {}  # track_id -> EnhancedKalmanFilter
    ema_smoothers = {}  # track_id -> EMASmoother
    confidence_history = {}  # track_id -> deque of recent confidences

    # CRITICAL FIX: Track last seen frame for each track ID to prevent aggressive cleanup
    # This prevents blinking by keeping smoothing state alive during brief
    # occlusions
    track_last_seen = {}  # track_id -> last frame number where track was seen
    track_first_seen = {}  # track_id -> first frame number where track appeared (for age-based protection)
    
    # HIGH-CONFIDENCE TRACK PROTECTION: Tracks with long successful Re-ID history get automatic protection
    # Format: track_id -> (player_name, first_high_confidence_frame, last_high_confidence_frame)
    # If a track has been consistently identified with high confidence (>0.8) for >100 frames, extend protection
    track_high_confidence_history = {}  # track_id -> (player_name, first_frame, last_frame, consecutive_frames)
    HIGH_CONFIDENCE_THRESHOLD = 0.80  # Minimum confidence to count as "high confidence"
    MIN_CONSECUTIVE_FRAMES = 100  # Minimum frames of high confidence to trigger auto-protection
    AUTO_PROTECTION_EXTENSION = 150  # Frames to extend protection beyond last high-confidence frame
    track_anchor_assigned = {}  # track_id -> (player_name, anchor_frame) - tracks that were assigned via anchor frames (permanent protection)
    
    # ðŸ›¡ï¸ DRIFT PREVENTION: Identity lock after N confident matches (Grok's Fix #1)
    LOCK_AFTER_N_CONFIDENT_MATCHES = 2  # Lock after just 2 confident matches (was 100 frames)
    track_confirmed_identity = {}  # track_id -> (player_name, lock_frame, confidence, match_count)
    
    # ðŸ›¡ï¸ DRIFT PREVENTION: Jersey color signature per track (Grok's Fix #4)
    track_jersey_signatures = {}  # track_id -> dominant_color_hsv (for color validation)
    
    # ðŸ›¡ï¸ DRIFT PREVENTION: Embedding smoothing (EMA) for Re-ID features (Grok's Fix #6)
    track_reid_embeddings = {}  # track_id -> deque of recent embeddings (maxlen=10)
    REID_EMA_ALPHA = 0.2  # Smoothing factor for embedding EMA
    
    # ðŸ›¡ï¸ DRIFT PREVENTION: Merge cooldown window (Grok's Fix #8)
    track_merge_cooldown = {}  # track_id -> (merge_frame, cooldown_end_frame)
    MERGE_COOLDOWN_FRAMES = 10  # Wait 10 frames after merge before allowing identity update
    
    # IDENTITY TRACKER: Comprehensive system for tracking player identity across track ID changes
    try:
        from identity_tracker import IdentityTracker
        identity_tracker = IdentityTracker(position_tolerance_px=200.0, iou_threshold=0.3)
        print("  âœ“ Identity Tracker initialized - will track players across track ID changes")
    except ImportError:
        identity_tracker = None
        print("  âš  Identity Tracker not available - identity persistence may be limited")
    track_match_consensus = {}  # track_id -> deque of recent match attempts (for multi-frame consensus)
    
    # EXPANSION IOU: Track velocities and positions for motion prediction
    track_velocities = {}  # track_id -> (vx, vy) in pixels per frame
    track_positions_history = {}  # track_id -> deque of [(frame_num, center_x, center_y, bbox), ...] for velocity calculation
    previous_frame_tracks = {}  # track_id -> {'bbox': (x1,y1,x2,y2), 'center': (cx,cy), 'frame': frame_num} from previous frame
    
    # GSI: Track history for Gaussian Smoothed Interpolation
    gsi_track_history = {}  # track_id -> [(frame_num, x, y), ...] for GSI smoothing
    
    # ðŸš« BALL FILTERING: Track movement history for post-tracking ball detection
    track_movement_history = {}  # track_id -> list of (frame, center_x, center_y) for detecting stationary balls
    
    # OCCLUSION RECOVERY: Track recently disappeared tagged/anchor-protected players
    # When they reappear nearby, recover their identity instead of creating new track
    disappeared_tagged_players = {}  # player_name -> {'track_id': old_id, 'last_pos': (x,y), 'last_frame': frame_num, 'team': team}
    # Convert occlusion_recovery_seconds to frames based on FPS
    OCCLUSION_RECOVERY_FRAMES = int(fps * occlusion_recovery_seconds) if fps > 0 else int(30 * occlusion_recovery_seconds)  # Look for player within N frames of disappearing
    OCCLUSION_RECOVERY_DISTANCE = occlusion_recovery_distance  # pixels - max distance for spatial recovery
    
    # ðŸŽ¯ REALITY-BASED RULES AND HARD LIMITS
    # These enforce physical constraints to prevent impossible tracking scenarios
    
    # Player uniqueness with grace period: Allow 3 frames of overlap during track transitions
    # This prevents flickering when a player switches tracks (e.g., during occlusion recovery)
    PLAYER_UNIQUENESS_GRACE_FRAMES = 3  # Allow player name on multiple tracks for up to 3 frames
    player_uniqueness_grace = {}  # player_name -> {'tracks': set(track_ids), 'start_frame': frame_num, 'frames_remaining': count}
    
    def check_player_uniqueness_grace(player_name, new_track_id, current_frame):
        """
        Check if player name can be assigned to a new track with grace period.
        Allows 3 frames of overlap during track transitions.
        
        Returns:
            (can_assign, should_clear_old): Tuple of (can assign, should clear old assignment)
        """
        if player_name not in player_to_track_global:
            return True, False  # No existing assignment - can assign freely
        
        old_track_id = player_to_track_global[player_name]
        if old_track_id == new_track_id:
            return True, False  # Same track - no conflict
        
        # Check if grace period is active
        if player_name in player_uniqueness_grace:
            grace_info = player_uniqueness_grace[player_name]
            grace_tracks = grace_info.get('tracks', set())
            start_frame = grace_info.get('start_frame', current_frame)
            frames_remaining = grace_info.get('frames_remaining', PLAYER_UNIQUENESS_GRACE_FRAMES)
            
            # Check if grace period is still active
            if current_frame - start_frame < PLAYER_UNIQUENESS_GRACE_FRAMES:
                # Grace period active - allow assignment, add new track to grace set
                grace_tracks.add(new_track_id)
                grace_tracks.add(old_track_id)
                player_uniqueness_grace[player_name] = {
                    'tracks': grace_tracks,
                    'start_frame': start_frame,
                    'frames_remaining': PLAYER_UNIQUENESS_GRACE_FRAMES - (current_frame - start_frame)
                }
                return True, False  # Can assign, don't clear old yet
            else:
                # Grace period expired - clear old assignment
                del player_uniqueness_grace[player_name]
                return True, True  # Can assign, clear old
        else:
            # Start new grace period
            player_uniqueness_grace[player_name] = {
                'tracks': {old_track_id, new_track_id},
                'start_frame': current_frame,
                'frames_remaining': PLAYER_UNIQUENESS_GRACE_FRAMES
            }
            return True, False  # Can assign, start grace period (don't clear old yet)
    
    def update_player_uniqueness_grace(current_frame):
        """Update grace periods - expire old ones"""
        expired_players = []
        for player_name, grace_info in player_uniqueness_grace.items():
            start_frame = grace_info.get('start_frame', current_frame)
            if current_frame - start_frame >= PLAYER_UNIQUENESS_GRACE_FRAMES:
                # Grace period expired - clear old assignments
                old_track_id = player_to_track_global.get(player_name)
                if old_track_id is not None:
                    grace_tracks = grace_info.get('tracks', set())
                    # Find the most recent track (the one that should keep the name)
                    # For now, keep the one in player_to_track_global (most recent assignment)
                    # Clear all other tracks in grace set
                    for track_id in grace_tracks:
                        if track_id != old_track_id:
                            track_str = str(int(track_id))
                            if track_str in player_names and player_names[track_str] == player_name:
                                player_names[track_str] = ""  # Clear old track
                                if track_id in track_name_confidence:
                                    del track_name_confidence[track_id]
                                if current_frame % 100 == 0:
                                    print(f"  ðŸ—‘ï¸ GRACE PERIOD EXPIRED: Cleared '{player_name}' from Track #{track_id} (grace period ended)")
                expired_players.append(player_name)
        
        # Remove expired grace periods
        for player_name in expired_players:
            del player_uniqueness_grace[player_name]
    
    # Maximum players per frame (hard limit enforcement)
    MAX_PLAYERS_PER_FRAME = max_players  # Use GUI setting, but enforce strictly
    
    # Maximum distance per frame (pixels) - prevents teleporting
    # Based on max velocity: 10 m/s = ~1000 px/s at 30fps = ~33 px/frame (conservative)
    MAX_PIXELS_PER_FRAME = int((10.0 * (width / 20.0)) / fps) if fps > 0 else 50  # 10 m/s in pixels per frame
    if MAX_PIXELS_PER_FRAME < 30:
        MAX_PIXELS_PER_FRAME = 30  # Minimum 30 pixels per frame
    if MAX_PIXELS_PER_FRAME > 100:
        MAX_PIXELS_PER_FRAME = 100  # Maximum 100 pixels per frame (safety cap)
    
    # Maximum acceleration (change in velocity per frame)
    # Soccer players can accelerate ~3-5 m/sÂ², which at 30fps = ~0.1-0.17 m/s per frame
    # In pixels: ~10-17 px/s per frame = ~0.3-0.6 px/frameÂ²
    MAX_ACCELERATION_PX_PER_FRAME2 = 1.0  # pixels per frame squared (conservative)
    
    # Bounding box size limits (reality checks)
    # Use GUI settings instead of hardcoded values - fully configurable
    MIN_BBOX_AREA = min_bbox_area  # Minimum area in pixels (from GUI, default: 200)
    MAX_BBOX_AREA = int(width * height * 0.3)  # Maximum 30% of frame (prevents oversized detections)
    MIN_BBOX_WIDTH = min_bbox_width  # Minimum width in pixels (from GUI, default: 10)
    MIN_BBOX_HEIGHT = min_bbox_height  # Minimum height in pixels (from GUI, default: 15)
    MAX_BBOX_WIDTH = int(width * 0.4)  # Maximum 40% of frame width
    MAX_BBOX_HEIGHT = int(height * 0.6)  # Maximum 60% of frame height
    
    # Frame-to-frame position change limit (hard consistency check)
    # Prevents impossible jumps even if velocity constraint passes
    MAX_POSITION_CHANGE_PX = MAX_PIXELS_PER_FRAME * 1.5  # 1.5x velocity limit for safety margin
    
    # ðŸ›¡ï¸ DRIFT PREVENTION: Motion-consistency gating function (Grok's Fix #2)
    def violates_motion_model(track_id, new_position, track_positions_history, MAX_PIXELS_PER_FRAME):
        """Check if position change violates motion model - prevents impossible swaps"""
        if track_id not in track_positions_history or len(track_positions_history[track_id]) < 2:
            return False  # Not enough history - allow assignment
        
        # Get last position (format: frame, x, y, bbox)
        last_frame, last_x, last_y, *_ = track_positions_history[track_id][-1]
        new_x, new_y = new_position
        
        # Calculate distance
        distance = np.sqrt((new_x - last_x)**2 + (new_y - last_y)**2)
        
        # Check if exceeds motion limit (2x safety margin for sudden direction changes)
        if distance > MAX_PIXELS_PER_FRAME * 2.0:
            return True  # Violates motion model
        
        # Check direction consistency (if we have velocity)
        if len(track_positions_history[track_id]) >= 3:
            prev_frame, prev_x, prev_y, *_ = track_positions_history[track_id][-2]
            frames_diff = max(1, last_frame - prev_frame)
            vx = (last_x - prev_x) / frames_diff
            vy = (last_y - prev_y) / frames_diff
            
            # Predict next position
            predicted_x = last_x + vx
            predicted_y = last_y + vy
            
            # Check if new position is way off prediction
            pred_distance = np.sqrt((new_x - predicted_x)**2 + (new_y - predicted_y)**2)
            if pred_distance > MAX_PIXELS_PER_FRAME * 1.5:
                return True  # Violates motion model
        
        return False
    
    # Track previous positions and velocities for reality checks
    track_previous_bbox = {}  # track_id -> previous bbox [x1, y1, x2, y2]
    track_previous_velocity = {}  # track_id -> previous velocity [vx, vy] in pixels per frame
    track_previous_frame = {}  # track_id -> previous frame number

    # CRITICAL FIX: Maintain track state for smooth interpolation/extrapolation
    # This prevents blinking by keeping boxes visible even on unprocessed
    # frames
    # track_id -> {'xyxy': last_bbox, 'frame': last_frame, 'velocity': [vx,
    # vy]}
    track_state = {}
    
    # Optical flow: Store previous frame (grayscale) for motion prediction
    prev_frame_gray = None  # Previous frame in grayscale for optical flow

    # Track jump variables: Initialize to prevent UnboundLocalError
    track_jump_target_id = None  # Target track ID for jump highlighting
    track_jump_target_frame = None  # Target frame for jump highlighting
    track_jump_highlight_frames = 0  # Number of frames remaining for highlight

    # OC-SORT error tracking: Track consecutive KeyErrors to detect corruption
    ocsort_keyerror_count = 0  # Consecutive KeyErrors from OC-SORT
    ocsort_keyerror_threshold = 5  # Reinitialize tracker after this many consecutive errors
    ocsort_last_keyerror_frame = -1  # Last frame with KeyError

    # Batch processing for YOLO - store frames and process in batches
    frame_queue = []  # Store frames waiting to be processed
    frame_data_queue = []  # Store frame data (ball centers, etc.)

    # Setup parallel preprocessing with ThreadPoolExecutor
    # Use ThreadPoolExecutor since OpenCV operations release GIL, allowing
    # true parallelism
    # Use 2-8 workers, leave 1 core free
    num_workers = max(2, min(cpu_count() - 1, 8))
    if dewarp or remove_net:
        print(
            f"ðŸš€ Using {num_workers} parallel workers for frame preprocessing")
        preprocess_executor = ThreadPoolExecutor(max_workers=num_workers)
    else:
        preprocess_executor = None

    # PERFORMANCE: Create a separate thread pool for CPU-bound operations (team classification, uniform extraction)
    # This allows parallelization even when preprocessing isn't needed
    # Use 2-4 workers for team classification (OpenCV operations release GIL)
    cpu_ops_executor = ThreadPoolExecutor(max_workers=min(4, cpu_count())) if cpu_count() > 1 else None

    def preprocess_frame_sync(frame, frame_num):
        """Synchronous preprocessing function (fallback or direct call)"""
        # Apply dewarping if requested
        if dewarp and dewarp_maps is not None:
            map1, map2 = dewarp_maps
            frame = cv2.remap(
                frame,
                map1,
                map2,
                interpolation=cv2.INTER_LINEAR,
                borderMode=cv2.BORDER_CONSTANT)

        # Remove net if requested - using improved battle-tested algorithm
        # NOTE: Store original frame for learning (Re-ID and gallery need sharp images)
        original_frame = frame.copy()
        if remove_net:
            frame = remove_net_pattern(frame, kernel_size=21, sigma=7)
        
        # Store original frame in frame_data for Re-ID and gallery learning
        return frame, original_frame

    # Initialize frame counter
    frame_count = 0
    last_frame_in_batch = 0  # Initialize to prevent UnboundLocalError
    last_printed_learning_frame = -1  # Initialize to prevent UnboundLocalError
    
    # Initialize timing variables
    start_time = time.time()  # Initialize to prevent UnboundLocalError
    
    # Initialize previous frame data (for matching with previous analysis)
    previous_frame_data = load_previous_tracking_data(input_path) if input_path else {}
    
    # Initialize player names dictionary (for track_id -> player_name mapping)
    player_names = load_player_names(input_path, include_seed_files=True) if input_path else {}
    
    # CRITICAL: Initialize include_only_players_set for Re-ID optimization
    # This set contains active players from roster and focused players (if any)
    # Used to speed up Re-ID by only matching against these players
    include_only_players_set = get_active_players_set(
        video_specific_roster=video_specific_roster,
        focused_players=focused_players
    )
    
    # CRITICAL FIX: Filter player_names to only include players actually in this video
    # This prevents track merging for players from previous videos (e.g., Ellie, James, Rocco)
    if players_in_current_video and len(players_in_current_video) > 0:
        # Create a filtered player_names dictionary with only current video players
        original_count = len(player_names)
        filtered_player_names = {}
        for track_id_str, player_name in player_names.items():
            # Extract actual player name if it's in list format
            actual_name = player_name
            if isinstance(player_name, list) and len(player_name) > 0:
                actual_name = str(player_name[0]).strip()
            elif isinstance(player_name, str):
                actual_name = player_name.strip()
            
            # Only keep if player is in current video
            if actual_name and actual_name in players_in_current_video:
                filtered_player_names[track_id_str] = player_name
            else:
                # Player not in current video - clear the mapping
                if actual_name:
                    print(f"  ðŸ” Filtered out player '{actual_name}' from player_names (not in current video)")
        
        # Replace player_names with filtered version
        player_names = filtered_player_names
        if len(filtered_player_names) < original_count:
            print(f"  âœ“ Filtered player_names: {original_count} -> {len(filtered_player_names)} (only players in current video)")
    
    # Initialize player names reload counter (for periodic reloading during analysis)
    last_player_names_reload = 0
    
    # Initialize processing flag (prevents gallery match messages after loop ends)
    is_processing = True
    
    # Progress update: Starting frame processing
    try:
        import shared_state
        shared_state.update_analysis_progress(
            current=0,
            total=total_frames,
            status="Starting frame processing",
            details="Reading video frames",
            phase="Frame Reading"
        )
    except (ImportError, Exception):
        pass
    
    while cap.isOpened():
        # STOP REQUEST: Check if analysis should stop gracefully
        try:
            import shared_state
            if shared_state.is_analysis_stop_requested():
                print("\nâ¹ STOP REQUESTED: Terminating analysis gracefully...")
                print(f"   â†’ Processed {frame_count}/{total_frames} frames ({100.0*frame_count/max(total_frames,1):.1f}%)")
                print(f"   â†’ Saving gallery and cleaning up...")
                # Save gallery if available
                if player_gallery:
                    try:
                        player_gallery.save_gallery()
                        gallery_logger.info(f"Saved player gallery (checkpoint at frame {frame_count})")
                    except Exception as e:
                        gallery_logger.error(f"Could not save gallery at frame {frame_count}: {e}", exc_info=True)
                # Save team colors if available
                if team_colors and learned_colors_by_team:
                    try:
                        updated_colors = learn_team_colors_from_detections(team_colors, learned_colors_by_team)
                        if updated_colors:
                            save_team_color_config(updated_colors)
                            print(f"âœ“ Saved team colors (checkpoint at frame {frame_count})")
                    except Exception as e:
                        print(f"âš  Could not save team colors: {e}")
                # Close video capture
                if cap:
                    cap.release()
                # Close video writer if open
                if out:
                    out.release()
                # Close live viewer if open
                if live_viewer_window:
                    try:
                        cv2.destroyWindow(live_viewer_window)
                    except:
                        pass
                print("âœ“ Analysis stopped gracefully")
                return  # Exit the function
        except ImportError:
            pass  # shared_state not available, continue normally
        except Exception as e:
            pass  # Silently continue if check fails
        
        # PAUSE/RESUME: Check if processing should be paused
        if dynamic_settings and dynamic_settings.paused:
            # Keep reading frames but don't process them (maintain video position)
            ret, frame = cap.read()
            if not ret:
                break
            # Still update frame count to maintain position
            frame_count += 1
            # SAFETY: Prevent exceeding total_frames
            if frame_count >= total_frames:
                break
            # Display current frame in live viewer if enabled
            if watch_only and show_live_viewer and live_viewer_window:
                try:
                    # Show the last processed frame (or current frame if no processing yet)
                    display_frame = frame.copy()
                    
                    # Get current window size to maintain aspect ratio when resizing
                    # OPTIMIZATION: Only check window size every 10 frames to reduce overhead
                    if dynamic_settings and (not hasattr(dynamic_settings, '_cached_window_size') or frame_count % 10 == 0):
                        try:
                            window_rect = cv2.getWindowImageRect(live_viewer_window)
                            if window_rect and len(window_rect) >= 4:
                                window_width = window_rect[2]
                                window_height = window_rect[3]
                                if not hasattr(dynamic_settings, '_cached_window_size'):
                                    dynamic_settings._cached_window_size = (window_width, window_height)
                                else:
                                    old_w, old_h = dynamic_settings._cached_window_size
                                    if abs(old_w - window_width) > 5 or abs(old_h - window_height) > 5:
                                        dynamic_settings._cached_window_size = (window_width, window_height)
                        except:
                            pass
                    
                    # Use cached window size for scaling
                    if dynamic_settings and hasattr(dynamic_settings, '_cached_window_size'):
                        window_width, window_height = dynamic_settings._cached_window_size
                        if window_width > 0 and window_height > 0:
                            frame_h, frame_w = display_frame.shape[:2]
                            if abs(window_width - frame_w) > 10 or abs(window_height - frame_h) > 10:
                                scale_w = window_width / frame_w
                                scale_h = window_height / frame_h
                                scale = min(scale_w, scale_h)
                                if abs(scale - 1.0) > 0.01:
                                    new_w = int(frame_w * scale)
                                    new_h = int(frame_h * scale)
                                    display_frame = cv2.resize(display_frame, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
                    
                    cv2.imshow(live_viewer_window, display_frame)
                    
                    # CRITICAL: Use minimal waitKey delay to keep window responsive
                    # Use 1ms delay to prevent blocking while still processing events
                    while dynamic_settings and dynamic_settings.paused:
                        key = cv2.waitKey(1) & 0xFF  # 1ms delay - keeps window responsive
                        if key == ord(' ') or key == 32:  # SPACEBAR - resume
                            dynamic_settings.paused = False
                            print("\nâ–¶ RESUMED")
                            break
                        elif key == ord('r') or key == ord('R'):  # 'r' - resume
                            dynamic_settings.paused = False
                            print("\nâ–¶ RESUMED")
                            break
                        elif key == ord('q') or key == 27:  # 'q' or ESC - close
                            print("\nâš  Live viewer closed by user (pressed 'q')")
                            cv2.destroyWindow(live_viewer_window)
                            live_viewer_window = None
                            break
                        # Keep redisplaying frame while paused (but don't block)
                        try:
                            cv2.imshow(live_viewer_window, display_frame)
                        except:
                            break  # Window was closed
                except:
                    pass
            continue  # Skip processing while paused
        
        # TRACK JUMP: Check for user requests to jump to a specific track
        try:
            import shared_state
            jump_requests = shared_state.get_track_jump_requests()
            if jump_requests:
                for track_id, jump_data in jump_requests.items():
                    target_frame = jump_data.get('frame', None)
                    if target_frame is not None and not jump_data.get('confirmed', False):
                        # Check if we need to jump to this frame (more than 30 frames away)
                        if abs(frame_count - target_frame) > 30:
                            # Jump to the target frame
                            cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)
                            frame_count = target_frame
                            track_jump_target_frame = target_frame
                            track_jump_target_id = track_id
                            track_jump_highlight_frames = 150  # Highlight for 150 frames
                            # Pause if dynamic_settings available
                            if dynamic_settings:
                                dynamic_settings.paused = True
                                print(f"\nâ¸ PAUSED: Jumped to Track #{track_id} at Frame {target_frame}")
                                print(f"   â†’ Check the conflict resolution window to confirm this track")
                                print(f"   â†’ Press Resume in the viewer controls to continue")
        except:
            pass  # Silently fail if shared_state not available
        
        # OPTIMIZATION: In watch-only mode without live viewer, skip frames for faster processing
        # Read frame but only process every Nth frame (skip frames for speed)
        if watch_only and not show_live_viewer:
            # Skip frames: only process every 2nd frame in watch-only mode (2x speedup)
            # Still read all frames to maintain position, but only process every other one
            ret, frame = cap.read()
            if not ret:
                break
            frame_count += 1
            # SAFETY: Prevent exceeding total_frames
            if frame_count >= total_frames:
                break
            # Skip processing for odd frames (process frames 0, 2, 4, 6...)
            if frame_count % 2 == 1:
                continue  # Skip this frame, just read the next one
        else:
            # Normal mode: process every frame
            ret, frame = cap.read()
            if not ret:
                break
            frame_count += 1
            # SAFETY: Prevent exceeding total_frames
            if frame_count >= total_frames:
                break
            
            # Update progress (every 10 frames to avoid overhead)
            if frame_count % 10 == 0:
                try:
                    import shared_state
                    # Determine current phase
                    current_phase = "Frame Reading"
                    if track_players_flag and model is not None:
                        current_phase = "Detection & Tracking"
                    elif track_ball_flag:
                        current_phase = "Ball Tracking"
                    else:
                        current_phase = "Processing"
                    
                    shared_state.update_analysis_progress(
                        current=frame_count,
                        total=total_frames,
                        status="Processing frame",
                        details=f"Frame {frame_count} of {total_frames}",
                        phase=current_phase
                    )
                except (ImportError, Exception):
                    pass  # shared_state not available, continue normally
            
            # Check for stop request after reading frame (more frequent checks)
            try:
                import shared_state
                if shared_state.is_analysis_stop_requested():
                    print("\nâ¹ STOP REQUESTED: Terminating analysis gracefully...")
                    print(f"   â†’ Processed {frame_count}/{total_frames} frames ({100.0*frame_count/max(total_frames,1):.1f}%)")
                    print(f"   â†’ Saving gallery and cleaning up...")
                    # Save gallery if available
                    if player_gallery:
                        try:
                            player_gallery.save_gallery()
                            gallery_logger.info(f"Saved player gallery (checkpoint at frame {frame_count})")
                        except Exception as e:
                            gallery_logger.error(f"Could not save gallery at frame {frame_count}: {e}", exc_info=True)
                    # Save team colors if available
                    if team_colors and learned_colors_by_team:
                        try:
                            updated_colors = learn_team_colors_from_detections(team_colors, learned_colors_by_team)
                            if updated_colors:
                                save_team_color_config(updated_colors)
                                print(f"âœ“ Saved team colors (checkpoint at frame {frame_count})")
                        except Exception as e:
                            print(f"âš  Could not save team colors: {e}")
                    # Close video capture
                    if cap:
                        cap.release()
                    # Close video writer if open
                    if out:
                        out.release()
                    # Close live viewer if open
                    if live_viewer_window:
                        try:
                            cv2.destroyWindow(live_viewer_window)
                        except:
                            pass
                    print("âœ“ Analysis stopped gracefully")
                    return  # Exit the function
            except ImportError:
                pass  # shared_state not available, continue normally
            except Exception:
                pass  # Silently continue if check fails

        # CRITICAL FIX: Preview mode - stop after processing preview_max_frames
        if preview_mode and frame_count >= preview_max_frames:
            break

        timestamp = frame_count / fps if fps > 0 else 0

        # Preprocess frame in parallel (submit and wait immediately for current frame)
        # This allows multiple frames to be preprocessed concurrently during
        # batch processing
        if preprocess_executor is not None:
            # Submit for parallel preprocessing
            future = preprocess_executor.submit(
                preprocess_frame_sync, frame.copy(), frame_count)
            # Wait for result (with timeout fallback)
            try:
                result = future.result(timeout=2.0)
                if isinstance(result, tuple):
                    frame, original_frame_for_learning = result
                else:
                    frame = result
                    original_frame_for_learning = frame.copy()  # Fallback if not tuple
            except Exception as e:
                # Timeout or error - preprocess synchronously
                # CRITICAL FIX: Copy frame before preprocessing to avoid
                # modifying original
                result = preprocess_frame_sync(frame.copy(), frame_count)
                if isinstance(result, tuple):
                    frame, original_frame_for_learning = result
                else:
                    frame = result
                    original_frame_for_learning = frame.copy()  # Fallback if not tuple
        else:
            # No preprocessing needed, frame is already original
            original_frame_for_learning = frame.copy()
        # If no preprocessing needed, use frame as-is

        ball_center = None
        ball_detected = False

        # Track ball first (on original frame) with velocity prediction
        # Always track ball on every frame (not skipped) for smooth ball trail
        # Use dynamic settings if available
        current_track_ball = get_setting('track_ball_flag', track_ball_flag)
        current_show_ball_trail = get_setting('show_ball_trail', show_ball_trail)
        current_trail_length = get_setting('trail_length', trail_length)
        current_ball_min_radius = get_setting('ball_min_radius', ball_min_radius)
        current_ball_max_radius = get_setting('ball_max_radius', ball_max_radius)
        
        # CRITICAL FIX: Clear ball trail if ball tracking is disabled
        if not current_track_ball and ball_pts is not None:
            ball_pts.clear()  # Clear any leftover trail points when ball tracking is disabled

        # ðŸŽ¯ YOLO BALL DETECTION: Detect ball using YOLO class 32 (if model supports it)
        yolo_ball_center = None
        yolo_ball_detected = False
        if current_track_ball and model is not None and YOLO_AVAILABLE:
            try:
                # Check if model supports ball class (32) - some models may not have it
                # Run YOLO with ball class (32) for ball detection
                # Use lower confidence for ball (balls are small and move fast)
                # Use adaptive confidence if available, otherwise use default
                ball_conf_thresh = max(0.15, (adaptive_conf_thresh * 0.5) if 'adaptive_conf_thresh' in locals() else 0.15)  # Lower threshold for ball
                
                # Run YOLO detection for ball (class 32)
                # Note: This is a separate call - we can't mix classes in one call easily
                ball_results = model(
                    frame,
                    classes=[32],  # Ball class in COCO dataset
                    conf=ball_conf_thresh,
                    verbose=False,
                    imgsz=imgsz if 'imgsz' in locals() else None,
                    half=use_half if 'use_half' in locals() else False,
                    max_det=5  # Only expect 1 ball, but allow a few for false positives
                )
                
                # Extract ball detections
                if ball_results and len(ball_results) > 0:
                    ball_detections = sv.Detections.from_ultralytics(ball_results[0])
                    if len(ball_detections) > 0:
                        # Use the highest confidence ball detection
                        best_ball_idx = np.argmax(ball_detections.confidence) if ball_detections.confidence is not None else 0
                        ball_bbox = ball_detections.xyxy[best_ball_idx]
                        ball_x = (ball_bbox[0] + ball_bbox[2]) / 2
                        ball_y = (ball_bbox[1] + ball_bbox[3]) / 2
                        yolo_ball_center = (ball_x, ball_y)
                        yolo_ball_detected = True
                        
                        if frame_count % 500 == 0:
                            ball_conf = ball_detections.confidence[best_ball_idx] if ball_detections.confidence is not None else 0.0
                            print(f"  âš½ YOLO Ball detected at frame {frame_count}: position ({ball_x:.1f}, {ball_y:.1f}), confidence: {ball_conf:.3f}")
            except Exception as e:
                # YOLO ball detection failed (model may not support class 32, or other error)
                # Fall back to color-based detection
                if frame_count % 1000 == 0:
                    print(f"  âš  YOLO ball detection failed (may not support class 32): {e} - using color-based detection")
                yolo_ball_center = None
                yolo_ball_detected = False
        
        # Color-based ball tracking (fallback or primary method)
        if current_track_ball and ball_pts is not None:
            frame, ball_center, ball_detected, last_ball_velocity = track_ball_in_frame(
                frame, ball_pts, buffer, current_ball_min_radius, current_ball_max_radius,
                edge_margin=50, show_trail=current_show_ball_trail, ball_velocity=last_ball_velocity,
                fps=fps, field_calibration=field_calibration, frame_num=frame_count,
                seed_ball_positions=seed_ball_positions,
                ball_history=ball_history, ball_last_seen_frame=ball_last_seen_frame,
                homography_matrix=homography_matrix, homography_inv=homography_inv,
                trail_length=current_trail_length  # Pass trail length from GUI
            )
            
            # ðŸŽ¯ HYBRID BALL DETECTION: Prefer YOLO if available, fallback to color-based
            if yolo_ball_detected and yolo_ball_center is not None:
                # YOLO detected ball - use it (more reliable)
                ball_center = yolo_ball_center
                ball_detected = True
                if frame_count % 500 == 0:
                    print(f"  âš½ Using YOLO ball detection (frame {frame_count})")
            # Otherwise use color-based detection result (already set above            )
            
            # ðŸŽ¯ HYBRID BALL DETECTION: Prefer YOLO if available, fallback to color-based
            if yolo_ball_detected and yolo_ball_center is not None:
                # YOLO detected ball - use it (more reliable)
                ball_center = yolo_ball_center
                ball_detected = True
                if frame_count % 500 == 0:
                    print(f"  âš½ Using YOLO ball detection (frame {frame_count})")
            # Otherwise use color-based detection result (already set above)

            # Update long-term ball history for out-of-bounds recovery
            # IMPROVED: Validate ball_center is not at (0,0) before storing
            if ball_detected and ball_center is not None:
                # Skip if center is at origin (0,0) - likely a bug
                if ball_center[0] > 10 or ball_center[1] > 10:
                    ball_history[frame_count] = ball_center
                    ball_last_seen_frame = frame_count
                    # Diagnostic: Log ball detection occasionally
                    if frame_count % 500 == 0:
                        print(f"  âš½ Ball detected at frame {frame_count}: position ({ball_center[0]:.1f}, {ball_center[1]:.1f}), trail points: {len(ball_pts) if ball_pts else 0}")
                else:
                    # Invalid center - don't store it
                    ball_center = None
                    ball_detected = False
            elif frame_count % 1000 == 0 and current_track_ball:
                # Diagnostic: Log when ball tracking is enabled but ball not detected
                print(f"  âš½ Ball tracking enabled but ball not detected at frame {frame_count} (check ball color config or visibility)")

            # Clean up old history (keep last 15 seconds)
            if ball_history:
                frames_to_keep = int(fps * 15)
                frames_to_remove = [
                    f for f in ball_history.keys() if frame_count -
                    f > frames_to_keep]
                for f in frames_to_remove:
                    del ball_history[f]
            
            if not ball_detected:
                # Ball not detected - check if we should use history for
                # recovery
                frames_since_last_seen = frame_count - ball_last_seen_frame
                if frames_since_last_seen > 0 and frames_since_last_seen < ball_history_max_age:
                    # Try to recover ball position from history (for out-of-bounds scenarios)
                    # Use last known position if within reasonable time
                    if ball_last_seen_frame in ball_history:
                        # Could use this for prediction, but for now just track
                        # that ball is missing
                        pass
        else:
            ball_center = None
            ball_detected = False

        # Add frame to queue for batch processing (with frame skipping support)
        # Note: We process every Nth frame for tracking, but write ALL frames
        # to output
        if track_players_flag and model is not None:
            # Only process frames based on process_every_nth_frame
            should_process_frame = (
                frame_count %
                process_every_nth_frame == 0) or (
                frame_count == total_frames -
                1)

            if should_process_frame:
                # CRITICAL FIX: Store full frame BEFORE resizing/cropping for team classification
                # Store original full-resolution frame for team classification (if needed)
                full_frame_ref = None
                if roi_bounds is not None and viz_color_mode == "team" and team_colors:
                    # Store reference to original full-resolution frame BEFORE any processing
                    # This ensures bbox coordinates match the frame dimensions
                    full_frame_ref = frame.copy()
                
                # Resize frame for YOLO if needed (can be done in parallel for batches)
                # yolo_width/yolo_height already set above (auto-downscaled for
                # 4K if needed)
                if yolo_resolution != "full" or (
                        width >= 3840 or height >= 2160):
                    # Resize to reduce memory usage (this is CPU-intensive,
                    # will benefit from parallelization)
                    frame_for_yolo = cv2.resize(
                        frame, (yolo_width, yolo_height))
                else:
                    # For non-4K videos, use frame directly (smaller memory
                    # footprint)
                    frame_for_yolo = frame.copy()
                
                # QUICK WIN #1: ROI Cropping - Crop to field bounds before YOLO
                if roi_bounds is not None:
                    frame_for_yolo = crop_frame_for_yolo(frame_for_yolo, roi_bounds)

                frame_queue.append(frame_for_yolo)
                # Store only necessary data - avoid copying full 4K frames
                # CRITICAL: Store original frame (before net removal) for Re-ID and gallery learning
                # This ensures learning uses sharp images, not blurred net-removed frames
                # original_frame_for_learning is already set above from preprocessing
                frame_data_queue.append({
                    'frame_num': frame_count,
                    'timestamp': timestamp,
                    'ball_center': ball_center,
                    'ball_detected': ball_detected,
                    'scale_factor': yolo_scale_factor,  # Already set correctly above
                    'full_frame': full_frame_ref,  # Store full-resolution frame for team classification
                    'original_frame_for_learning': original_frame_for_learning,  # Store original (sharp) frame for Re-ID and gallery
                    'original_width': width,  # Store original dimensions for coordinate validation
                    'original_height': height
                })

            # Process batch when it reaches batch_size or at end
            # For 4K videos, we can use larger batches on GPU (GPU has 8GB)
            effective_batch_size = batch_size
            if width >= 3840 or height >= 2160:
                # With GPU, we can process more frames at once for better utilization
                # 4K frames are downscaled to 1080p for YOLO, so memory usage
                # is reasonable
                if device == 'cuda':
                    # Increase batch size for GPU to maximize utilization (RTX
                    # 4060 has 8GB VRAM)
                    # Larger batch for better GPU utilization
                    effective_batch_size = min(batch_size, 16)
                else:
                    effective_batch_size = min(
                        batch_size, 4)  # Smaller batch on CPU
            elif device == 'cuda':
                # For non-4K videos on GPU, still use larger batches
                effective_batch_size = min(batch_size, 16)

            if len(frame_queue) >= effective_batch_size or (
                    frame_count == total_frames - 1 and len(frame_queue) > 0):
                # Process batch with YOLO (better GPU utilization)
                # Model is already on the correct device (set during initialization)
                # Ensure we're using the NVIDIA GPU (not Intel integrated)
                if device == 'cuda' and cuda_device_id is not None:
                    # Set CUDA device context to ensure NVIDIA GPU is used
                    with torch.cuda.device(cuda_device_id):
                        # Optimize GPU inference for maximum throughput
                        try:
                            # QUICK WIN #3: Adaptive Confidence - Calculate per-frame thresholds
                            # Use first frame in batch for threshold calculation (batch should be similar lighting)
                            adaptive_conf_thresh = get_adaptive_confidence_threshold(
                                frame_queue[0], base_thresh=track_thresh, adaptive_confidence=adaptive_confidence
                            ) if len(frame_queue) > 0 else track_thresh
                            
                            # Warm up GPU on first batch (reduces
                            # initialization overhead)
                            if frame_count == 0 or frame_count == process_every_nth_frame:
                                # Quick warmup with 2 frames
                                warmup_frames = frame_queue[:min(
                                    2, len(frame_queue))]
                                # Calculate imgsz for warmup
                                warmup_imgsz = None
                                warmup_half = device == 'cuda' and torch.cuda.is_available()
                                if len(warmup_frames) > 0:
                                    frame_h, frame_w = warmup_frames[0].shape[:2]
                                    # Round to nearest multiple of 32 (YOLO stride requirement)
                                    max_dim = max(frame_w, frame_h)
                                    warmup_imgsz = int(round(max_dim / 32) * 32)
                                _ = model(
                                    warmup_frames, 
                                    classes=[0], 
                                    conf=adaptive_conf_thresh, 
                                    verbose=False,
                                    imgsz=warmup_imgsz,
                                    half=warmup_half,
                                    max_det=25
                                )

                            # Main batch inference - YOLO handles GPU transfer automatically
                            # Don't use synchronize() here as it blocks - let GPU pipeline work
                            # Ensure we're using GPU by checking device
                            if hasattr(
                                    model, 'device') and 'cuda' not in str(
                                    model.device):
                                print(
                                    f"âš  Warning: Model device is {
                                        model.device}, expected CUDA")

                            # OPTIMIZATION: Calculate imgsz based on yolo_resolution
                            # YOLO expects square input, use the larger dimension
                            imgsz = None
                            if len(frame_queue) > 0:
                                frame_h, frame_w = frame_queue[0].shape[:2]
                                # Round to nearest multiple of 32 (YOLO stride requirement)
                                max_dim = max(frame_w, frame_h)
                                imgsz = int(round(max_dim / 32) * 32)
                            
                            # OPTIMIZATION: Use FP16 (half precision) for faster GPU inference
                            # Only use half=True on GPU (CUDA), not on CPU
                            use_half = device == 'cuda' and torch.cuda.is_available()
                            
                            # NOTE: Batch processing with frame arrays doesn't use streaming mode
                            # Streaming mode (stream=True) is for direct video file processing, not frame arrays
                            # Batch processing provides better GPU utilization for frame arrays
                            
                            results = model(
                                frame_queue,
                                classes=[0],
                                conf=adaptive_conf_thresh,  # QUICK WIN #3: Use adaptive confidence
                                verbose=False,
                                imgsz=imgsz,  # Explicit image size for optimization
                                half=use_half,  # FP16 inference for faster GPU processing (20-30% speedup)
                                max_det=max(30, max_players + 10)  # Allow more detections than max_players to account for false positives and filtering
                            )

                            # Optional: Log GPU memory usage periodically for
                            # diagnostics
                            if frame_count % 1000 == 0 and torch.cuda.is_available():
                                try:
                                    memory_allocated = torch.cuda.memory_allocated(
                                        cuda_device_id) / 1024**3
                                    memory_reserved = torch.cuda.memory_reserved(
                                        cuda_device_id) / 1024**3
                                    print(
                                        f"ðŸ“Š GPU Memory: {
                                            memory_allocated:.2f}GB allocated, {
                                            memory_reserved:.2f}GB reserved")
                                except:
                                    pass
                        except Exception as e:
                            # Fallback to basic inference if optimizations fail
                            print(f"âš  GPU inference error: {e}")
                        # QUICK WIN #3: Use adaptive confidence in fallback
                        adaptive_conf_thresh = get_adaptive_confidence_threshold(
                            frame_queue[0], base_thresh=track_thresh, adaptive_confidence=adaptive_confidence
                        ) if len(frame_queue) > 0 else track_thresh
                        
                        # OPTIMIZATION: Calculate imgsz and use FP16 for fallback GPU path
                        imgsz = None
                        use_half = False
                        if len(frame_queue) > 0:
                            frame_h, frame_w = frame_queue[0].shape[:2]
                            # Round to nearest multiple of 32 (YOLO stride requirement)
                            max_dim = max(frame_w, frame_h)
                            imgsz = int(round(max_dim / 32) * 32)
                            use_half = device == 'cuda' and torch.cuda.is_available()
                        
                        results = model(
                            frame_queue, 
                            classes=[0], 
                            conf=adaptive_conf_thresh, 
                            verbose=False,
                            imgsz=imgsz,
                            half=use_half,
                            max_det=25
                        )
                else:
                    # QUICK WIN #3: Use adaptive confidence for CPU inference
                    adaptive_conf_thresh = get_adaptive_confidence_threshold(
                        frame_queue[0], base_thresh=track_thresh, adaptive_confidence=adaptive_confidence
                    ) if len(frame_queue) > 0 else track_thresh
                    
                    # OPTIMIZATION: Calculate imgsz for CPU inference (no FP16 on CPU)
                    imgsz = None
                    if len(frame_queue) > 0:
                        frame_h, frame_w = frame_queue[0].shape[:2]
                        # Round to nearest multiple of 32 (YOLO stride requirement)
                        max_dim = max(frame_w, frame_h)
                        imgsz = int(round(max_dim / 32) * 32)
                    
                    results = model(
                        frame_queue, 
                        classes=[0], 
                        conf=adaptive_conf_thresh, 
                        verbose=False,
                        imgsz=imgsz,
                        half=False,  # FP16 not supported on CPU
                        max_det=25
                    )

                # Check if results is valid (not None and iterable)
                if results is None:
                    print(
                        f"âš  YOLO model returned None for batch at frame {frame_count}, skipping batch")
                    frame_queue.clear()
                    frame_data_queue.clear()
                    continue

                # Ensure results is iterable (convert to list if it's a single
                # result)
                try:
                    results_list = list(results) if results is not None else []
                except (TypeError, AttributeError):
                    print(
                        f"âš  YOLO results is not iterable at frame {frame_count}, skipping batch")
                    frame_queue.clear()
                    frame_data_queue.clear()
                    continue

                # Verify results length matches frame queue
                if len(results_list) != len(frame_queue):
                    print(
                        f"âš  YOLO results length ({
                            len(results_list)}) doesn't match frame queue ({
                            len(frame_queue)}) at frame {frame_count}")
                    # Use minimum length to avoid index errors
                    min_len = min(
                        len(results_list),
                        len(frame_queue),
                        len(frame_data_queue))
                    results_list = results_list[:min_len]
                    frame_queue = frame_queue[:min_len]
                    frame_data_queue = frame_data_queue[:min_len]

                # Store original frame numbers for later retrieval
                frame_numbers_in_batch = [fd['frame_num']
                                          for fd in frame_data_queue]

                # Process each frame in batch sequentially for tracking
                for batch_idx, (batch_frame, result, frame_data) in enumerate(
                        zip(frame_queue, results_list, frame_data_queue)):
                    try:
                        # Get current frame number early for use throughout frame processing
                        current_frame_num = frame_data.get('frame_num', 0)
                        
                        # Create detections from YOLO result
                        try:
                            # For pose models, we need to handle the result structure differently
                            # Pose models still have boxes, but from_ultralytics might not handle them correctly
                            # When foot_based_tracking is enabled, we're using a pose model, so always use manual extraction
                            if foot_based_tracking:
                                # Pose model: extract boxes manually since from_ultralytics fails with pose models
                                if hasattr(result, 'boxes') and result.boxes is not None:
                                    boxes = result.boxes
                                    try:
                                        # Extract boxes data - handle both tensor and numpy formats
                                        # Check if boxes have data
                                        if hasattr(boxes, 'xyxy') and boxes.xyxy is not None:
                                            xyxy_tensor = boxes.xyxy
                                            
                                            # Debug logging for first few frames
                                            if current_frame_num < 10:
                                                print(f"ðŸ” Frame {current_frame_num}: boxes.xyxy type={type(xyxy_tensor)}, shape={getattr(xyxy_tensor, 'shape', 'no shape')}")
                                            
                                            # Convert to numpy
                                            if hasattr(xyxy_tensor, 'cpu'):
                                                xyxy = xyxy_tensor.cpu().numpy()
                                                confidence = boxes.conf.cpu().numpy() if hasattr(boxes, 'conf') and boxes.conf is not None else np.ones(len(xyxy))
                                                class_id = boxes.cls.cpu().numpy().astype(int) if hasattr(boxes, 'cls') and boxes.cls is not None else np.zeros(len(xyxy), dtype=int)
                                            elif hasattr(xyxy_tensor, 'numpy'):
                                                xyxy = xyxy_tensor.numpy()
                                                confidence = boxes.conf.numpy() if hasattr(boxes, 'conf') and boxes.conf is not None else np.ones(len(xyxy))
                                                class_id = boxes.cls.numpy().astype(int) if hasattr(boxes, 'cls') and boxes.cls is not None else np.zeros(len(xyxy), dtype=int)
                                            else:
                                                xyxy = np.array(xyxy_tensor)
                                                confidence = np.array(boxes.conf) if hasattr(boxes, 'conf') and boxes.conf is not None else np.ones(len(xyxy))
                                                class_id = np.array(boxes.cls).astype(int) if hasattr(boxes, 'cls') and boxes.cls is not None else np.zeros(len(xyxy), dtype=int)
                                            
                                            # Ensure arrays are 2D (N, 4) for xyxy
                                            if len(xyxy.shape) == 1:
                                                if len(xyxy) == 4:
                                                    xyxy = xyxy.reshape(1, -1)
                                                    confidence = np.array([confidence]) if np.isscalar(confidence) else confidence.reshape(-1)
                                                    class_id = np.array([class_id]) if np.isscalar(class_id) else class_id.reshape(-1)
                                                else:
                                                    # Invalid shape
                                                    detections = sv.Detections.empty()
                                            elif len(xyxy.shape) == 0:
                                                # Single value, skip
                                                detections = sv.Detections.empty()
                                            else:
                                                # Check if we have any detections
                                                if len(xyxy) > 0 and xyxy.shape[1] == 4:
                                                    # Ensure confidence and class_id match xyxy length
                                                    if len(confidence) != len(xyxy):
                                                        confidence = np.ones(len(xyxy))
                                                    if len(class_id) != len(xyxy):
                                                        class_id = np.zeros(len(xyxy), dtype=int)
                                                    
                                                    detections = sv.Detections(
                                                        xyxy=xyxy,
                                                        confidence=confidence,
                                                        class_id=class_id
                                                    )
                                                    
                                                    # Debug logging for first few frames - check confidence values
                                                    if current_frame_num <= 10:
                                                        print(f"âœ… Frame {current_frame_num}: Created {len(detections)} detections from pose model")
                                                        print(f"   â†’ Confidence range: {confidence.min():.3f} - {confidence.max():.3f}, mean: {confidence.mean():.3f}")
                                                        print(f"   â†’ Track threshold: {track_thresh}")
                                                        print(f"   â†’ Detections object: len={len(detections)}, xyxy shape={detections.xyxy.shape if detections.xyxy is not None else 'None'}")
                                                else:
                                                    detections = sv.Detections.empty()
                                        else:
                                            # Debug logging
                                            if current_frame_num < 10:
                                                print(f"âš  Frame {current_frame_num}: boxes.xyxy not available or None")
                                            detections = sv.Detections.empty()
                                    except Exception as e:
                                        # Log the error for debugging
                                        if current_frame_num < 100 or current_frame_num % 1000 == 0:
                                            print(f"âš  Pose model box extraction error at frame {current_frame_num}: {e}")
                                            import traceback
                                            if current_frame_num < 10:
                                                traceback.print_exc()
                                        detections = sv.Detections.empty()
                                else:
                                    # Debug logging
                                    if current_frame_num < 10:
                                        print(f"âš  Frame {current_frame_num}: result.boxes not available or None")
                                    detections = sv.Detections.empty()
                            else:
                                # Detection model: use standard method
                                try:
                                    detections = sv.Detections.from_ultralytics(result)
                                except (ValueError, TypeError, AttributeError) as e:
                                    # If from_ultralytics fails, try manual extraction
                                    if hasattr(result, 'boxes') and result.boxes is not None:
                                        try:
                                            boxes = result.boxes
                                            xyxy_tensor = boxes.xyxy
                                            if hasattr(xyxy_tensor, 'cpu'):
                                                xyxy = xyxy_tensor.cpu().numpy()
                                                confidence = boxes.conf.cpu().numpy()
                                                class_id = boxes.cls.cpu().numpy().astype(int)
                                            elif hasattr(xyxy_tensor, 'numpy'):
                                                xyxy = xyxy_tensor.numpy()
                                                confidence = boxes.conf.numpy()
                                                class_id = boxes.cls.numpy().astype(int)
                                            else:
                                                xyxy = np.array(xyxy_tensor)
                                                confidence = np.array(boxes.conf)
                                                class_id = np.array(boxes.cls).astype(int)
                                            
                                            if len(xyxy) > 0:
                                                detections = sv.Detections(
                                                    xyxy=xyxy,
                                                    confidence=confidence,
                                                    class_id=class_id
                                                )
                                            else:
                                                detections = sv.Detections.empty()
                                        except Exception:
                                            detections = sv.Detections.empty()
                                    else:
                                        detections = sv.Detections.empty()
                            
                            # Extract foot keypoints from pose detection if foot_based_tracking is enabled
                            foot_keypoints = None  # Store foot positions: [(x, y), ...] for each detection
                            if foot_based_tracking and hasattr(result, 'keypoints') and result.keypoints is not None:
                                try:
                                    # YOLO pose keypoints: 17 keypoints per person
                                    # Keypoint indices: 15 = left ankle, 16 = right ankle
                                    # Format: keypoints.data shape is (num_detections, 17, 3) where 3 = (x, y, confidence)
                                    keypoints_data = result.keypoints.data.cpu().numpy() if hasattr(result.keypoints.data, 'cpu') else result.keypoints.data
                                    
                                    if len(keypoints_data) > 0 and len(keypoints_data.shape) >= 2:
                                        foot_keypoints = []
                                        for kp in keypoints_data:
                                            if len(kp) >= 17:
                                                # Get left ankle (index 15) and right ankle (index 16)
                                                left_ankle = kp[15] if len(kp) > 15 else None  # [x, y, confidence]
                                                right_ankle = kp[16] if len(kp) > 16 else None  # [x, y, confidence]
                                                
                                                # Use ankle positions if confidence > 0.3, otherwise use bbox bottom
                                                foot_pos = None
                                                if left_ankle is not None and len(left_ankle) >= 3 and left_ankle[2] > 0.3:
                                                    foot_pos = (float(left_ankle[0]), float(left_ankle[1]))
                                                elif right_ankle is not None and len(right_ankle) >= 3 and right_ankle[2] > 0.3:
                                                    foot_pos = (float(right_ankle[0]), float(right_ankle[1]))
                                                
                                                foot_keypoints.append(foot_pos)
                                            else:
                                                foot_keypoints.append(None)
                                        
                                        if current_frame_num % 500 == 0 and any(kp is not None for kp in foot_keypoints):
                                            valid_foot_count = sum(1 for kp in foot_keypoints if kp is not None)
                                            print(f"ðŸ‘Ÿ Frame {current_frame_num}: Extracted {valid_foot_count}/{len(foot_keypoints)} foot keypoints from pose detection")
                                except Exception as e:
                                    if current_frame_num % 500 == 0:
                                        print(f"âš  Could not extract foot keypoints from pose: {e}")
                                    foot_keypoints = None
                            
                            # QUICK WIN #1: Translate detections from ROI space back to full frame space
                            detections_before_roi = len(detections) if detections is not None else 0
                            
                            if roi_bounds is not None:
                                if current_frame_num <= 10:
                                    print(f"ðŸ” Frame {current_frame_num}: Before ROI translation: {detections_before_roi} detections, roi_bounds={roi_bounds}")
                                detections = translate_detections_from_roi(detections, roi_bounds)
                                # Also translate foot keypoints if available
                                if foot_keypoints is not None:
                                    # roi_bounds is (x1, y1, x2, y2), so extract x1 and y1
                                    roi_x, roi_y = roi_bounds[0], roi_bounds[1]
                                    for i, kp in enumerate(foot_keypoints):
                                        if kp is not None:
                                            foot_keypoints[i] = (kp[0] + roi_x, kp[1] + roi_y)
                            
                            if current_frame_num <= 10:
                                detections_after_roi = len(detections) if detections is not None else 0
                                print(f"ðŸ” Frame {current_frame_num}: After ROI translation: {detections_after_roi} detections")
                                if detections_before_roi > 0 and detections_after_roi == 0:
                                    print(f"  âš  Frame {current_frame_num}: ROI translation cleared {detections_before_roi} detections!")
                            
                            # PATCH 1: Scale detections back to original resolution if YOLO was run on resized frame
                            # CRITICAL: This prevents boxes from sliding due to coordinate mismatch
                            detections_before_scale = len(detections) if detections is not None else 0
                            
                            scale_factor = frame_data.get('scale_factor', 1.0)
                            if scale_factor != 1.0 and len(detections) > 0:
                                # Scale xyxy boxes (all 4 coordinates)
                                detections.xyxy = detections.xyxy * scale_factor
                                
                                # Also scale xywh if present (centers and dimensions)
                                if hasattr(detections, 'xywh') and detections.xywh is not None and len(detections.xywh) > 0:
                                    detections.xywh[:, :2] *= scale_factor  # Centers (x, y)
                                    detections.xywh[:, 2:] *= scale_factor  # Width/height
                                
                                # Also scale foot keypoints if available
                                if foot_keypoints is not None:
                                    for i, kp in enumerate(foot_keypoints):
                                        if kp is not None:
                                            foot_keypoints[i] = (kp[0] * scale_factor, kp[1] * scale_factor)
                                
                                # Verification logging (first 10 frames only to avoid spam)
                                if current_frame_num <= 10:
                                    sample_bbox = detections.xyxy[0] if len(detections.xyxy) > 0 else None
                                    if sample_bbox is not None:
                                        print(f"  âœ“ Frame {current_frame_num}: Scaled {len(detections)} detections by {scale_factor:.3f}x (sample bbox: {sample_bbox})")
                            
                            if current_frame_num <= 10:
                                detections_after_scale = len(detections) if detections is not None else 0
                                if detections_before_scale > 0 and detections_after_scale == 0:
                                    print(f"  âš  Frame {current_frame_num}: Scaling cleared {detections_before_scale} detections")
                            
                            # Filter out net detections if remove_net is enabled
                            detections_before_net = len(detections) if detections is not None else 0
                            
                            if remove_net and len(detections) > 0:
                                # Convert detections to format expected by filter_net_detections
                                frame_h, frame_w = batch_frame.shape[:2]
                                dets_array = np.column_stack([
                                    detections.xyxy,
                                    detections.confidence if detections.confidence is not None else np.ones(len(detections)),
                                    detections.class_id if detections.class_id is not None else np.zeros(len(detections))
                                ])
                                # Get exclusion zones from field calibration if available
                                exclude_zones = None
                                if field_calibration and 'exclude_zones' in field_calibration:
                                    exclude_zones = field_calibration['exclude_zones']
                                
                                filtered_dets = filter_net_detections(dets_array, frame_h, frame_w, 
                                                                      exclude_zones=exclude_zones,
                                                                      min_bbox_area=min_bbox_area,
                                                                      min_bbox_width=min_bbox_width,
                                                                      min_bbox_height=min_bbox_height)
                                
                                # Recreate detections from filtered results
                                if len(filtered_dets) > 0:
                                    detections = sv.Detections(
                                        xyxy=filtered_dets[:, :4],
                                        confidence=filtered_dets[:, 4],
                                        class_id=filtered_dets[:, 5].astype(int) if len(filtered_dets[0]) > 5 else None
                                    )
                                else:
                                    detections = sv.Detections.empty()
                            
                            if current_frame_num <= 10:
                                detections_after_net = len(detections) if detections is not None else 0
                                if detections_before_net > 0 and detections_after_net == 0:
                                    print(f"  âš  Frame {current_frame_num}: Net filtering cleared {detections_before_net} detections")
                            
                            # PERFORMANCE: Reduced logging frequency (was every 100 frames)
                            # Log detection count for diagnostics
                            if current_frame_num % 500 == 0:  # Log every 500 frames (reduced from 100)
                                # Only print detection count every 500 frames to reduce spam
                                # (0 detections is normal when players leave field, no need to log every frame)
                                if len(detections) > 0:
                                    print(
                                        f"ðŸ“Š Frame {current_frame_num}: YOLO detected {len(detections)} players")
                        except Exception as e:
                            # Only log the error occasionally to avoid spam, and suppress the unpacking error
                            # which is a known issue with supervision library and pose models
                            if current_frame_num <= 10 or (current_frame_num % 100 == 0 and "unpack" not in str(e).lower()):
                                print(
                                    f"âš  Failed to create detections from YOLO result at frame {
                                        frame_data['frame_num']}: {e}")
                                if current_frame_num <= 10:
                                    import traceback
                                    traceback.print_exc()
                            detections = sv.Detections.empty()

                        # Safety check: ensure detections is not None
                        if detections is None:
                            detections = sv.Detections.empty()
                        
                        # ðŸš« AGGRESSIVE BALL FILTERING: Filter out balls before tracking
                        # Balls are being detected as players and getting track IDs
                        # Filter criteria: size, aspect ratio, and height
                        if len(detections) > 0:
                            ball_filter_mask = np.ones(len(detections), dtype=bool)  # Start with all True
                            
                            for i in range(len(detections)):
                                x1, y1, x2, y2 = detections.xyxy[i]
                                bbox_width = x2 - x1
                                bbox_height = y2 - y1
                                bbox_area = bbox_width * bbox_height
                                aspect_ratio = bbox_height / bbox_width if bbox_width > 0 else 0
                                
                                # BALL FILTERING CRITERIA (more aggressive than player_gallery_seeder):
                                # 1. Small area: Balls are typically < 3000 pixels (players are > 5000)
                                # 2. Circular shape: Balls have aspect ratio ~0.8-1.2 (players are > 1.3)
                                # 3. Short height: Balls are < 80 pixels tall (players are > 100)
                                # 4. Small width: Balls are < 80 pixels wide (players are > 50)
                                
                                min_area_for_player = 3000  # Increased from 2000 to be more aggressive
                                min_height_for_player = 80   # Increased from 60 to be more aggressive
                                min_aspect_ratio_for_player = 1.3  # Increased from 1.1 - players are clearly taller
                                max_aspect_ratio_for_ball = 1.2  # Balls are roughly circular (ratio ~1.0)
                                
                                # Check if this looks like a ball
                                is_likely_ball = False
                                
                                # Criterion 1: Small area
                                if bbox_area < min_area_for_player:
                                    is_likely_ball = True
                                
                                # Criterion 2: Circular shape (aspect ratio close to 1.0)
                                if aspect_ratio < min_aspect_ratio_for_player:
                                    # If aspect ratio is close to 1.0 (circular), it's likely a ball
                                    if 0.7 <= aspect_ratio <= max_aspect_ratio_for_ball:
                                        is_likely_ball = True
                                
                                # Criterion 3: Short height
                                if bbox_height < min_height_for_player:
                                    is_likely_ball = True
                                
                                # Criterion 4: Small and square-ish (ball characteristics)
                                if bbox_area < 2000 and 0.8 <= aspect_ratio <= 1.2:
                                    is_likely_ball = True
                                
                                # If it looks like a ball, filter it out
                                if is_likely_ball:
                                    ball_filter_mask[i] = False
                                    if current_frame_num % 500 == 0:
                                        print(f"ðŸš« Filtered ball detection: area={bbox_area:.0f}, height={bbox_height:.0f}, aspect={aspect_ratio:.2f}")
                            
                            # Apply filter mask
                            if not np.all(ball_filter_mask):
                                filtered_count = np.sum(~ball_filter_mask)
                                if current_frame_num % 500 == 0 or current_frame_num <= 10:
                                    print(f"ðŸš« Ball filtering: Removed {filtered_count} ball detection(s) from {len(detections)} total detections")
                                
                                # Create filtered detections
                                if np.any(ball_filter_mask):
                                    detections = detections[ball_filter_mask]
                                else:
                                    detections = sv.Detections.empty()
                        
                        # Store raw YOLO detections (before tracking) for visualization
                        raw_yolo_detections = None
                        if show_yolo_boxes and detections is not None and len(detections) > 0:
                            # Copy detections before tracking (store xyxy, confidence, class_id)
                            raw_yolo_detections = {
                                'xyxy': detections.xyxy.copy() if hasattr(detections, 'xyxy') and detections.xyxy is not None else None,
                                'confidence': detections.confidence.copy() if hasattr(detections, 'confidence') and detections.confidence is not None else None,
                                'class_id': detections.class_id.copy() if hasattr(detections, 'class_id') and detections.class_id is not None else None
                            }
                        
                        # Debug logging for first few frames - check detections after all processing
                        if current_frame_num <= 10:
                            detections_after_processing = len(detections) if detections is not None else 0
                            xyxy_shape = detections.xyxy.shape if detections is not None and detections.xyxy is not None else "None"
                            print(f"ðŸ” Frame {current_frame_num}: After all processing (before safety checks): {detections_after_processing} detections, xyxy shape={xyxy_shape}")

                        # Safety check: ensure detections attributes are valid
                        # (not None)
                        if current_frame_num <= 10:
                            has_xyxy = hasattr(detections, 'xyxy')
                            xyxy_is_none = detections.xyxy is None if has_xyxy else True
                            xyxy_len = len(detections.xyxy) if has_xyxy and detections.xyxy is not None else 0
                            print(f"ðŸ” Frame {current_frame_num}: Safety check - has_xyxy={has_xyxy}, xyxy_is_none={xyxy_is_none}, xyxy_len={xyxy_len}")
                        
                        if not hasattr(
                                detections, 'xyxy') or detections.xyxy is None:
                            if current_frame_num <= 10:
                                print(f"  âš  Frame {current_frame_num}: Clearing xyxy because it's None or missing")
                            detections.xyxy = np.array([]).reshape(0, 4)
                        # CRITICAL: tracker_id should be None (not empty array) if no tracking IDs assigned yet
                        # Empty arrays (size 0) cause issues when applying
                        # masks - supervision library can't handle them
                        if not hasattr(detections, 'tracker_id'):
                            detections.tracker_id = None
                        elif detections.tracker_id is not None and isinstance(detections.tracker_id, np.ndarray):
                            # If tracker_id is an empty array (size 0), set it
                            # to None
                            if len(
                                    detections.tracker_id.shape) > 0 and detections.tracker_id.shape[0] == 0:
                                detections.tracker_id = None
                        if not hasattr(
                                detections,
                                'confidence') or detections.confidence is None:
                            detections.confidence = np.array([])

                        # PATCH 1: Scale detections back to original resolution if YOLO was run on resized frame
                        # CRITICAL: This prevents boxes from sliding due to coordinate mismatch
                        # (scale_factor was already applied to foot_keypoints above if they exist)
                        scale_factor = frame_data.get('scale_factor', 1.0)
                        if scale_factor != 1.0 and len(detections) > 0:
                            # Scale xyxy boxes (all 4 coordinates)
                            detections.xyxy = detections.xyxy * scale_factor
                            
                            # Also scale xywh if present (centers and dimensions)
                            if hasattr(detections, 'xywh') and detections.xywh is not None and len(detections.xywh) > 0:
                                detections.xywh[:, :2] *= scale_factor  # Centers (x, y)
                                detections.xywh[:, 2:] *= scale_factor  # Width/height
                            
                            # Verification logging (first 10 frames only to avoid spam)
                            if current_frame_num <= 10:
                                sample_bbox = detections.xyxy[0] if len(detections.xyxy) > 0 else None
                                if sample_bbox is not None:
                                    print(f"  âœ“ Frame {current_frame_num}: Scaled {len(detections)} detections by {scale_factor:.3f}x (sample bbox: {sample_bbox})")

                        # Apply foot-based bounding box adjustment for more
                        # stable tracking using actual foot keypoints from YOLO pose
                        detections_before_foot = len(detections) if detections is not None else 0
                        
                        if foot_based_tracking and len(detections) > 0:
                            adjusted_xyxy = detections.xyxy.copy()
                            for i in range(len(detections)):
                                x1, y1, x2, y2 = detections.xyxy[i]
                                box_height = y2 - y1
                                box_width = x2 - x1
                                center_x = (x1 + x2) / 2
                                
                                # Use actual foot keypoint if available, otherwise use bbox bottom
                                if foot_keypoints is not None and i < len(foot_keypoints) and foot_keypoints[i] is not None:
                                    foot_x, foot_y = foot_keypoints[i]
                                    # Use foot keypoint position for tracking anchor
                                    foot_y_anchor = foot_y
                                    # Adjust center X to foot position if available
                                    center_x = foot_x
                                else:
                                    # Fallback: use bottom of bounding box
                                    foot_y_anchor = y2
                                
                                # Shrink box height by 20% and anchor to foot position
                                new_height = box_height * 0.8
                                adjusted_xyxy[i][1] = foot_y_anchor - new_height  # New top anchored to foot
                                adjusted_xyxy[i][3] = foot_y_anchor  # Keep bottom at foot position
                                # Center box on foot X position
                                adjusted_xyxy[i][0] = center_x - box_width / 2
                                adjusted_xyxy[i][2] = center_x + box_width / 2
                            detections.xyxy = adjusted_xyxy
                            
                            # Store foot keypoints in frame_data for later use (tracking, visualization, etc.)
                            if foot_keypoints is not None:
                                frame_data['foot_keypoints'] = foot_keypoints
                        
                        if current_frame_num <= 10:
                            detections_after_foot = len(detections) if detections is not None else 0
                            if detections_before_foot > 0 and detections_after_foot == 0:
                                print(f"  âš  Frame {current_frame_num}: Foot-based adjustment cleared {detections_before_foot} detections")

                        # Enhanced confidence filtering (Option A)
                        detections_before_confidence = len(
                            detections) if detections is not None else 0
                        if confidence_filtering and len(
                                detections) > 0 and ENHANCED_TRACKING_AVAILABLE:
                            try:
                                # Use enhanced confidence filtering with adaptive threshold
                                # Safety check: ensure detections has
                                # confidence array and is not empty
                                if (len(detections) > 0 and hasattr(detections, 'confidence') and
                                        detections.confidence is not None and len(detections.confidence) == len(detections)):
                                    # Store detections length before calling
                                    # filter_by_confidence
                                    detections_len_before = len(detections)
                                    # ENHANCED: Adaptive confidence threshold based on bounding box size
                                    # Smaller players (under 8 years old) need lower thresholds
                                    # Calculate adaptive threshold for each
                                    # detection
                                    conf_mask = np.ones(
                                        # Start with all True
                                        len(detections), dtype=bool)

                                    for idx in range(len(detections)):
                                        if hasattr(
                                                detections, 'confidence') and detections.confidence is not None:
                                            conf = detections.confidence[idx]

                                            # Get bounding box size
                                            xyxy = detections.xyxy[idx]
                                            box_width = xyxy[2] - xyxy[0]
                                            box_height = xyxy[3] - xyxy[1]
                                            box_area = box_width * box_height

                                            # CRITICAL: Lowered base threshold to detect small/distant players
                                            # Original: min(0.10, track_thresh - 0.15) was too high
                                            base_min_conf = min(
                                                0.05, track_thresh - 0.20) if track_thresh > 0.20 else 0.05  # Lowered from 0.10 to 0.05

                                            # ENHANCED: Lower threshold for small players
                                            # Small players typically have box area < 5000 pixels (roughly 50x100 or smaller)
                                            # For very small boxes, reduce threshold even more
                                            # Very small (distant players) - lowered thresholds
                                            if box_area < 2000:  # Very small/distant players
                                                adaptive_min_conf = max(
                                                    0.03, base_min_conf - 0.02)  # Even lower threshold (lowered from 0.05)
                                            elif box_area < 3000:  # Small players
                                                adaptive_min_conf = max(
                                                    0.04, base_min_conf - 0.03)  # Lower threshold (lowered from 0.05)
                                            elif box_area < 5000:  # Medium-small players
                                                adaptive_min_conf = max(
                                                    0.05, base_min_conf - 0.02)  # Lower threshold (lowered from 0.08)
                                            else:  # Normal size
                                                adaptive_min_conf = base_min_conf

                                            # Apply threshold
                                            conf_mask[idx] = conf >= adaptive_min_conf

                                    # If enhanced tracking available, also use
                                    # its adaptive features
                                    if adaptive_confidence and len(
                                            detections) > 0:
                                        try:
                                            # Use enhanced filter for
                                            # additional adaptive features
                                            enhanced_mask = filter_by_confidence(
                                                detections,
                                                min_confidence=0.05,  # Very low base, size adaptation handled above
                                                adaptive_threshold=True,
                                                confidence_history=confidence_history
                                            )
                                            # Combine masks (both must pass)
                                            if isinstance(enhanced_mask, np.ndarray) and len(
                                                    enhanced_mask) == len(conf_mask):
                                                conf_mask = conf_mask & enhanced_mask
                                        except:
                                            pass  # Fallback to size-based mask only
                                    # Validate mask immediately after creation
                                    if not isinstance(conf_mask, np.ndarray):
                                        conf_mask = np.array(conf_mask)
                                    # Apply mask using safe helper function
                                    if (len(detections) > 0 and len(conf_mask) == len(detections) and
                                            len(conf_mask) == detections_len_before and not np.all(conf_mask)):
                                        detections_removed = detections_before_confidence - \
                                            np.sum(conf_mask)
                                        if detections_removed > 0 and frame_data.get(
                                                'frame_num', 0) % 100 == 0:  # Log every 100 frames
                                            print(
                                                f"ðŸ“Š Frame {
                                                    frame_data.get(
                                                        'frame_num',
                                                        'unknown')}: Confidence filter removed {detections_removed} detections ({detections_before_confidence} â†’ {
                                                    np.sum(conf_mask)})")
                                        success, detections = safe_apply_detection_mask(
                                            detections, conf_mask,
                                            frame_num=frame_data.get(
                                                'frame_num', 'unknown')
                                        )
                                        if not success:
                                            # Mask application failed -
                                            # detections unchanged, continue
                                            # processing
                                            pass
                            except Exception as e:
                                # Fallback to simple filtering
                                # Re-check detections is not empty before
                                # applying mask
                                if (len(detections) > 0 and hasattr(detections, 'confidence') and
                                        detections.confidence is not None and len(detections.confidence) == len(detections)):
                                    # IMPROVED: Much lower threshold (was 0.15,
                                    # -0.10) to prevent dropping detections
                                    min_conf = max(0.10, track_thresh - 0.15)
                                    conf_mask = detections.confidence >= min_conf
                                    # Apply mask using safe helper function
                                    if len(detections) > 0 and len(conf_mask) == len(
                                            detections) and not np.all(conf_mask):
                                        success, detections = safe_apply_detection_mask(
                                            detections, conf_mask,
                                            frame_num=frame_data.get(
                                                'frame_num', 'unknown')
                                        )
                                        if not success:
                                            # Mask application failed -
                                            # detections unchanged, continue
                                            # processing
                                            pass
                        elif len(detections) > 0:
                            # Simple confidence filtering (fallback)
                            # Safety check: ensure confidence array exists and
                            # matches detections size
                            if (hasattr(detections, 'confidence') and detections.confidence is not None and
                                    len(detections.confidence) == len(detections)):
                                # IMPROVED: Much lower threshold (was 0.18,
                                # -0.05) to prevent dropping detections
                                min_conf = max(0.10, track_thresh - 0.15)
                                conf_mask = detections.confidence >= min_conf
                                # Apply mask using safe helper function
                                if len(detections) > 0 and len(conf_mask) == len(
                                        detections) and not np.all(conf_mask):
                                    success, detections = safe_apply_detection_mask(
                                        detections, conf_mask,
                                        frame_num=frame_data.get(
                                            'frame_num', 'unknown')
                                    )
                                    if not success:
                                        # Mask application failed - detections
                                        # unchanged, continue processing
                                        pass

                        # Filter out ball-sized detections (size and aspect
                        # ratio check)
                        detections_before_ball_filter = len(
                            detections) if detections is not None else 0
                        # CRITICAL: Check actual xyxy array size, not just
                        # len(detections)
                        xyxy_actual_len = 0
                        if hasattr(detections,
                                   'xyxy') and detections.xyxy is not None:
                            try:
                                xyxy_actual_len = detections.xyxy.shape[0] if len(
                                    detections.xyxy.shape) > 0 else 0
                            except (AttributeError, IndexError):
                                xyxy_actual_len = 0

                        if len(detections) > 0 and xyxy_actual_len > 0 and xyxy_actual_len == len(
                                detections):
                            # Store detections length when creating mask
                            detections_len_when_mask_created = len(detections)
                            keep_mask = np.ones(len(detections), dtype=bool)
                            for i, xyxy in enumerate(detections.xyxy):
                                x1, y1, x2, y2 = xyxy
                                box_width = x2 - x1
                                box_height = y2 - y1
                                box_area = box_width * box_height
                                aspect_ratio = box_height / box_width if box_width > 0 else 0

                                # Reject detections that are too small (likely balls or false positives)
                                # Players should be at least 30 pixels tall in
                                # CRITICAL: Lowered thresholds to detect small/distant players
                                # Scale with resolution - much more lenient for distant players
                                min_player_height = 15 * (width / 3840.0)  # Lowered from 30 to 15
                                # Much lower area threshold for more detections (lowered from 1000)
                                min_player_area = 200 * \
                                    (width / 3840.0) * (height / 2160.0)  # Lowered from 1000 to 200

                                # Reject if too small (only very small detections)
                                # CRITICAL: Only reject if significantly below minimum (allow some margin)
                                if box_height < min_player_height * 0.8 or box_area < min_player_area * 0.8:
                                    keep_mask[i] = False
                                    continue

                                # ðŸš« AGGRESSIVE BALL FILTERING: Reject circular/square detections
                                # Players are typically tall rectangles (aspect ratio > 1.3), balls are circular (0.8-1.2)
                                # More aggressive filtering to prevent balls from getting track IDs
                                
                                # Criterion 1: Very square/circular AND small = definitely a ball
                                if 0.8 <= aspect_ratio <= 1.2 and box_area < 3000:
                                    # Circular and small = likely ball
                                    keep_mask[i] = False
                                    if current_frame_num % 500 == 0:
                                        print(f"ðŸš« Filtered ball (circular + small): area={box_area:.0f}, aspect={aspect_ratio:.2f}")
                                    continue
                                
                                # Criterion 2: Small area AND short height = likely ball
                                if box_area < 3000 and box_height < 80:
                                    # Small and short = likely ball
                                    keep_mask[i] = False
                                    if current_frame_num % 500 == 0:
                                        print(f"ðŸš« Filtered ball (small + short): area={box_area:.0f}, height={box_height:.0f}")
                                    continue
                                
                                # Criterion 3: Aspect ratio too low (players are taller than wide)
                                if aspect_ratio < 1.3:
                                    # If it's also small, it's likely a ball
                                    if box_area < 4000 or box_height < 90:
                                        keep_mask[i] = False
                                        if current_frame_num % 500 == 0:
                                            print(f"ðŸš« Filtered ball (low aspect + small): area={box_area:.0f}, aspect={aspect_ratio:.2f}")
                                    continue

                            # Apply mask using safe helper function
                            if not np.all(keep_mask):
                                detections_removed = detections_before_ball_filter - \
                                    np.sum(keep_mask)
                                if detections_removed > 0 and frame_data.get(
                                        'frame_num', 0) % 100 == 0:  # Log every 100 frames
                                    print(
                                        f"ðŸ“Š Frame {
                                            frame_data.get(
                                                'frame_num',
                                                'unknown')}: Ball-size filter removed {detections_removed} detections ({detections_before_ball_filter} â†’ {
                                            np.sum(keep_mask)})")
                                success, detections = safe_apply_detection_mask(
                                    detections, keep_mask,
                                    frame_num=frame_data.get(
                                        'frame_num', 'unknown')
                                )
                                if not success:
                                    # Mask application failed - detections
                                    # unchanged, continue processing
                                    pass

                        # ðŸŽ¯ FIELD BOUNDARY FILTERING: Filter detections clearly outside field bounds
                        # This helps reduce false positives (referees, coaches, spectators, etc.)
                        # Uses field calibration boundary as-is (field calibration should already include sidelines)
                        # CRITICAL FIX: Disable filter if it's removing too many detections (likely incorrect calibration)
                        detections_before_field_filter = len(detections) if detections is not None else 0
                        if detections is not None and len(detections) > 0 and field_calibration is not None and not field_boundary_filter_disabled:
                            # Debug logging for first few frames
                            if current_frame_num <= 10:
                                print(f"ðŸ” Frame {current_frame_num}: Before field filter: {detections_before_field_filter} detections")
                            # Only filter if field calibration exists and has valid points
                            if "points" in field_calibration and len(field_calibration["points"]) >= 4:
                                keep_mask = np.ones(len(detections), dtype=bool)
                                detections_filtered = 0
                                
                                for det_idx in range(len(detections)):
                                    if detections.xyxy is not None and det_idx < len(detections.xyxy):
                                        # Check if detection center is within field bounds
                                        bbox = detections.xyxy[det_idx]
                                        center_x = (bbox[0] + bbox[2]) / 2.0
                                        center_y = (bbox[1] + bbox[3]) / 2.0
                                        
                                        # Check if center is within field bounds (strict_mode=False uses boundary as-is)
                                        # Field calibration should already include sidelines, so this filters out
                                        # detections clearly outside the field (stands, benches, etc.)
                                        is_within_field = is_point_in_field(
                                            (center_x, center_y),
                                            field_calibration,
                                            strict_mode=False,  # Use field boundary as-is (no shrinking)
                                            margin_pixels=0  # No margin - use calibration boundary exactly
                                        )
                                        
                                        if not is_within_field:
                                            keep_mask[det_idx] = False
                                            detections_filtered += 1
                                
                                # SAFETY CHECK: If filter is removing >80% of detections, it's likely incorrect
                                # Disable the filter permanently to prevent removing all valid player detections
                                filter_ratio = detections_filtered / len(detections) if len(detections) > 0 else 0
                                if filter_ratio > 0.8:
                                    # Filter is too aggressive - likely incorrect field calibration
                                    # Disable permanently for the rest of the video
                                    field_boundary_filter_disabled = True
                                    print(f"  âš  FIELD BOUNDARY FILTER: PERMANENTLY DISABLED - was removing {detections_filtered}/{len(detections)} detections ({filter_ratio*100:.1f}%)")
                                    print(f"     â†’ Field calibration appears incorrect or doesn't match this video")
                                    print(f"     â†’ All detections will be kept to prevent data loss")
                                    print(f"     â†’ This is normal if:")
                                    print(f"       â€¢ Analyzing a different sport (basketball, volleyball, etc.)")
                                    print(f"       â€¢ Field calibration was done for a different field/court")
                                    print(f"       â€¢ Camera angle or position changed")
                                    print(f"       â€¢ Spectators/coaches are visible in the frame")
                                    print(f"     â†’ Tip: Re-calibrate field for this specific video if needed, or ignore this warning")
                                elif detections_filtered > 0:
                                    # Apply filter if it's reasonable
                                    success, detections = safe_apply_detection_mask(
                                        detections, keep_mask,
                                        frame_num=current_frame_num
                                    )
                                    # Debug logging for first few frames
                                    if current_frame_num <= 10:
                                        print(f"ðŸ” Frame {current_frame_num}: After field filter: {len(detections) if detections is not None else 0} detections (removed {detections_filtered})")
                        
                        # ðŸŽ¯ ALTERNATIVE SPECTATOR FILTERING: If field boundary filter is disabled, use size/position-based filtering
                        # This helps filter out spectators when field calibration doesn't match
                        if detections is not None and len(detections) > 0 and field_boundary_filter_disabled:
                            # Only apply if we have more detections than expected players (likely includes spectators)
                            if len(detections) > max_players * 1.5:  # 50% more than expected players
                                keep_mask = np.ones(len(detections), dtype=bool)
                                detections_filtered = 0
                                
                                # Get frame dimensions for position-based filtering
                                if len(detections.xyxy) > 0:
                                    frame_h, frame_w = None, None
                                    # Try multiple sources for frame dimensions
                                    if len(frame_queue) > 0:
                                        frame_h, frame_w = frame_queue[0].shape[:2]
                                    elif 'height' in locals() and 'width' in locals():
                                        frame_h, frame_w = height, width
                                    elif 'yolo_height' in locals() and 'yolo_width' in locals():
                                        frame_h, frame_w = yolo_height, yolo_width
                                    
                                    if frame_h and frame_w:
                                        # Calculate detection properties
                                        bbox_areas = []
                                        bbox_heights = []
                                        bbox_y_centers = []
                                        
                                        for det_idx in range(len(detections)):
                                            if detections.xyxy is not None and det_idx < len(detections.xyxy):
                                                bbox = detections.xyxy[det_idx]
                                                width = bbox[2] - bbox[0]
                                                height = bbox[3] - bbox[1]
                                                area = width * height
                                                center_y = (bbox[1] + bbox[3]) / 2.0
                                                
                                                bbox_areas.append(area)
                                                bbox_heights.append(height)
                                                bbox_y_centers.append(center_y)
                                        
                                        if len(bbox_areas) > 0:
                                            # Calculate statistics
                                            areas = np.array(bbox_areas)
                                            heights = np.array(bbox_heights)
                                            y_centers = np.array(bbox_y_centers)
                                            
                                            # Filter criteria:
                                            # 1. Size: Keep larger detections (players are usually larger than distant spectators)
                                            # 2. Position: Keep detections in lower 80% of frame (spectators usually in upper portion)
                                            # 3. Use median as threshold (more robust than mean)
                                            
                                            median_area = np.median(areas)
                                            median_height = np.median(heights)
                                            
                                            # Keep detections that are:
                                            # - Larger than 60% of median size (filters out very small spectators)
                                            # - Position-based: Camera location determines where spectators are
                                            #   * Camera in stands (common): Spectators at bottom, players at top
                                            #   * Camera on field: Spectators at top, players at bottom
                                            
                                            size_threshold = median_area * 0.6
                                            
                                            # Detect camera position: If most detections are in lower portion, camera is likely in stands
                                            # If most detections are in upper portion, camera is likely on field
                                            lower_half_count = np.sum(y_centers > frame_h * 0.5)
                                            upper_half_count = np.sum(y_centers <= frame_h * 0.5)
                                            
                                            # Determine spectator location based on camera position
                                            if lower_half_count > upper_half_count * 1.5:
                                                # Most detections in lower half - camera likely in stands, spectators at bottom
                                                spectator_zone_start = frame_h * 0.75  # Bottom 25% of frame
                                                player_zone_end = frame_h * 0.85  # Top 85% is player zone
                                                camera_in_stands = True
                                            else:
                                                # Most detections in upper half - camera likely on field, spectators at top
                                                spectator_zone_start = 0  # Top of frame
                                                spectator_zone_end = frame_h * 0.25  # Top 25% of frame
                                                player_zone_end = frame_h * 0.75  # Bottom 75% is player zone
                                                camera_in_stands = False
                                            
                                            for det_idx in range(len(detections)):
                                                if detections.xyxy is not None and det_idx < len(detections.xyxy):
                                                    area = areas[det_idx]
                                                    center_y = y_centers[det_idx]
                                                    
                                                    # Filter out extremely small detections (likely distant spectators)
                                                    if area < size_threshold * 0.4:
                                                        keep_mask[det_idx] = False
                                                        detections_filtered += 1
                                                    elif camera_in_stands:
                                                        # Camera in stands: Filter small detections in bottom portion (spectators)
                                                        if center_y > spectator_zone_start and area < median_area * 0.8:
                                                            # Small detection in bottom 25% - likely spectator
                                                            keep_mask[det_idx] = False
                                                            detections_filtered += 1
                                                    else:
                                                        # Camera on field: Filter small detections in top portion (spectators)
                                                        if center_y < spectator_zone_end and area < median_area * 0.8:
                                                            # Small detection in top 25% - likely spectator
                                                            keep_mask[det_idx] = False
                                                            detections_filtered += 1
                                            
                                            # Only apply if we're filtering a reasonable amount (not too aggressive)
                                            if detections_filtered > 0 and detections_filtered < len(detections) * 0.7:
                                                success, detections = safe_apply_detection_mask(
                                                    detections, keep_mask,
                                                    frame_num=current_frame_num
                                                )
                                                if current_frame_num <= 10:
                                                    print(f"ðŸ” Frame {current_frame_num}: Spectator filter removed {detections_filtered} detection(s) (size/position-based)")
                                            elif detections_filtered >= len(detections) * 0.7:
                                                # Too aggressive - don't apply
                                                if current_frame_num <= 10:
                                                    print(f"ðŸ” Frame {current_frame_num}: Spectator filter too aggressive ({detections_filtered}/{len(detections)}), skipping")
                                    elif current_frame_num % 100 == 0:
                                        # PERFORMANCE: Reduced field boundary filter logging (was every 100 frames)
                                        if current_frame_num % 500 == 0:  # Log every 500 frames
                                            print(f"  ðŸŽ¯ FIELD BOUNDARY FILTER: Removed {detections_filtered} detection(s) outside field bounds (frame {current_frame_num})")
                        elif detections is not None and len(detections) > 0 and field_calibration is not None:
                            # Field calibration exists but doesn't have valid points
                            if current_frame_num % 1000 == 0:
                                print(f"  âš  FIELD BOUNDARY FILTER: Field calibration exists but has invalid points (frame {current_frame_num})")

                        # ðŸŽ¯ REALITY CHECK 1: Maximum players per frame (hard limit enforcement)
                        if detections is not None and len(detections) > 0:
                            if len(detections) > MAX_PLAYERS_PER_FRAME:
                                # Sort by confidence and keep only top N
                                if hasattr(detections, 'confidence') and detections.confidence is not None:
                                    # Get indices sorted by confidence (highest first)
                                    sorted_indices = np.argsort(detections.confidence)[::-1]
                                    keep_indices = sorted_indices[:MAX_PLAYERS_PER_FRAME]
                                    
                                    # Create mask for top N detections
                                    keep_mask = np.zeros(len(detections), dtype=bool)
                                    keep_mask[keep_indices] = True
                                    
                                    # Apply mask
                                    detections_removed = len(detections) - MAX_PLAYERS_PER_FRAME
                                    # Log more frequently to help diagnose missing players
                                    if current_frame_num % 50 == 0 or detections_removed > 0:
                                        print(f"  ðŸŽ¯ MAX PLAYERS ENFORCEMENT: Frame {current_frame_num} had {len(detections)} detections, limited to {MAX_PLAYERS_PER_FRAME} (removed {detections_removed} lowest confidence)")
                                        if detections_removed > 0:
                                            # Show confidence range of removed detections
                                            removed_confidences = detections.confidence[sorted_indices[MAX_PLAYERS_PER_FRAME:]]
                                            if len(removed_confidences) > 0:
                                                print(f"     â†’ Removed detections confidence range: {removed_confidences.min():.3f} - {removed_confidences.max():.3f}")
                                    
                                    success, detections = safe_apply_detection_mask(
                                        detections, keep_mask,
                                        frame_num=current_frame_num
                                    )
                                else:
                                    # No confidence available - just truncate (shouldn't happen)
                                    if current_frame_num % 100 == 0:
                                        print(f"  âš  MAX PLAYERS ENFORCEMENT: Frame {current_frame_num} had {len(detections)} detections but no confidence - truncating to {MAX_PLAYERS_PER_FRAME}")
                                    # Truncate detections (last resort)
                                    detections = sv.Detections(
                                        xyxy=detections.xyxy[:MAX_PLAYERS_PER_FRAME],
                                        confidence=detections.confidence[:MAX_PLAYERS_PER_FRAME] if detections.confidence is not None else None,
                                        class_id=detections.class_id[:MAX_PLAYERS_PER_FRAME] if detections.class_id is not None else None,
                                        tracker_id=detections.tracker_id[:MAX_PLAYERS_PER_FRAME] if detections.tracker_id is not None else None
                                    )

                        # Initialize track position history for erratic track detection (if not exists)
                        if 'track_position_history_for_erratic' not in locals():
                            track_position_history_for_erratic = {}
                        
                        # ðŸŽ¯ REALITY CHECK 2: Validate each detection's physical constraints
                        if detections is not None and len(detections) > 0 and detections.xyxy is not None:
                            reality_check_violations = []
                            corrected_bboxes = []
                            invalid_indices = []
                            
                            for det_idx in range(len(detections)):
                                xyxy = detections.xyxy[det_idx]
                                track_id = detections.tracker_id[det_idx] if detections.tracker_id is not None else None
                                
                                # Get previous bbox and velocity for this track
                                prev_xyxy = None
                                prev_frame = None
                                prev_velocity = None
                                if track_id is not None:
                                    track_id_int = int(track_id)
                                    if track_id_int in track_previous_bbox:
                                        prev_xyxy = track_previous_bbox[track_id_int]
                                        prev_frame = track_previous_frame.get(track_id_int)
                                        prev_velocity = track_previous_velocity.get(track_id_int)
                                
                                # Validate reality checks
                                is_valid, corrected_xyxy, violations = validate_reality_checks(
                                    track_id=track_id,
                                    xyxy=xyxy,
                                    prev_xyxy=prev_xyxy,
                                    prev_frame=prev_frame,
                                    current_frame=current_frame_num,
                                    fps=fps,
                                    max_pixels_per_frame=MAX_PIXELS_PER_FRAME,
                                    max_acceleration=MAX_ACCELERATION_PX_PER_FRAME2,
                                    max_position_change=MAX_POSITION_CHANGE_PX,
                                    min_bbox_area=MIN_BBOX_AREA,
                                    max_bbox_area=MAX_BBOX_AREA,
                                    min_bbox_width=MIN_BBOX_WIDTH,
                                    min_bbox_height=MIN_BBOX_HEIGHT,
                                    max_bbox_width=MAX_BBOX_WIDTH,
                                    max_bbox_height=MAX_BBOX_HEIGHT,
                                    width=width,
                                    height=height,
                                    track_previous_velocity=prev_velocity,
                                    enable_checks=True
                                )
                                
                                if not is_valid:
                                    # Invalid detection - mark for removal
                                    invalid_indices.append(det_idx)
                                    if current_frame_num % 100 == 0:
                                        print(f"  âš  REALITY CHECK FAILED: Track {track_id} at frame {current_frame_num}: {violations}")
                                else:
                                    # Valid but may have been corrected
                                    corrected_bboxes.append(corrected_xyxy)
                                    if len(violations) > 0 and current_frame_num % 100 == 0:
                                        print(f"  âœ“ REALITY CHECK CORRECTED: Track {track_id} at frame {current_frame_num}: {violations}")
                            
                            # Remove invalid detections
                            if len(invalid_indices) > 0:
                                keep_mask = np.ones(len(detections), dtype=bool)
                                keep_mask[invalid_indices] = False
                                success, detections = safe_apply_detection_mask(
                                    detections, keep_mask,
                                    frame_num=current_frame_num
                                )
                                if current_frame_num % 100 == 0:
                                    print(f"  ðŸŽ¯ REALITY CHECK: Removed {len(invalid_indices)} invalid detections at frame {current_frame_num}")
                            
                            # Apply corrected bboxes
                            if len(corrected_bboxes) > 0 and len(corrected_bboxes) == len(detections):
                                detections.xyxy = np.array(corrected_bboxes)
                            
                            # Update track history for next frame (AFTER validation)
                            if detections.tracker_id is not None:
                                for det_idx, track_id in enumerate(detections.tracker_id):
                                    if track_id is not None and det_idx < len(detections.xyxy):
                                        track_id_int = int(track_id)
                                        
                                        # Calculate velocity BEFORE updating bbox
                                        prev_frame = track_previous_frame.get(track_id_int)
                                        prev_xyxy = track_previous_bbox.get(track_id_int)
                                        
                                        if prev_frame is not None and prev_xyxy is not None and current_frame_num > prev_frame:
                                            frame_diff = current_frame_num - prev_frame
                                            if frame_diff > 0:
                                                curr_center_x = (detections.xyxy[det_idx][0] + detections.xyxy[det_idx][2]) / 2.0
                                                curr_center_y = (detections.xyxy[det_idx][1] + detections.xyxy[det_idx][3]) / 2.0
                                                prev_center_x = (prev_xyxy[0] + prev_xyxy[2]) / 2.0
                                                prev_center_y = (prev_xyxy[1] + prev_xyxy[3]) / 2.0
                                                vx = (curr_center_x - prev_center_x) / frame_diff
                                                vy = (curr_center_y - prev_center_y) / frame_diff
                                                track_previous_velocity[track_id_int] = [vx, vy]
                                        
                                        # NOW update bbox and frame for next iteration
                                        track_previous_bbox[track_id_int] = detections.xyxy[det_idx].copy()
                                        track_previous_frame[track_id_int] = current_frame_num

                        # ===== ADVANCED RECOGNITION: Jersey Number Detection =====
                        jersey_numbers = {}  # track_id -> jersey_number
                        if jersey_ocr is not None and detections is not None and len(detections) > 0:
                            try:
                                # Detect jersey numbers for all detections
                                for i, (x1, y1, x2, y2) in enumerate(detections.xyxy):
                                    bbox = [float(x1), float(y1), float(x2), float(y2)]
                                    result = jersey_ocr.detect_jersey_number(frame, bbox)
                                    if result and result['confidence'] >= 0.5:
                                        track_id = detections.tracker_id[i] if detections.tracker_id is not None and i < len(detections.tracker_id) else None
                                        if track_id is not None:
                                            jersey_numbers[int(track_id)] = result['number']
                                # Log periodically
                                if current_frame_num % 500 == 0 and len(jersey_numbers) > 0:
                                    print(f"  ðŸ”¢ Jersey OCR: Detected {len(jersey_numbers)} jersey numbers at frame {current_frame_num}")
                            except Exception as e:
                                if current_frame_num % 1000 == 0:
                                    print(f"  âš  Jersey OCR error: {e}")

                        # ===== ADVANCED RECOGNITION: Gait Analysis Update =====
                        if gait_analyzer is not None and detections is not None and detections.tracker_id is not None:
                            try:
                                # Update gait analyzer with pose keypoints if available
                                # Note: This requires pose keypoints from YOLO pose model
                                # For now, we'll update with position and velocity data
                                for i, track_id in enumerate(detections.tracker_id):
                                    if track_id is not None and i < len(detections.xyxy):
                                        track_id_int = int(track_id)
                                        x1, y1, x2, y2 = detections.xyxy[i]
                                        center_x = (x1 + x2) / 2.0
                                        center_y = (y1 + y2) / 2.0
                                        position = (center_x, center_y)
                                        
                                        # Calculate velocity
                                        velocity = 0.0
                                        if track_id_int in track_previous_velocity:
                                            vel = track_previous_velocity[track_id_int]
                                            if isinstance(vel, (list, np.ndarray)) and len(vel) >= 2:
                                                velocity = np.sqrt(vel[0]**2 + vel[1]**2)
                                        
                                        # Update gait analyzer (keypoints would be added here if pose model is used)
                                        gait_analyzer.update_track(
                                            track_id=track_id_int,
                                            keypoints=None,  # Would be pose keypoints if available
                                            position=position,
                                            velocity=velocity,
                                            frame_num=current_frame_num
                                        )
                            except Exception as e:
                                if current_frame_num % 1000 == 0:
                                    print(f"  âš  Gait Analyzer error: {e}")

                        # Extract Re-ID features before tracking (for matching lost tracks)
                        # OPTIMIZATION: Only extract features every Nth frame to reduce GPU overhead
                        # Re-ID is most useful when tracks are lost, so we
                        # don't need it every frame
                        reid_features = None
                        reid_color_features = None
                        current_frame_num = frame_data.get('frame_num', 0)
                        
                        # SMART RE-ID OPTIMIZATION: Only extract Re-ID features when needed
                        # Strategy: Skip Re-ID for tracks that already have high-confidence assignments
                        tracks_needing_reid = []
                        if reid_tracker is not None and detections is not None and detections.tracker_id is not None:
                            for det_idx, track_id in enumerate(detections.tracker_id):
                                if track_id is not None:
                                    track_id_int = int(track_id)
                                    track_id_str = str(track_id_int)
                                    
                                    # Check if this track needs Re-ID verification
                                    needs_reid = False
                                    
                                    # Case 1: Track has no player assigned - definitely needs Re-ID
                                    if track_id_str not in player_names or not player_names.get(track_id_str):
                                        needs_reid = True
                                    # Case 2: Track has low-confidence assignment - check periodically
                                    elif track_id_int in track_name_confidence:
                                        player_name, similarity, assigned_frame = track_name_confidence[track_id_int]
                                        last_check = track_last_reid_check.get(track_id_int, 0)
                                        
                                        # Check if confidence is low OR hasn't been checked recently
                                        if similarity < REID_CONFIDENCE_THRESHOLD:
                                            needs_reid = True  # Low confidence - check more often
                                        elif (current_frame_num - last_check) >= REID_CHECK_INTERVAL:
                                            needs_reid = True  # High confidence but periodic check needed
                                        # else: High confidence and recently checked - skip Re-ID
                                    else:
                                        # Track has name but no confidence recorded - check it
                                        needs_reid = True
                                    
                                    if needs_reid:
                                        tracks_needing_reid.append(det_idx)
                                        track_last_reid_check[track_id_int] = current_frame_num
                        
                        # OPTIMIZATION: In watch-only mode, skip color features (expensive, only needed for team filtering)
                        # and process fewer frames for faster learning
                        # PERFORMANCE: Gallery matching doesn't need Re-ID features EVERY frame
                        # Only extract for gallery matching periodically (every 5 frames) or when tracks need checking
                        # This prevents extracting Re-ID for all detections every frame when gallery is active
                        current_frame_num = frame_data.get('frame_num', 0)
                        gallery_needs_reid = False
                        if player_gallery is not None and len(detections) > 0:
                            # OPTIMIZATION: Adaptive gallery check interval based on processing speed
                            # Faster processing (higher FPS) can check more frequently
                            # Slower processing should check less frequently to maintain speed
                            # Default: 5 frames (~0.17s at 30fps, ~0.08s at 60fps)
                            # For high FPS videos (60+), can check more frequently (every 3 frames)
                            # For lower FPS or slow processing, check less frequently (every 10 frames)
                            base_gallery_interval = 5
                            # Adjust based on input FPS (if available)
                            if 'fps' in locals() and fps > 50:
                                gallery_check_interval = max(3, base_gallery_interval - 2)  # More frequent for high FPS
                            elif 'fps' in locals() and fps < 25:
                                gallery_check_interval = base_gallery_interval + 5  # Less frequent for low FPS
                            else:
                                gallery_check_interval = base_gallery_interval
                            gallery_needs_reid = (current_frame_num % gallery_check_interval == 0) or len(tracks_needing_reid) > 0
                        
                        if watch_only:
                            # In watch-only mode: process every 2nd frame and skip color features
                            extract_reid_this_frame = (current_frame_num % 2 == 0) and (len(tracks_needing_reid) > 0 or gallery_needs_reid)
                            extract_color_features = False  # Skip color features in watch-only mode
                        else:
                            # Normal mode: Extract Re-ID if tracks need checking OR if gallery matching needs it (periodic)
                            extract_reid_this_frame = len(tracks_needing_reid) > 0 or gallery_needs_reid
                            extract_color_features = extract_reid_this_frame  # Only extract colors if extracting Re-ID
                        
                        # Performance logging
                        if current_frame_num % 500 == 0 and len(detections) > 0 and detections.tracker_id is not None:
                            total_tracks = len([t for t in detections.tracker_id if t is not None])
                            tracks_with_high_conf = sum(1 for tid in (t for t in detections.tracker_id if t is not None) 
                                                       if tid and int(tid) in track_name_confidence 
                                                       and track_name_confidence[int(tid)][1] >= REID_CONFIDENCE_THRESHOLD)
                            if extract_reid_this_frame:
                                reason = "gallery matching" if gallery_needs_reid and len(tracks_needing_reid) == 0 else f"{len(tracks_needing_reid)}/{total_tracks} tracks need checking"
                                print(f"  âš¡ Re-ID: Extracting features for {reason}")
                            else:
                                print(f"  âš¡ Re-ID: Skipped (all {total_tracks} tracks have high-confidence assignments and no gallery matching)")
                        
                        if reid_tracker is not None and len(
                                detections) > 0 and extract_reid_this_frame:
                            try:
                                # CRITICAL FIX: Re-ID extraction needs coordinates in YOLO-frame space
                                # detections.xyxy was scaled to 4K, but batch_frame is still 1080p!
                                # Create a copy with YOLO-resolution coordinates
                                scale_factor = frame_data.get('scale_factor', 1.0)
                                if scale_factor != 1.0:
                                    # Scale coordinates back down to YOLO resolution for Re-ID
                                    detections_for_reid = sv.Detections(
                                        xyxy=detections.xyxy / scale_factor,  # Scale back to 1080p
                                        confidence=detections.confidence,
                                        class_id=detections.class_id,
                                        tracker_id=detections.tracker_id
                                    )
                                else:
                                    detections_for_reid = detections
                                
                                # CRITICAL FIX: Use original (sharp) frame for Re-ID extraction, not net-removed frame
                                # Net removal blurs the image, which hurts Re-ID feature quality
                                # Use original_frame_for_learning if available, otherwise fall back to full_frame
                                frame_for_reid = batch_frame
                                original_frame = frame_data.get('original_frame_for_learning', None)
                                if original_frame is not None:
                                    # Use original sharp frame for Re-ID (better feature quality)
                                    frame_for_reid = original_frame
                                elif roi_bounds is not None:
                                    # Get full frame from frame_data (stored when frame was queued)
                                    full_frame = frame_data.get('full_frame', None)
                                    if full_frame is not None:
                                        frame_for_reid = full_frame
                                    else:
                                        # Fallback: if full frame not stored, use batch_frame
                                        # This shouldn't happen, but handle gracefully
                                        frame_for_reid = batch_frame
                                
                                # Extract Re-ID features (extract_reid_this_frame already includes seed frame logic)
                                if reid_tracker is not None:
                                    reid_features = reid_tracker.extract_features(
                                        frame_for_reid, detections_for_reid, team_colors, ball_colors)
                                    # Extract color features (team and ball colors) - SKIP in watch-only mode
                                    if extract_color_features:
                                        reid_color_features = reid_tracker.extract_color_features(
                                            frame_for_reid, detections_for_reid, team_colors, ball_colors)
                                    else:
                                        reid_color_features = None  # Skip expensive color extraction
                                else:
                                    reid_features = None
                                    reid_color_features = None
                                
                                # Extract foot/shoe features for player identification (bottom 20-40% of bbox)
                                # Foot features help identify players by their shoes, which are often distinctive
                                foot_features = None
                                if reid_tracker is not None and len(detections_for_reid) > 0:
                                    try:
                                        foot_features = reid_tracker.extract_foot_features(
                                            frame_for_reid, detections_for_reid)
                                        if frame_data.get('frame_num', 0) % 500 == 0 and foot_features is not None:
                                            print(f"ðŸ‘Ÿ Frame {frame_data.get('frame_num', 'unknown')}: Extracted foot features for {len(detections)} detections")
                                    except Exception as e:
                                        if frame_data.get('frame_num', 0) % 500 == 0:
                                            print(f"âš  Foot feature extraction failed: {e}")
                                        foot_features = None
                                
                                # DEBUG: Confirm feature extraction and check for NaN
                                if frame_data.get('frame_num', 0) % 100 == 0:
                                    if reid_features is not None:
                                        # Check how many features have NaN
                                        nan_count = sum(1 for feat in reid_features if np.isnan(feat).any())
                                        valid_count = len(reid_features) - nan_count
                                        # PERFORMANCE: Reduced Re-ID logging frequency (was every frame with detections)
                                        if frame_data.get('frame_num', 0) % 500 == 0:  # Log every 500 frames
                                            print(f"ðŸ” Frame {frame_data.get('frame_num', 'unknown')}: Extracted Re-ID features for {len(detections)} detections (shape: {reid_features.shape})")
                                            print(f"   â†’ Feature validity: {valid_count} valid, {nan_count} with NaN")
                                    else:
                                        print(f"ðŸ” Frame {frame_data.get('frame_num', 'unknown')}: Re-ID feature extraction returned None")
                            except Exception as e:
                                print(
                                    f"âš  Re-ID feature extraction failed at frame {frame_data.get('frame_num', 'unknown')}: {e}")
                                import traceback
                                traceback.print_exc()
                                reid_features = None
                                reid_color_features = None

                        # ðŸ›¡ï¸ CRITICAL PRE-FRAME PROTECTION: Force protected names BEFORE any processing
                        # This ensures anchor-protected players keep their identity even if track IDs change
                        current_frame_num = frame_data.get('frame_num', 0)
                        
                        # OPTIMIZATION: Force gallery mapping every N frames if seed_frame_interval is set
                        # This ensures players are re-identified regularly even without anchor frames
                        # Calculate at the start of frame processing so it's available throughout
                        force_gallery_mapping = False
                        if seed_frame_interval is not None:
                            if seed_frame_interval == 0:
                                # Every 100 frames (default when --seed-frame 0)
                                force_gallery_mapping = (current_frame_num % 100 == 0 and current_frame_num > 0)
                            else:
                                # Every N frames
                                force_gallery_mapping = (current_frame_num % seed_frame_interval == 0 and current_frame_num > 0)
                        
                        # Update player uniqueness grace periods (expire old ones)
                        update_player_uniqueness_grace(current_frame_num)
                        
                        # OPTIMIZATION: Skip protection checks if current frame is not in any protection window
                        # This significantly speeds up processing for frames far from anchor frames
                        should_check_protection = (len(frames_needing_protection_check) == 0 or 
                                                  current_frame_num in frames_needing_protection_check)
                        
                        # Check ALL tracks in current frame and force protected names
                        if should_check_protection and detections is not None and detections.tracker_id is not None:
                            for det_idx, track_id in enumerate(detections.tracker_id):
                                if track_id is not None:
                                    track_id_int = int(track_id)
                                    track_id_str = str(track_id_int)
                                    
                                    # ðŸ›¡ï¸ CRITICAL: Check if track is permanently anchor-assigned FIRST (highest priority)
                                    # This ensures anchor-assigned tracks maintain identity for entire video duration
                                    is_permanently_protected = track_id_int in track_anchor_assigned
                                    
                                    # ðŸŽ¯ AUTO-PROTECTION: Check if track has long successful Re-ID history
                                    # If Re-ID has been working correctly for many frames, extend protection automatically
                                    is_auto_protected = False
                                    auto_protected_name = None
                                    if track_id_int in track_high_confidence_history:
                                        hist_name, first_frame, last_frame, consecutive_frames = track_high_confidence_history[track_id_int]
                                        # Check if track has been consistently identified with high confidence
                                        if consecutive_frames >= MIN_CONSECUTIVE_FRAMES:
                                            # Track has been successfully identified for long time - extend protection
                                            protection_end_frame = last_frame + AUTO_PROTECTION_EXTENSION
                                            if current_frame_num <= protection_end_frame:
                                                is_auto_protected = True
                                                auto_protected_name = hist_name
                                                # Update last frame if still high confidence
                                                if track_id_int in track_name_confidence:
                                                    conf_name, conf_score, conf_frame = track_name_confidence[track_id_int]
                                                    if conf_score >= HIGH_CONFIDENCE_THRESHOLD and conf_name == hist_name:
                                                        # Still high confidence - extend history
                                                        track_high_confidence_history[track_id_int] = (hist_name, first_frame, current_frame_num, consecutive_frames + 1)
                                                        if current_frame_num % 200 == 0:
                                                            print(f"  ðŸŽ¯ AUTO-PROTECTION: Track #{track_id_int} '{hist_name}' - {consecutive_frames} frames of high confidence (extended to frame {current_frame_num + AUTO_PROTECTION_EXTENSION})")
                                    if is_permanently_protected:
                                        protected_name, anchor_frame = track_anchor_assigned[track_id_int]
                                        # PERMANENT PROTECTION: Extend protection window dynamically to current frame
                                        if track_id_int in track_anchor_protection:
                                            _, protection_start, _ = track_anchor_protection[track_id_int]
                                            # Extend protection to current frame + buffer
                                            track_anchor_protection[track_id_int] = (protected_name, protection_start, current_frame_num + ANCHOR_DECAY_FRAMES)
                                        else:
                                            # Create protection window if it doesn't exist
                                            track_anchor_protection[track_id_int] = (protected_name, max(0, anchor_frame - ANCHOR_DECAY_FRAMES), current_frame_num + ANCHOR_DECAY_FRAMES)
                                        
                                        protection_start, protection_end = track_anchor_protection[track_id_int][1], track_anchor_protection[track_id_int][2]
                                        track_age = current_frame_num - anchor_frame
                                        
                                        # PERMANENT PROTECTION: Always enforce (no distance check needed)
                                        current_name = player_names.get(track_id_str, "")
                                        if current_name != protected_name:
                                            # CRITICAL: Check if protected_name is already assigned to a different track
                                            if protected_name in player_to_track_global:
                                                old_track_id = player_to_track_global[protected_name]
                                                if old_track_id != track_id_int:
                                                    # Check grace period
                                                    can_assign, should_clear = check_player_uniqueness_grace(protected_name, track_id_int, current_frame_num)
                                                    if should_clear:
                                                        old_track_str = str(int(old_track_id))
                                                        if old_track_str in player_names:
                                                            player_names[old_track_str] = ""
                                                        if old_track_id in track_name_confidence:
                                                            del track_name_confidence[old_track_id]
                                            
                                            player_names[track_id_str] = protected_name
                                            track_name_confidence[track_id_int] = (protected_name, 1.00, current_frame_num)
                                            player_to_track_global[protected_name] = track_id_int
                                            if current_name:
                                                if current_name in player_to_track_global and player_to_track_global[current_name] == track_id_int:
                                                    del player_to_track_global[current_name]
                                            if current_frame_num % 100 == 0:
                                                print(f"  ðŸ›¡ï¸ PERMANENT ANCHOR PROTECTION: Frame {current_frame_num}, Track #{track_id_int} FORCED to '{protected_name}' (anchor at frame {anchor_frame}, age: {track_age} frames, was: '{current_name}')")
                                    
                                    # Check if this track has temporary anchor protection (not permanently assigned)
                                    elif track_id_int in track_anchor_protection:
                                        protected_name, protection_start, protection_end = track_anchor_protection[track_id_int]
                                        
                                        if protection_start <= current_frame_num <= protection_end:
                                            # Calculate distance from protection center
                                            protection_center = (protection_start + protection_end) // 2
                                            distance_from_center = abs(current_frame_num - protection_center)
                                            
                                            # HARD PROTECTION: Force name immediately, no exceptions
                                            if distance_from_center <= ANCHOR_HARD_PROTECTION_FRAMES:
                                                current_name = player_names.get(track_id_str, "")
                                                if current_name != protected_name:
                                                    # CRITICAL: Check if protected_name is already assigned to a different track
                                                    # Use grace period to allow smooth transitions
                                                    if protected_name in player_to_track_global:
                                                        old_track_id = player_to_track_global[protected_name]
                                                        if old_track_id != track_id_int:
                                                            # Check grace period
                                                            can_assign, should_clear = check_player_uniqueness_grace(protected_name, track_id_int, current_frame_num)
                                                            if should_clear:
                                                                # Grace period expired - clear old assignment
                                                                old_track_str = str(int(old_track_id))
                                                                if old_track_str in player_names:
                                                                    player_names[old_track_str] = ""  # Clear old track
                                                                if old_track_id in track_name_confidence:
                                                                    del track_name_confidence[old_track_id]
                                                                if current_frame_num % 10 == 0:
                                                                    print(f"  ðŸ—‘ï¸ UNIQUENESS: Cleared '{protected_name}' from Track #{old_track_id} (moving to Track #{track_id_int}, grace expired)")
                                                            elif current_frame_num % 10 == 0:
                                                                print(f"  â³ GRACE PERIOD: '{protected_name}' on Track #{old_track_id} and Track #{track_id_int} (grace period active)")
                                                    
                                                    player_names[track_id_str] = protected_name
                                                    # Update confidence to 1.00 (anchor level)
                                                    track_name_confidence[track_id_int] = (protected_name, 1.00, current_frame_num)
                                                    # CRITICAL FIX: Ensure player_names is synced (defensive - already set above, but ensures consistency)
                                                    player_names[track_id_str] = protected_name
                                                    # Update global mapping - CRITICAL: Remove any other player that was on this track
                                                    # This prevents Anay from being in player_to_track_global for this track
                                                    if current_name and current_name in player_to_track_global:
                                                        if player_to_track_global[current_name] == track_id_int:
                                                            del player_to_track_global[current_name]
                                                    player_to_track_global[protected_name] = track_id_int
                                                    if current_frame_num % 100 == 0:
                                                        print(f"  ðŸ›¡ï¸ PERMANENT ANCHOR PROTECTION: Frame {current_frame_num}, Track #{track_id_int} FORCED to '{protected_name}' (anchor at frame {anchor_frame}, track age: {track_age} frames, was: '{current_name}')")
                                    
                                    # Check if this track has temporary anchor protection (not permanently assigned)
                                    elif track_id_int in track_anchor_protection:
                                        protected_name, protection_start, protection_end = track_anchor_protection[track_id_int]
                                        
                                        # BUG FIX: Extend protection window if track continues beyond original window
                                        if current_frame_num > protection_end:
                                            # Track has continued beyond original protection window - extend it
                                            new_protection_end = current_frame_num + ANCHOR_DECAY_FRAMES
                                            track_anchor_protection[track_id_int] = (protected_name, protection_start, new_protection_end)
                                            protection_end = new_protection_end
                                            if current_frame_num % 500 == 0:
                                                print(f"  ðŸ”‹ PROTECTION EXTENDED: Track #{track_id_int} '{protected_name}' protection extended to frame {protection_end}")
                                        
                                        if protection_start <= current_frame_num <= protection_end:
                                            # Calculate distance from protection center
                                            protection_center = (protection_start + protection_end) // 2
                                            distance_from_center = abs(current_frame_num - protection_center)
                                            
                                            # HARD PROTECTION: Force name immediately, no exceptions
                                            if distance_from_center <= ANCHOR_HARD_PROTECTION_FRAMES:
                                                current_name = player_names.get(track_id_str, "")
                                                if current_name != protected_name:
                                                    # CRITICAL: Check if protected_name is already assigned to a different track
                                                    # Use grace period to allow smooth transitions
                                                    if protected_name in player_to_track_global:
                                                        old_track_id = player_to_track_global[protected_name]
                                                        if old_track_id != track_id_int:
                                                            # Check grace period
                                                            can_assign, should_clear = check_player_uniqueness_grace(protected_name, track_id_int, current_frame_num)
                                                            if should_clear:
                                                                # Grace period expired - clear old assignment
                                                                old_track_str = str(int(old_track_id))
                                                                if old_track_str in player_names:
                                                                    player_names[old_track_str] = ""  # Clear old track
                                                                if old_track_id in track_name_confidence:
                                                                    del track_name_confidence[old_track_id]
                                                                if current_frame_num % 10 == 0:
                                                                    print(f"  ðŸ—‘ï¸ UNIQUENESS: Cleared '{protected_name}' from Track #{old_track_id} (moving to Track #{track_id_int}, grace expired)")
                                                            elif current_frame_num % 10 == 0:
                                                                print(f"  â³ GRACE PERIOD: '{protected_name}' on Track #{old_track_id} and Track #{track_id_int} (grace period active)")
                                                    
                                                    player_names[track_id_str] = protected_name
                                                    # Update confidence to 1.00 (anchor level)
                                                    track_name_confidence[track_id_int] = (protected_name, 1.00, current_frame_num)
                                                    # CRITICAL FIX: Ensure player_names is synced (defensive - already set above, but ensures consistency)
                                                    player_names[track_id_str] = protected_name
                                                    # Update global mapping - CRITICAL: Remove any other player that was on this track
                                                    # This prevents Anay from being in player_to_track_global for this track
                                                    if current_name and current_name in player_to_track_global:
                                                        if player_to_track_global[current_name] == track_id_int:
                                                            del player_to_track_global[current_name]
                                                    player_to_track_global[protected_name] = track_id_int
                                                    if current_frame_num % 10 == 0:
                                                        print(f"  ðŸ›¡ï¸ PRE-FRAME HARD PROTECTION: Frame {current_frame_num}, Track #{track_id_int} FORCED to '{protected_name}' (was: '{current_name}')")
                                                
                                                # Update protection bbox to current position (for spatial recovery)
                                                if det_idx < len(detections.xyxy) and protected_name in player_anchor_protection:
                                                    current_bbox = detections.xyxy[det_idx].tolist()
                                                    # Update the protection window with current bbox
                                                    for i, (ps, pe, pb, pt) in enumerate(player_anchor_protection[protected_name]):
                                                        if ps == protection_start and pe == protection_end:
                                                            player_anchor_protection[protected_name][i] = (ps, pe, current_bbox, track_id_int)
                                                            break
                                            
                                            # SOFT/DECAY PROTECTION: Still force name, but allow Re-ID with high evidence
                                            elif distance_from_center <= ANCHOR_DECAY_FRAMES:
                                                current_name = player_names.get(track_id_str, "")
                                                if current_name != protected_name:
                                                    # CRITICAL: Check if protected_name is already assigned to a different track
                                                    if protected_name in player_to_track_global:
                                                        old_track_id = player_to_track_global[protected_name]
                                                        if old_track_id != track_id_int:
                                                            # Player is on a different track - clear it first
                                                            old_track_str = str(int(old_track_id))
                                                            if old_track_str in player_names:
                                                                player_names[old_track_str] = ""  # Clear old track
                                                            if old_track_id in track_name_confidence:
                                                                del track_name_confidence[old_track_id]
                                                            if current_frame_num % 50 == 0:
                                                                print(f"  ðŸ—‘ï¸ UNIQUENESS (soft): Cleared '{protected_name}' from Track #{old_track_id} (moving to Track #{track_id_int})")
                                                    # Force name but allow Re-ID override with high evidence
                                                    player_names[track_id_str] = protected_name
                                                    player_to_track_global[protected_name] = track_id_int
                                                    if current_frame_num % 50 == 0:
                                                        print(f"  ðŸ›¡ï¸ PRE-FRAME SOFT PROTECTION: Frame {current_frame_num}, Track #{track_id_int} set to '{protected_name}' (was: '{current_name}')")

                        # ANCHOR FRAMES: Apply frame-specific player tags with 1.00 confidence (HIGHEST PRIORITY)
                        # âš ï¸ CRITICAL: Anchor frames MUST be processed AFTER tracker runs (track IDs are required for matching)
                        # This block is processed AFTER tracker.update() - see line 11203 for the actual processing
                        # (This early check is just for logging - actual processing happens after tracker)
                        if False and anchor_frames:  # DISABLED: Processed after tracker instead
                            # Log anchor frame summary on first frame only
                            if current_frame_num == 0:
                                total_anchor_frames = len(anchor_frames)
                                total_anchor_tags = sum(len(anchors) if isinstance(anchors, list) else 0 for anchors in anchor_frames.values())
                                anchor_players = set()
                                for anchors in anchor_frames.values():
                                    if isinstance(anchors, list):
                                        for anchor in anchors:
                                            player_name = anchor.get('player_name')
                                            if player_name:
                                                anchor_players.add(player_name)
                                print(f"  ðŸŽ¯ ANCHOR FRAMES LOADED: {total_anchor_frames} frame(s) with {total_anchor_tags} tag(s) for players: {', '.join(sorted(anchor_players))}")
                            
                            # Initialize anchor_tags to empty list (will be populated if current frame has anchors)
                            anchor_tags = []
                            anchors_applied = 0
                            
                            if current_frame_num in anchor_frames:
                                anchor_tags = anchor_frames[current_frame_num]
                                if anchor_tags is None or not isinstance(anchor_tags, list):
                                    anchor_tags = []
                                if len(anchor_tags) > 0:
                                    print(f"  ðŸŽ¯ ANCHOR FRAME: Frame {current_frame_num} has {len(anchor_tags)} anchor tag(s) - applying protection")
                                    # DIAGNOSTIC: Log anchor frame details
                                    for anchor in anchor_tags:
                                        anchor_name = anchor.get('player_name', 'Unknown')
                                        # Handle malformed anchor names (lists, etc.) - clean for display AND fix the anchor dict
                                        if isinstance(anchor_name, list):
                                            anchor_name = anchor_name[0].strip() if len(anchor_name) > 0 and isinstance(anchor_name[0], str) else 'Unknown'
                                            # Also fix the anchor dict
                                            anchor['player_name'] = anchor_name
                                        elif not isinstance(anchor_name, str):
                                            anchor_name = str(anchor_name).strip() if anchor_name else 'Unknown'
                                            # Also fix the anchor dict
                                            anchor['player_name'] = anchor_name
                                        else:
                                            anchor_name = anchor_name.strip() if anchor_name else 'Unknown'
                                            # Ensure anchor dict has cleaned name
                                            anchor['player_name'] = anchor_name
                                        anchor_tid = anchor.get('track_id')
                                        anchor_bbox = anchor.get('bbox')
                                        print(f"     â†’ '{anchor_name}': track_id={anchor_tid}, bbox={'present' if anchor_bbox else 'missing'}")
                            
                            # ðŸ›¡ï¸ PRE-CALCULATE FULL PROTECTION WINDOW: If player has multiple anchor frames, use full range
                            # This ensures protection covers ALL frames from first anchor to last anchor + protection
                            # OPTIMIZATION: Use pre-computed anchor ranges (calculated at startup) instead of iterating all frames
                            # Only process if we have anchor tags for this frame
                            if len(anchor_tags) > 0:
                                for anchor in anchor_tags:
                                    anchor_player_name = anchor.get('player_name')
                                    anchor_track_id = anchor.get('track_id')
                                    
                                    # CRITICAL: Clean anchor_player_name immediately to prevent malformed names
                                    if isinstance(anchor_player_name, list):
                                        if len(anchor_player_name) > 0 and isinstance(anchor_player_name[0], str):
                                            anchor_player_name = anchor_player_name[0].strip()
                                            anchor['player_name'] = anchor_player_name  # Fix the anchor
                                        else:
                                            print(f"  âš  WARNING: Skipping anchor with invalid player_name list: {anchor_player_name}")
                                            continue
                                    elif not isinstance(anchor_player_name, str):
                                        anchor_player_name = str(anchor_player_name).strip() if anchor_player_name else None
                                        if anchor_player_name:
                                            anchor['player_name'] = anchor_player_name  # Fix the anchor
                                        else:
                                            print(f"  âš  WARNING: Skipping anchor with invalid player_name type: {type(anchor_player_name)}")
                                            continue
                                    else:
                                        anchor_player_name = anchor_player_name.strip() if anchor_player_name else ''
                                    
                                    if not anchor_player_name:
                                        print(f"  âš  WARNING: Skipping anchor with empty player_name")
                                        continue
                                    
                                    # Handle anchors with track_id (pre-assigned)
                                    if anchor_player_name and anchor_track_id is not None:
                                        # OPTIMIZED: Use pre-computed anchor ranges from startup (much faster than iterating all frames)
                                        anchor_key = (anchor_player_name, anchor_track_id)
                                        if anchor_key in anchor_frame_ranges:
                                            first_anchor_frame, last_anchor_frame = anchor_frame_ranges[anchor_key]
                                        else:
                                            # Fallback: Use current frame if not pre-computed (shouldn't happen, but safe)
                                            first_anchor_frame = current_frame_num
                                            last_anchor_frame = current_frame_num
                                        
                                        # Calculate full protection window: from first anchor - protection to last anchor + protection
                                        full_protection_start = max(0, first_anchor_frame - ANCHOR_DECAY_FRAMES)
                                        full_protection_end = last_anchor_frame + ANCHOR_DECAY_FRAMES
                                        
                                        # Store full protection window for this player/track (will be used when applying anchor)
                                        if anchor_player_name not in player_anchor_protection:
                                            player_anchor_protection[anchor_player_name] = []
                                        
                                        # Check if we already have a protection window for this track
                                        track_protection_exists = False
                                        for i, (ps, pe, pb, pt) in enumerate(player_anchor_protection[anchor_player_name]):
                                            if pt == anchor_track_id:
                                                # Update existing protection window to full range
                                                player_anchor_protection[anchor_player_name][i] = (
                                                    full_protection_start, full_protection_end, pb, anchor_track_id
                                                )
                                                track_protection_exists = True
                                                break
                                        
                                        if not track_protection_exists:
                                            # Add new protection window
                                            player_anchor_protection[anchor_player_name].append((
                                                full_protection_start, full_protection_end, None, anchor_track_id
                                            ))
                                        
                                        # Also update track_anchor_protection with full window
                                        if anchor_track_id in track_anchor_protection:
                                            old_name, old_start, old_end = track_anchor_protection[anchor_track_id]
                                            # Extend to full range
                                            track_anchor_protection[anchor_track_id] = (
                                                anchor_player_name,
                                                min(old_start, full_protection_start),
                                                max(old_end, full_protection_end)
                                            )
                                        else:
                                            track_anchor_protection[anchor_track_id] = (
                                                anchor_player_name, full_protection_start, full_protection_end
                                            )
                                        
                                        if current_frame_num % 50 == 0:
                                            print(f"  ðŸ›¡ï¸ FULL PROTECTION: '{anchor_player_name}' Track #{anchor_track_id} - frames {first_anchor_frame}-{last_anchor_frame} â†’ protection: {full_protection_start}-{full_protection_end} ({full_protection_end - full_protection_start} frames)")
                            
                            # OPTIMIZATION: Pre-compute detection data once for all anchors (only if we have anchor tags)
                            if len(anchor_tags) > 0:
                                if detections is not None and len(detections) > 0:
                                    # Pre-compute all detection centers and areas once (used by all anchors)
                                    det_xyxy = detections.xyxy
                                    det_centers_x = (det_xyxy[:, 0] + det_xyxy[:, 2]) / 2
                                    det_centers_y = (det_xyxy[:, 1] + det_xyxy[:, 3]) / 2
                                    det_areas = (det_xyxy[:, 2] - det_xyxy[:, 0]) * (det_xyxy[:, 3] - det_xyxy[:, 1])
                                else:
                                    # No detections available - can't match anchor frames
                                    det_xyxy = None
                                det_centers_x = None
                                det_centers_y = None
                                det_areas = None
                                
                                # Apply anchor frames to detections
                                for anchor in anchor_tags:
                                    anchor_track_id = anchor.get('track_id')
                                    anchor_player_name = anchor.get('player_name')
                                    anchor_team = anchor.get('team', '')
                                    anchor_bbox = anchor.get('bbox')
                                    
                                    # CRITICAL: Clean and validate anchor_player_name to prevent name mixing
                                    if isinstance(anchor_player_name, list):
                                        # Extract first valid string from list
                                        if len(anchor_player_name) > 0 and isinstance(anchor_player_name[0], str):
                                            anchor_player_name = anchor_player_name[0].strip()
                                            anchor['player_name'] = anchor_player_name  # Fix the anchor
                                            if current_frame_num < 10:
                                                print(f"  âš  FIXED: Anchor player_name was a list, extracted '{anchor_player_name}'")
                                        else:
                                            print(f"  âš  WARNING: Skipping anchor with invalid player_name list: {anchor_player_name}")
                                            continue
                                    elif not isinstance(anchor_player_name, str):
                                        anchor_player_name = str(anchor_player_name).strip() if anchor_player_name else None
                                        if anchor_player_name:
                                            anchor['player_name'] = anchor_player_name  # Fix the anchor
                                        else:
                                            print(f"  âš  WARNING: Skipping anchor with invalid player_name type: {type(anchor_player_name)}")
                                            continue
                                    else:
                                        anchor_player_name = anchor_player_name.strip() if anchor_player_name else ''
                                    
                                    if not anchor_player_name:
                                        print(f"  âš  WARNING: Skipping anchor with empty player_name")
                                        continue
                                    
                                    # Ensure anchor dict has the cleaned name
                                    anchor['player_name'] = anchor_player_name
                                    
                                    matched_track_id = None
                                    match_method = None
                                    
                                    # ðŸŽ¯ PRIORITY 1: Try track_id matching FIRST if available (most reliable)
                                    # If anchor specifies a track_id, trust it - it's from Setup Wizard where user explicitly tagged that track
                                    # OPTIMIZATION: Use numpy vectorized search instead of Python loop
                                    skip_bbox_matching = False
                                    if anchor_track_id is not None and detections.tracker_id is not None:
                                        anchor_track_id_int = int(anchor_track_id)
                                        # Vectorized comparison (much faster than Python loop)
                                        # Convert None values to -1 for numpy comparison
                                        track_ids_list = [int(t) if t is not None else -1 for t in detections.tracker_id]
                                        track_ids_array = np.array(track_ids_list)
                                        matching_indices = np.where(track_ids_array == anchor_track_id_int)[0]
                                        if len(matching_indices) > 0:
                                            matched_track_id = anchor_track_id_int
                                            match_method = "track ID (exact match)"
                                            # CRITICAL FIX: If anchor_bbox is None but we matched by track_id, 
                                            # use the detection's bbox as the anchor bbox (for future matching and recovery)
                                            if anchor_bbox is None and len(matching_indices) > 0:
                                                matched_det_idx = matching_indices[0]
                                                if matched_det_idx < len(detections.xyxy):
                                                    anchor_bbox = detections.xyxy[matched_det_idx].tolist()
                                                    # Update anchor dict with bbox for future use
                                                    anchor['bbox'] = anchor_bbox
                                                    if current_frame_num < 30:
                                                        print(f"  ðŸ”§ ANCHOR FIX: Frame {current_frame_num}, {anchor_player_name} - recovered bbox from detection (track_id={matched_track_id})")
                                            # Skip expensive bbox matching if track_id match succeeded
                                            skip_bbox_matching = True
                                    
                                    # ðŸŽ¯ PRIORITY 2: If track_id matching failed, try bbox matching (fallback)
                                    # Track IDs can change between Setup Wizard and Analyzer, but bbox positions are more stable
                                    # OPTIMIZATION: Skip expensive bbox matching if track_id match already succeeded
                                    if not skip_bbox_matching and matched_track_id is None and anchor_bbox is not None and len(anchor_bbox) >= 4:
                                        # CRITICAL: Check if we have detections to match against
                                        if det_xyxy is None or len(det_xyxy) == 0:
                                            # No detections available - can't match by bbox
                                            if current_frame_num < 30 or current_frame_num % 100 == 0:
                                                print(f"  âš  ANCHOR FRAME: Frame {current_frame_num}, '{anchor_player_name}' - no detections available for bbox matching (anchor will be checked again next frame)")
                                            continue
                                        else:
                                            # DIAGNOSTIC: Log that we're attempting bbox matching
                                            if current_frame_num < 10:
                                                print(f"  ðŸ” ANCHOR BBOX MATCH: Frame {current_frame_num}, '{anchor_player_name}' - attempting bbox match (anchor bbox: {anchor_bbox}, detections: {len(det_xyxy)})")
                                        # Match by bbox position (from Gallery Seeder - no track ID available yet)
                                        # Find detection with closest bbox overlap
                                        anchor_x1, anchor_y1, anchor_x2, anchor_y2 = anchor_bbox[:4]
                                        anchor_center_x = (anchor_x1 + anchor_x2) / 2
                                        anchor_center_y = (anchor_y1 + anchor_y2) / 2
                                        anchor_area = (anchor_x2 - anchor_x1) * (anchor_y2 - anchor_y1)
                                        
                                        best_match_idx = None
                                        best_iou = 0.0
                                        
                                        # OPTIMIZATION: Vectorized matching for better performance
                                        # Use pre-computed detection centers and areas (computed once above)
                                        # Vectorized center distance calculation
                                        center_distances = np.sqrt((det_centers_x - anchor_center_x)**2 + (det_centers_y - anchor_center_y)**2)
                                        
                                        # Quick filter: only consider detections within reasonable distance
                                        # More lenient for anchor frames - players can move between frames (especially at high FPS)
                                        # CRITICAL: Anchor frames are ground truth - use very lenient distance (1000px) to handle movement
                                        candidate_mask = center_distances < 1000
                                        if not np.any(candidate_mask):
                                            # No candidates nearby, skip to fallback
                                            pass
                                        else:
                                            # Vectorized IoU calculation for candidates only
                                            candidate_indices = np.where(candidate_mask)[0]
                                            
                                            for det_idx in candidate_indices:
                                                det_x1, det_y1, det_x2, det_y2 = det_xyxy[det_idx]
                                                
                                                # Calculate IoU (Intersection over Union)
                                                inter_x1 = max(anchor_x1, det_x1)
                                                inter_y1 = max(anchor_y1, det_y1)
                                                inter_x2 = min(anchor_x2, det_x2)
                                                inter_y2 = min(anchor_y2, det_y2)
                                                
                                                if inter_x2 > inter_x1 and inter_y2 > inter_y1:
                                                    inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)
                                                    union_area = anchor_area + det_areas[det_idx] - inter_area
                                                    iou = inter_area / union_area if union_area > 0 else 0.0
                                                    
                                                    # IMPROVED: More lenient matching for anchor frames (bbox is ground truth)
                                                    # Lower IoU threshold (0.1) and larger distance (1000px) to handle position changes
                                                    center_distance = center_distances[det_idx]
                                                    if iou > best_iou:
                                                        # CRITICAL FIX: Very lenient matching for anchor frames - if distance is < 50px, accept even with low IoU
                                                        # This handles cases where bbox sizes differ but positions are very close
                                                        # Increased distance thresholds to 1000px for anchor frames (ground truth)
                                                        if (iou > 0.1 and center_distance < 1000) or (center_distance < 200 and iou > 0.05) or (center_distance < 100):
                                                            best_iou = iou
                                                            best_match_idx = det_idx
                                        
                                        # FALLBACK: If no IoU match, try nearest neighbor by center distance only
                                        # OPTIMIZATION: Use pre-computed distances and areas (already computed above)
                                        if best_match_idx is None:
                                            # Filter by distance and area ratio using vectorized operations
                                            area_ratios = np.minimum(anchor_area, det_areas) / np.maximum(anchor_area, det_areas)
                                            area_ratios = np.nan_to_num(area_ratios, nan=0.0)
                                            
                                            # CRITICAL FIX: Very lenient matching for anchor frames
                                            # If distance is very close (< 50px), accept regardless of area ratio (bbox sizes can vary)
                                            # Otherwise, require area ratio > 0.2 for distances < 400px
                                            very_close_mask = center_distances < 50
                                            if np.any(very_close_mask):
                                                # Accept nearest detection if very close (within 50px)
                                                valid_distances = center_distances.copy()
                                                valid_distances[~very_close_mask] = np.inf
                                                best_match_idx = np.argmin(valid_distances)
                                            else:
                                                # For farther detections, require reasonable area ratio
                                                # CRITICAL: Use 1000px for anchor frames (ground truth) - players can move significantly
                                                valid_mask = (center_distances < 1000) & (area_ratios > 0.2)
                                                if np.any(valid_mask):
                                                    # Find closest valid detection
                                                    valid_distances = center_distances.copy()
                                                    valid_distances[~valid_mask] = np.inf
                                                    best_match_idx = np.argmin(valid_distances)
                                        
                                        # FINAL FALLBACK: If still no match, try ANY detection within 1000px (very lenient for anchor frames)
                                        # CRITICAL: For anchor frames, we MUST match if there's any detection nearby
                                        # Anchor frames are ground truth - if a detection is within 1000px, it's likely the same player
                                        # Increased from 500px to 1000px to handle cases where player has moved significantly
                                        if best_match_idx is None:
                                            very_lenient_mask = center_distances < 1000
                                            if np.any(very_lenient_mask):
                                                valid_distances = center_distances.copy()
                                                valid_distances[~very_lenient_mask] = np.inf
                                                best_match_idx = np.argmin(valid_distances)
                                                min_distance = center_distances[best_match_idx]
                                                # Log when using final fallback (especially for close matches)
                                                if min_distance < 50:
                                                    print(f"  âš  ANCHOR FALLBACK (close): Frame {current_frame_num}, {anchor_player_name} - using very lenient match (distance: {min_distance:.1f}px) - IoU/area checks may have been too strict")
                                                elif min_distance < 200:
                                                    # Log close matches (within 200px) for first 30 frames
                                                    if current_frame_num < 30:
                                                        print(f"  âš  ANCHOR FALLBACK (nearby): Frame {current_frame_num}, {anchor_player_name} - using lenient match (distance: {min_distance:.1f}px)")
                                                elif current_frame_num % 50 == 0:
                                                    print(f"  âš  ANCHOR FALLBACK: Frame {current_frame_num}, {anchor_player_name} - using very lenient match (distance: {min_distance:.1f}px)")
                                        
                                        # CRITICAL FIX: Handle case where we match a detection but it doesn't have a track_id yet
                                        # This can happen early in the video before the tracker has assigned IDs
                                        # SOLUTION: Match by bbox position and store the match, then apply when tracker assigns track_id
                                        if best_match_idx is not None:
                                            if detections.tracker_id is not None and best_match_idx < len(detections.tracker_id):
                                                track_id = detections.tracker_id[best_match_idx]
                                                if track_id is not None:
                                                    matched_track_id = int(track_id)
                                                    match_method = "bbox position (lenient)"
                                            else:
                                                # Detection matched but no track_id yet - try to match by anchor's track_id if available
                                                # The anchor frame has a track_id from Setup Wizard, so we can use that
                                                if anchor_track_id is not None:
                                                    # Use the anchor's track_id directly - it's from Setup Wizard ground truth
                                                    matched_track_id = int(anchor_track_id)
                                                    match_method = "bbox position (using anchor track_id)"
                                                    if current_frame_num < 10:
                                                        print(f"  ðŸ” ANCHOR MATCH (by bbox+anchor_track_id): Frame {current_frame_num}, '{anchor_player_name}' matched to detection #{best_match_idx} using anchor track_id {anchor_track_id}")
                                                else:
                                                    # No track_id available at all - can't apply anchor frame yet
                                                    # CRITICAL FIX: Don't skip - wait for tracker to assign track_id, then match
                                                    # Store the anchor frame for later matching when track_id is available
                                                    if current_frame_num < 10:
                                                        print(f"  âš  ANCHOR MATCH (no track_id): Frame {current_frame_num}, '{anchor_player_name}' matched to detection #{best_match_idx} but no track_id available (tracker not run yet) - will retry next frame")
                                                    # Store anchor frame info for delayed matching
                                                    # We'll try to match it again in the next frame when tracker has assigned track_id
                                                    # For now, skip this frame but don't give up - the anchor will be checked again next frame
                                                    continue
                                    
                                    # Apply anchor frame assignment if we found a match
                                    if matched_track_id is not None:
                                        track_id_str = str(matched_track_id)
                                        
                                        # DIAGNOSTIC: Log anchor frame matching (first few frames only)
                                        if current_frame_num < 10:
                                            print(f"  ðŸ” ANCHOR MATCH: Frame {current_frame_num}, '{anchor_player_name}' matched to Track #{matched_track_id} via {match_method}")
                                        
                                        # ðŸ›¡ï¸ CRITICAL: Check if this track is already assigned to a DIFFERENT player via anchor protection
                                        # If so, only override if the new anchor is much closer spatially
                                        existing_name = player_names.get(track_id_str, "")
                                        has_existing_anchor = matched_track_id in track_anchor_protection
                                        
                                        if has_existing_anchor and existing_name != anchor_player_name:
                                            existing_prot_name, _, _ = track_anchor_protection[matched_track_id]
                                            if existing_prot_name != anchor_player_name:
                                                # Track is already protected for a different player!
                                                # Only override if new anchor is much closer (within 100px vs 500px)
                                                if best_match_idx is not None:
                                                    new_distance = center_distances[best_match_idx] if best_match_idx < len(center_distances) else float('inf')
                                                    if new_distance > 100:  # New anchor is far away
                                                        if current_frame_num % 50 == 0:
                                                            print(f"  âš  ANCHOR CONFLICT: Track #{matched_track_id} already protected as '{existing_prot_name}', rejecting '{anchor_player_name}' anchor (distance: {new_distance:.1f}px)")
                                                        continue  # Skip this anchor - don't override existing protection
                                                else:
                                                    # No distance info - be conservative, don't override
                                                    if current_frame_num % 50 == 0:
                                                        print(f"  âš  ANCHOR CONFLICT: Track #{matched_track_id} already protected as '{existing_prot_name}', rejecting '{anchor_player_name}' anchor (no distance info)")
                                                    continue
                                        
                                        # ðŸ›¡ï¸ CRITICAL: Check if this PLAYER already has an anchor-protected track in CURRENT frame
                                        # If so, only assign to new track if it's significantly closer (prevents wrong assignments)
                                        # NOTE: Multiple tracks for same player is OK if they're from different time periods (ID switches)
                                        # But we need to prevent assigning same player to multiple tracks in the SAME frame
                                        # OPTIMIZATION: Use player_anchor_protection lookup instead of iterating all tracks
                                        player_has_protected_track_in_frame = False
                                        existing_protected_track = None
                                        existing_protected_distance = float('inf')
                                        
                                        # OPTIMIZED: Check player_anchor_protection directly (faster than iterating all tracks)
                                        if anchor_player_name in player_anchor_protection:
                                            for prot_start, prot_end, prot_bbox, prot_track_id in player_anchor_protection[anchor_player_name]:
                                                if prot_start <= current_frame_num <= prot_end and prot_track_id != matched_track_id:
                                                    # Check if this protected track is active in current frame and different from matched track
                                                    if detections.tracker_id is not None:
                                                        # Check if this protected track_id exists in current detections
                                                        track_ids_array = np.array(detections.tracker_id)
                                                        if prot_track_id in track_ids_array:
                                                            # This player's protected track is active in current frame
                                                            player_has_protected_track_in_frame = True
                                                            existing_protected_track = prot_track_id
                                                            # Find distance if we have center_distances computed
                                                            if 'center_distances' in locals() and len(center_distances) > 0:
                                                                prot_det_idx = np.where(track_ids_array == prot_track_id)[0]
                                                                if len(prot_det_idx) > 0 and prot_det_idx[0] < len(center_distances):
                                                                    existing_protected_distance = center_distances[prot_det_idx[0]]
                                                            break
                                                if player_has_protected_track_in_frame:
                                                    break
                                    
                                        if player_has_protected_track_in_frame and existing_protected_track != matched_track_id:
                                            # Player already has an active protected track in this frame
                                            # Calculate distance from anchor to existing protected track
                                            if anchor_bbox is not None and len(anchor_bbox) >= 4:
                                                anchor_center_x = (anchor_bbox[0] + anchor_bbox[2]) / 2
                                                anchor_center_y = (anchor_bbox[1] + anchor_bbox[3]) / 2
                                                
                                                # Find existing track's position
                                                existing_track_distance = float('inf')
                                                if detections.tracker_id is not None:
                                                    for det_idx, track_id in enumerate(detections.tracker_id):
                                                        if track_id == existing_protected_track:
                                                            det_center_x = (det_xyxy[det_idx][0] + det_xyxy[det_idx][2]) / 2
                                                            det_center_y = (det_xyxy[det_idx][1] + det_xyxy[det_idx][3]) / 2
                                                            existing_track_distance = np.sqrt((det_center_x - anchor_center_x)**2 + (det_center_y - anchor_center_y)**2)
                                                            break
                                                
                                                # Get new track's distance from anchor
                                                new_track_distance = float('inf')
                                                if best_match_idx is not None and best_match_idx < len(center_distances):
                                                    new_track_distance = center_distances[best_match_idx]
                                                
                                                # Only allow if new track is significantly closer (at least 2x closer)
                                                if new_track_distance >= existing_track_distance * 0.5:
                                                    if current_frame_num % 50 == 0:
                                                        print(f"  âš  PLAYER ALREADY IN FRAME: '{anchor_player_name}' already on Track #{existing_protected_track} (dist from anchor: {existing_track_distance:.1f}px), rejecting Track #{matched_track_id} (dist: {new_track_distance:.1f}px)")
                                                    continue  # Skip - existing track is closer or similar distance
                                        
                                        # ANCHOR FRAME: Apply with 1.00 confidence (highest priority)
                                        # Override any existing assignment - anchor frames are ground truth
                                        # CRITICAL: Check if anchor_player_name is already assigned to a different track
                                        if anchor_player_name in player_to_track_global:
                                            old_track_id = player_to_track_global[anchor_player_name]
                                            if old_track_id != matched_track_id:
                                                # Player is on a different track - clear it first (anchor frames are ground truth)
                                                old_track_str = str(int(old_track_id))
                                                if old_track_str in player_names:
                                                    player_names[old_track_str] = ""  # Clear old track
                                                if old_track_id in track_name_confidence:
                                                    del track_name_confidence[old_track_id]
                                                if current_frame_num % 10 == 0:
                                                    print(f"  ðŸ—‘ï¸ UNIQUENESS (anchor): Cleared '{anchor_player_name}' from Track #{old_track_id} (anchor frame assigns to Track #{matched_track_id})")
                                        
                                        old_name = player_names.get(track_id_str, "")
                                        
                                        # ðŸ›¡ï¸ CRITICAL: Anchor frame application - set ALL three protection mechanisms
                                        # 1. Set player_names (for display)
                                        player_names[track_id_str] = anchor_player_name
                                        
                                        # ðŸ›¡ï¸ CRITICAL: Check if track is already anchor-assigned to a DIFFERENT player
                                        # If so, this is an error - anchor-protected tracks cannot be reassigned
                                        if matched_track_id in track_anchor_assigned:
                                            existing_player, existing_frame = track_anchor_assigned[matched_track_id]
                                            if existing_player != anchor_player_name:
                                                print(f"  âš  ERROR: Track #{matched_track_id} is already owned by '{existing_player}' (anchor frame {existing_frame}) - cannot reassign to '{anchor_player_name}' (anchor frames are permanent ownership)")
                                                continue  # Skip this anchor - track is already owned
                                        else:
                                                # Same player hitting another anchor point - REFRESH confidence and extend protection
                                                print(f"  ðŸ”„ ANCHOR REFRESH: Track #{matched_track_id} '{anchor_player_name}' hit anchor point at frame {current_frame_num} (previous anchor: frame {existing_frame}) - refreshing confidence to 1.00")
                                        
                                        # 2. Set track_name_confidence with 1.00 (highest confidence)
                                        # CRITICAL: Always refresh to 1.00 when track hits an anchor frame
                                        # ðŸ• TEMPORAL CONFIDENCE: Check if this player was on this track or nearby in previous frames
                                        temporal_confidence_boost = 0.0
                                        if current_frame_num > 0:
                                            # Look back to see if this player was on this track or nearby
                                            for lookback_frame in range(max(0, current_frame_num - TEMPORAL_LOOK_BACK_FRAMES), current_frame_num):
                                                if lookback_frame in temporal_player_history:
                                                    frame_data_lookback = temporal_player_history[lookback_frame]
                                                    
                                                    # Check if this track had the same player
                                                    if matched_track_id in frame_data_lookback:
                                                        prev_data = frame_data_lookback[matched_track_id]
                                                        if prev_data['player_name'] == anchor_player_name:
                                                            # Same player on same track - strong temporal evidence
                                                            temporal_confidence_boost += 0.05
                                                            if current_frame_num % 100 == 0:
                                                                print(f"  ðŸ• ANCHOR TEMPORAL: Track #{matched_track_id} '{anchor_player_name}' found in frame {lookback_frame} - reaffirming identity")
                                                    
                                                    # Check if this player was on a nearby track (track change)
                                                    for prev_track_id, prev_data in frame_data_lookback.items():
                                                        if prev_data['player_name'] == anchor_player_name and prev_track_id != matched_track_id:
                                                            # Same player, different track - track change detected
                                                            if prev_data['bbox'] is not None and best_match_idx is not None and detections is not None:
                                                                if best_match_idx < len(detections.xyxy):
                                                                    current_bbox = detections.xyxy[best_match_idx]
                                                                    prev_bbox = prev_data['bbox']
                                                                    prev_center_x = (prev_bbox[0] + prev_bbox[2]) / 2
                                                                    prev_center_y = (prev_bbox[1] + prev_bbox[3]) / 2
                                                                    current_center_x = (current_bbox[0] + current_bbox[2]) / 2
                                                                    current_center_y = (current_bbox[1] + current_bbox[3]) / 2
                                                                    distance = np.sqrt((current_center_x - prev_center_x)**2 + (current_center_y - prev_center_y)**2)
                                                                    
                                                                    if distance < 300:  # 300px threshold
                                                                        temporal_confidence_boost += 0.03
                                                                        if current_frame_num % 100 == 0:
                                                                            print(f"  ðŸ• ANCHOR TEMPORAL: Track #{matched_track_id} '{anchor_player_name}' likely same as Track #{prev_track_id} in frame {lookback_frame} (distance: {distance:.1f}px) - identity maintained across track change")
                                                            
                                                            break  # Found match
                                        
                                        # Anchor frames are always 1.00 confidence, but temporal evidence can be logged
                                        final_confidence = 1.00  # Anchor frames are ground truth
                                        track_name_confidence[matched_track_id] = (anchor_player_name, final_confidence, current_frame_num)
                                        
                                        if temporal_confidence_boost > 0 and current_frame_num % 100 == 0:
                                            print(f"  ðŸ• ANCHOR TEMPORAL EVIDENCE: Track #{matched_track_id} '{anchor_player_name}' - temporal boost: {temporal_confidence_boost:.2f} (anchor confidence: {final_confidence:.2f})")
                                        
                                        # ðŸ›¡ï¸ CRITICAL: Mark this track as anchor-assigned for PERMANENT protection
                                        # This ensures the track maintains its identity for the entire video duration
                                        # If track already exists, update the anchor frame number (refresh)
                                        track_anchor_assigned[matched_track_id] = (anchor_player_name, current_frame_num)
                                        
                                        # ðŸŽ¯ IDENTITY TRACKER: Register anchor frame assignment
                                        if identity_tracker is not None:
                                            # Get bbox from detection or anchor
                                            identity_bbox = anchor_bbox if anchor_bbox else None
                                            if identity_bbox is None and best_match_idx is not None and detections is not None:
                                                if best_match_idx < len(detections.xyxy):
                                                    identity_bbox = detections.xyxy[best_match_idx].tolist()
                                            
                                            if identity_bbox:
                                                identity_tracker.assign_identity(
                                                    track_id=matched_track_id,
                                                    player_name=anchor_player_name,
                                                    confidence=1.00,
                                                    frame_num=current_frame_num,
                                                    bbox=identity_bbox
                                                )
                                                if current_frame_num < 10:
                                                    print(f"  ðŸŽ¯ IDENTITY TRACKED: '{anchor_player_name}' â†’ Track #{matched_track_id} (bbox: {identity_bbox})")
                                        
                                        # 3. Set track_anchor_protection (protection window)
                                        # CRITICAL: Extend protection continuously from one anchor point to the next
                                        # If track already has protection, extend it to include this anchor point
                                        if matched_track_id in track_anchor_protection:
                                            # Track already has protection - EXTEND it to include this anchor point
                                            existing_name, existing_start, existing_end = track_anchor_protection[matched_track_id]
                                            if existing_name == anchor_player_name:
                                                # Same player - extend protection window to include this anchor point
                                                new_start = min(existing_start, max(0, current_frame_num - ANCHOR_DECAY_FRAMES))
                                                new_end = max(existing_end, current_frame_num + ANCHOR_DECAY_FRAMES)
                                                track_anchor_protection[matched_track_id] = (anchor_player_name, new_start, new_end)
                                                if current_frame_num % 500 == 0 or current_frame_num < 10:
                                                    print(f"  ðŸ”„ PROTECTION EXTENDED: Track #{matched_track_id} '{anchor_player_name}' - protection extended from frames {existing_start}-{existing_end} to {new_start}-{new_end} (anchor point at frame {current_frame_num})")
                                            else:
                                                # Different player - this shouldn't happen (caught above), but log error
                                                print(f"  âš  ERROR: Track #{matched_track_id} protection conflict - existing: '{existing_name}', new: '{anchor_player_name}'")
                                        else:
                                            # New protection window - create it
                                            protection_start = max(0, current_frame_num - ANCHOR_DECAY_FRAMES)
                                            protection_end = current_frame_num + ANCHOR_DECAY_FRAMES
                                            track_anchor_protection[matched_track_id] = (anchor_player_name, protection_start, protection_end)
                                        
                                        # CRITICAL FIX: Ensure player_names is synced with track_name_confidence
                                        # This ensures the display pipeline shows the correct name
                                        player_names[track_id_str] = anchor_player_name
                                        
                                        # VERIFICATION: Log that all three protection mechanisms are set
                                        has_name = track_id_str in player_names and player_names[track_id_str] == anchor_player_name
                                        has_confidence = matched_track_id in track_name_confidence
                                        conf_name, conf_val, _ = track_name_confidence[matched_track_id] if has_confidence else (None, None, None)
                                        has_protection = matched_track_id in track_anchor_protection
                                        
                                        if has_name and has_confidence and conf_val >= 1.00 and has_protection:
                                            if current_frame_num % 500 == 0 or current_frame_num < 10:
                                                prot_name, prot_start, prot_end = track_anchor_protection[matched_track_id]
                                                print(f"  âœ… ANCHOR VERIFIED: Track #{matched_track_id} '{anchor_player_name}' - name:âœ“ confidence:{conf_val:.2f}âœ“ protection:{prot_start}-{prot_end}âœ“")
                                        else:
                                            # Log warning if any protection mechanism is missing
                                            print(f"  âš  ANCHOR INCOMPLETE: Track #{matched_track_id} '{anchor_player_name}' - name:{has_name} confidence:{has_confidence}({conf_val}) protection:{has_protection}")
                                        
                                        # Update global player-to-track mapping
                                        player_to_track_global[anchor_player_name] = matched_track_id
                                        
                                        # ENHANCEMENT: Mirror jersey + team assignments from anchor frame
                                        anchor_jersey = anchor.get('jersey_number')
                                        anchor_team = anchor.get('team')
                                        if anchor_jersey:
                                            jersey_to_track_global[anchor_jersey] = matched_track_id
                                            if current_frame_num % 500 == 0 or current_frame_num < 10:
                                                print(f"  âœ… ANCHOR JERSEY: Track #{matched_track_id} '{anchor_player_name}' â†’ jersey #{anchor_jersey}")
                                        if anchor_team:
                                            track_to_team_global[matched_track_id] = anchor_team
                                            player_to_team_global[anchor_player_name] = (anchor_team, matched_track_id)
                                            if current_frame_num % 500 == 0 or current_frame_num < 10:
                                                print(f"  âœ… ANCHOR TEAM: Track #{matched_track_id} '{anchor_player_name}' â†’ team '{anchor_team}'")
                                        
                                        # VERIFICATION: Log that bbox from anchor frame is available for matching
                                        anchor_bbox_available = anchor.get('bbox') is not None
                                        if anchor_bbox_available and (current_frame_num % 500 == 0 or current_frame_num < 10):
                                            print(f"  âœ… ANCHOR BBOX: Track #{matched_track_id} '{anchor_player_name}' has bbox for matching (bbox is critical when track_id changes)")
                                        elif not anchor_bbox_available and current_frame_num < 10:
                                            print(f"  âš  ANCHOR BBOX MISSING: Track #{matched_track_id} '{anchor_player_name}' - no bbox (will rely on track_id matching only)")
                                        
                                        # Optional debug logging (dev mode - can be disabled)
                                        if current_frame_num % 500 == 0:
                                            print(f"  ðŸ”„ Synced name (anchor): Track #{matched_track_id} â†’ {anchor_player_name} (jersey: {anchor_jersey or 'N/A'}, team: {anchor_team or 'N/A'})")
                                        
                                        # LOG: Show when anchor is applied (ALWAYS log - this is critical for debugging)
                                        if old_name != anchor_player_name:
                                            print(f"  âœ… ANCHOR APPLIED: Frame {current_frame_num}, Track #{matched_track_id} = '{anchor_player_name}' (was: '{old_name}') via {match_method}")
                                        else:
                                            if current_frame_num % 50 == 0:  # Log every 50 frames to confirm anchor is still active
                                                print(f"  âœ… ANCHOR CONFIRMED: Frame {current_frame_num}, Track #{matched_track_id} = '{anchor_player_name}' (already correct) via {match_method}")
                                        
                                        # ðŸ”’ ROUTE LOCKING: Lock route for anchor frames (regardless of frame number)
                                        # Anchor frames are ground truth and should always lock routes
                                        try:
                                            import shared_state
                                            if shared_state.lock_early_route(anchor_player_name, matched_track_id, current_frame_num, force_lock=True):
                                                # Only print if this is a NEW lock (not already locked)
                                                if is_processing:
                                                    print(f"  ðŸ”’ ROUTE LOCKED: {anchor_player_name} â†’ Track #{matched_track_id} (Frame {current_frame_num}) - anchor frame locks this route")
                                        except:
                                            pass  # Silently fail if shared_state not available
                                        
                                        # ANCHOR FRAME PROTECTION: Lock this track's ID with decay zones
                                        # Use pre-calculated full protection window if available (covers all anchor frames)
                                        # Otherwise calculate per-frame protection
                                        if matched_track_id in track_anchor_protection:
                                            # Use existing full protection window (pre-calculated from all anchor frames)
                                            old_name, protection_start, protection_end = track_anchor_protection[matched_track_id]
                                            # Update name if needed
                                            if old_name != anchor_player_name:
                                                track_anchor_protection[matched_track_id] = (anchor_player_name, protection_start, protection_end)
                                        else:
                                            # Calculate per-frame protection (fallback if not pre-calculated)
                                            protection_start = max(0, current_frame_num - ANCHOR_DECAY_FRAMES)
                                            protection_end = current_frame_num + ANCHOR_DECAY_FRAMES
                                            track_anchor_protection[matched_track_id] = (anchor_player_name, protection_start, protection_end)
                                        
                                        # ðŸ›¡ï¸ PLAYER-NAME-BASED PROTECTION: Store protection by player name (works even if track ID changes)
                                        # Get the detection bbox for this matched track (for spatial recovery)
                                        recovery_bbox = None
                                        if matched_track_id is not None and detections is not None and detections.tracker_id is not None:
                                            for det_idx, track_id in enumerate(detections.tracker_id):
                                                if track_id == matched_track_id and det_idx < len(detections.xyxy):
                                                    recovery_bbox = detections.xyxy[det_idx].tolist()
                                                    break
                                        
                                        # If no detection bbox, use anchor bbox
                                        if recovery_bbox is None and anchor_bbox is not None:
                                            recovery_bbox = anchor_bbox[:4] if len(anchor_bbox) >= 4 else None
                                        
                                        # Store player-name-based protection (use full protection window from track_anchor_protection)
                                        if anchor_player_name not in player_anchor_protection:
                                            player_anchor_protection[anchor_player_name] = []
                                        
                                        # Use the full protection window (already calculated and stored in track_anchor_protection)
                                        # This ensures all 32 anchor frames create one continuous protection window
                                        
                                        # Check if protection window already exists for this track
                                        track_protection_idx = None
                                        for i, (ps, pe, pb, pt) in enumerate(player_anchor_protection[anchor_player_name]):
                                            if pt == matched_track_id:
                                                track_protection_idx = i
                                                break
                                        
                                        if track_protection_idx is not None:
                                            # Update existing protection window with full range and current bbox
                                            player_anchor_protection[anchor_player_name][track_protection_idx] = (
                                                protection_start, protection_end, recovery_bbox, matched_track_id
                                            )
                                        else:
                                            # Add new protection window
                                            new_protection = (protection_start, protection_end, recovery_bbox, matched_track_id)
                                            player_anchor_protection[anchor_player_name].append(new_protection)
                                        
                                        # Merge overlapping protection windows for same player (in case of multiple tracks)
                                        if len(player_anchor_protection[anchor_player_name]) > 1:
                                            # Sort by start frame
                                            player_anchor_protection[anchor_player_name].sort(key=lambda x: x[0])
                                            merged = []
                                            for prot in player_anchor_protection[anchor_player_name]:
                                                if not merged:
                                                    merged.append(prot)
                                                else:
                                                    last_start, last_end, last_bbox, last_tid = merged[-1]
                                                    curr_start, curr_end, curr_bbox, curr_tid = prot
                                                    # If overlapping or adjacent (within 10 frames), merge
                                                    if curr_start <= last_end + 10:
                                                        merged_end = max(last_end, curr_end)
                                                        # Use most recent bbox
                                                        merged_bbox = curr_bbox if curr_bbox else last_bbox
                                                        merged_tid = curr_tid  # Use most recent track_id
                                                        merged[-1] = (last_start, merged_end, merged_bbox, merged_tid)
                                                    else:
                                                        merged.append(prot)
                                            player_anchor_protection[anchor_player_name] = merged
                                        
                                        # Update team if provided
                                        if anchor_team:
                                            track_to_team_global[matched_track_id] = anchor_team
                                        
                                        # ðŸ”„ RETROACTIVE CORRECTION: Fix frames BEFORE this anchor that have wrong names
                                        # Anchor frames apply backward, so we need to correct already-processed frames
                                        # This fixes the issue where frames 1-53 were processed before anchor at frame 54
                                        retroactive_corrections = 0
                                        for retro_frame_num in range(protection_start, current_frame_num):
                                            # Check if this frame has data in frame_detections
                                            if retro_frame_num in frame_detections:
                                                retro_detections = frame_detections[retro_frame_num]
                                                # Find detections with matched_track_id
                                                for retro_idx, retro_track_id in enumerate(retro_detections.get('tracker_id', [])):
                                                    if retro_track_id == matched_track_id:
                                                        # Check current name
                                                        retro_track_id_str = str(retro_track_id)
                                                        current_retro_name = player_names.get(retro_track_id_str, "")
                                                        # If wrong name, correct it
                                                        if current_retro_name != anchor_player_name:
                                                            player_names[retro_track_id_str] = anchor_player_name
                                                            retroactive_corrections += 1
                                                            # Also update the stored frame data
                                                            if 'player_names' in retro_detections:
                                                                retro_detections['player_names'][retro_idx] = anchor_player_name
                                        
                                        # Also correct player_analytics for these frames (used for overlay metadata)
                                        # NOTE: player_analytics is now keyed by player_name, not track_id
                                        # The retroactive correction updates the player_name key if it changed
                                        if retroactive_corrections > 0:
                                            for retro_frame_num in range(protection_start, current_frame_num):
                                                if retro_frame_num in player_analytics:
                                                    retro_analytics = player_analytics[retro_frame_num]
                                                    # Find analytics entry for this track_id (might be under old player_name)
                                                    # Get old player_name for this track_id
                                                    old_player_name = player_names.get(str(matched_track_id), f"Player_{matched_track_id}")
                                                    # If the old name doesn't match anchor name, we need to update the key
                                                    if old_player_name != anchor_player_name and old_player_name in retro_analytics:
                                                        # Move analytics from old name to new name
                                                        retro_analytics[anchor_player_name] = retro_analytics.pop(old_player_name)
                                            
                                            print(f"  ðŸ”„ RETROACTIVE: Corrected {retroactive_corrections} frames (f{protection_start}-f{current_frame_num-1}) for Track #{matched_track_id} â†’ {anchor_player_name}")
                                        
                                        # ðŸŽ“ RE-ID LEARNING: Extract Re-ID features from anchor frame and update gallery immediately
                                        # ANCHOR FRAMES ARE ABSOLUTE TRUTH - The player is manually tagged, so we KNOW WHO THE PLAYER IS
                                        # These features should be used for learning each player properly with maximum confidence (1.00)
                                        # All features (body, jersey, foot) are extracted and saved to the gallery for future matching
                                        # This ensures future matching uses features from anchor frames (ground truth), not old gallery features
                                        if player_gallery is not None and reid_tracker is not None:
                                            # DIAGNOSTIC: Log when we attempt to extract features from anchor frame
                                            if current_frame_num < 10 or current_frame_num % 100 == 0:
                                                print(f"  ðŸ” DIAGNOSTIC: Attempting to extract features for anchor frame '{anchor_player_name}' (frame {current_frame_num}, track_id={anchor_track_id})")
                                            try:
                                                # Find the detection index that matches this anchor frame
                                                detection_idx = None
                                                if anchor_track_id is not None:
                                                    # Match by track ID
                                                    if detections.tracker_id is not None:
                                                        for det_idx, track_id in enumerate(detections.tracker_id):
                                                            if track_id is not None and int(track_id) == int(anchor_track_id):
                                                                detection_idx = det_idx
                                                                break
                                                elif best_match_idx is not None:
                                                    # Match by bbox position (from earlier matching)
                                                    detection_idx = best_match_idx
                                                
                                                # If we found the detection, extract/use Re-ID features
                                                detection_feature = None
                                                if detection_idx is not None and detection_idx < len(detections.xyxy):
                                                    # DIAGNOSTIC: Log detection matching
                                                    if current_frame_num < 10:
                                                        print(f"  ðŸ” DIAGNOSTIC: Found detection #{detection_idx} for anchor '{anchor_player_name}' (bbox: {detections.xyxy[detection_idx] if detection_idx < len(detections.xyxy) else 'N/A'})")
                                                    # Try to use existing reid_features if available
                                                    if reid_features is not None and detection_idx < len(reid_features):
                                                        detection_feature = reid_features[detection_idx]
                                                        if current_frame_num < 10:
                                                            print(f"  ðŸ” DIAGNOSTIC: Using existing Re-ID features for detection #{detection_idx}")
                                                    else:
                                                        # Extract Re-ID features on-demand for this anchor frame detection
                                                        # This ensures we always update the gallery, even if features weren't extracted yet
                                                        try:
                                                            # Get the detection bbox
                                                            anchor_det_bbox = detections.xyxy[detection_idx]
                                                            
                                                            # Create a single-detection Detections object for Re-ID extraction
                                                            scale_factor = frame_data.get('scale_factor', 1.0)
                                                            if scale_factor != 1.0:
                                                                # Scale coordinates back down to YOLO resolution for Re-ID
                                                                anchor_xyxy = anchor_det_bbox / scale_factor
                                                            else:
                                                                anchor_xyxy = anchor_det_bbox
                                                            
                                                            # Create single-detection object
                                                            anchor_detection = sv.Detections(
                                                                xyxy=np.array([anchor_xyxy]),
                                                                confidence=np.array([detections.confidence[detection_idx]] if detection_idx < len(detections.confidence) else [0.5]),
                                                                class_id=np.array([detections.class_id[detection_idx]] if detection_idx < len(detections.class_id) else [0]),
                                                                tracker_id=np.array([detections.tracker_id[detection_idx]] if detections.tracker_id is not None and detection_idx < len(detections.tracker_id) else [None])
                                                            )
                                                            
                                                            # Get frame for Re-ID (use original sharp frame if available)
                                                            frame_for_reid = batch_frame
                                                            original_frame = frame_data.get('original_frame_for_learning', None)
                                                            if original_frame is not None:
                                                                frame_for_reid = original_frame
                                                            elif roi_bounds is not None:
                                                                full_frame = frame_data.get('full_frame', None)
                                                                if full_frame is not None:
                                                                    frame_for_reid = full_frame
                                                            
                                                            # Extract features for this single detection
                                                            if current_frame_num < 10:
                                                                print(f"  ðŸ” DIAGNOSTIC: Extracting Re-ID features on-demand for anchor '{anchor_player_name}' (bbox: {anchor_xyxy})")
                                                            anchor_features = reid_tracker.extract_features(
                                                                frame_for_reid, anchor_detection, team_colors, ball_colors)
                                                            
                                                            if anchor_features is not None and len(anchor_features) > 0:
                                                                detection_feature = anchor_features[0]  # Get first (and only) feature
                                                                if current_frame_num < 10 or current_frame_num % 100 == 0:
                                                                    print(f"  ðŸ” Extracted Re-ID features on-demand for anchor frame: {anchor_player_name} (feature shape: {detection_feature.shape if hasattr(detection_feature, 'shape') else 'N/A'})")
                                                            else:
                                                                if current_frame_num < 10:
                                                                    print(f"  âš  DIAGNOSTIC: extract_features returned None or empty for anchor '{anchor_player_name}'")
                                                        except Exception as e:
                                                            print(f"  âš  Could not extract Re-ID features on-demand for anchor frame '{anchor_player_name}' (frame {current_frame_num}): {e}")
                                                            if current_frame_num < 10:
                                                                import traceback
                                                                traceback.print_exc()
                                                
                                                # Update gallery if we have features
                                                if detection_feature is not None:
                                                    # DIAGNOSTIC: Log feature availability
                                                    if current_frame_num < 10:
                                                        print(f"  ðŸ” DIAGNOSTIC: Have detection_feature for '{anchor_player_name}' (shape: {detection_feature.shape if hasattr(detection_feature, 'shape') else type(detection_feature)})")
                                                    
                                                    # Get player ID from gallery by searching for name
                                                    # CRITICAL: Ensure anchor_player_name is a clean string before gallery lookup
                                                    if not isinstance(anchor_player_name, str):
                                                        if isinstance(anchor_player_name, list) and len(anchor_player_name) > 0:
                                                            anchor_player_name = str(anchor_player_name[0]).strip()
                                                        else:
                                                            anchor_player_name = str(anchor_player_name).strip() if anchor_player_name else ''
                                                    
                                                    if not anchor_player_name:
                                                        print(f"  âš  WARNING: Cannot update gallery - anchor_player_name is empty (frame {current_frame_num})")
                                                        # Skip gallery update but continue with anchor frame application
                                                    else:
                                                        # CRITICAL: Exclude Ellie Hill and Cameron Hill from gallery updates for this video
                                                        # These players are likely incorrectly matched in this video
                                                        anchor_player_name_clean = extract_player_name(anchor_player_name)
                                                        excluded_players = ['Ellie Hill', 'Cameron Hill']
                                                        if anchor_player_name_clean in excluded_players:
                                                            if current_frame_num % 100 == 0:
                                                                print(f"  ðŸš« GALLERY UPDATE SKIPPED: '{anchor_player_name_clean}' excluded from gallery updates for this video (likely incorrect matches)")
                                                            # Skip gallery update for excluded players - don't proceed with player_id lookup or update
                                                        else:
                                                            # Normalize player name for consistent matching
                                                            anchor_player_name_normalized = anchor_player_name_clean
                                                            
                                                            player_id = None
                                                            for pid, profile in player_gallery.players.items():
                                                                # Normalize both names for comparison
                                                                profile_name_clean = extract_player_name(profile.name)
                                                                if profile_name_clean == anchor_player_name_normalized:
                                                                    player_id = pid
                                                                    break
                                                            
                                                            # VERIFICATION: Log which player we're updating to catch name mixing
                                                            if current_frame_num < 10 or current_frame_num % 100 == 0:
                                                                if player_id:
                                                                    existing_name = player_gallery.players[player_id].name
                                                                    existing_name_clean = extract_player_name(existing_name)
                                                                    if existing_name_clean != anchor_player_name_normalized:
                                                                        print(f"  âš  WARNING: Player ID '{player_id}' has name '{existing_name}' but anchor says '{anchor_player_name}' - this may indicate name mixing!")
                                                                    else:
                                                                        print(f"  âœ… VERIFIED: Player ID '{player_id}' matches anchor name '{anchor_player_name}'")
                                                        
                                                            # DIAGNOSTIC: Log player ID lookup
                                                            if current_frame_num < 10:
                                                                if player_id:
                                                                    print(f"  ðŸ” DIAGNOSTIC: Found existing player_id '{player_id}' for '{anchor_player_name}'")
                                                                else:
                                                                    print(f"  ðŸ” DIAGNOSTIC: No existing player_id for '{anchor_player_name}' - will create new")
                                                            
                                                            if not player_id:
                                                                # ðŸŽ“ RE-ID LEARNING: Player doesn't exist in gallery yet - create them with anchor frame features
                                                                # Extract all features (body, jersey, foot) for new player creation
                                                                body_features_new = None
                                                                jersey_features_new = None
                                                                foot_features_new = None
                                                                if reid_tracker is not None and detection_idx is not None and detection_idx < len(detections) and YOLO_AVAILABLE:
                                                                    try:
                                                                        # Get frame for Re-ID
                                                                        frame_for_reid_new = batch_frame
                                                                        original_frame_new = frame_data.get('original_frame_for_learning', None)
                                                                        if original_frame_new is not None:
                                                                            frame_for_reid_new = original_frame_new
                                                                        elif roi_bounds is not None:
                                                                            full_frame_new = frame_data.get('full_frame', None)
                                                                            if full_frame_new is not None:
                                                                                frame_for_reid_new = full_frame_new
                                                                        
                                                                        # Create single detection
                                                                        single_bbox_new = detections.xyxy[detection_idx:detection_idx+1]
                                                                        single_detection_new = sv.Detections(xyxy=single_bbox_new)
                                                                        
                                                                        # Extract all features
                                                                        body_features_new = reid_tracker.extract_body_features(frame_for_reid_new, single_detection_new)
                                                                        if body_features_new is not None and len(body_features_new) > 0:
                                                                            body_features_new = body_features_new[0]
                                                                        
                                                                        jersey_features_new = reid_tracker.extract_jersey_features(frame_for_reid_new, single_detection_new)
                                                                        if jersey_features_new is not None and len(jersey_features_new) > 0:
                                                                            jersey_features_new = jersey_features_new[0]
                                                                        
                                                                        if hasattr(reid_tracker, 'extract_foot_features'):
                                                                            foot_feat_new = reid_tracker.extract_foot_features(frame_for_reid_new, single_detection_new)
                                                                            if foot_feat_new is not None and len(foot_feat_new) > 0:
                                                                                foot_features_new = foot_feat_new[0]
                                                                    except Exception as e:
                                                                        if current_frame_num < 10:
                                                                            print(f"  âš  Could not extract features for new player creation: {e}")
                                                                
                                                                # Create new player with all anchor frame features (GROUND TRUTH) - use normalized name
                                                                player_id = anchor_player_name_normalized.lower().replace(" ", "_")
                                                                player_gallery.add_player(
                                                                    name=anchor_player_name_normalized,
                                                                    player_id=player_id,
                                                                    features=detection_feature.reshape(1, -1),  # Main Re-ID feature
                                                                    jersey_number=anchor_jersey if anchor_jersey else None,
                                                                    team=anchor_team if anchor_team else None,
                                                                    body_features=body_features_new,  # Body features from anchor frame
                                                                    jersey_features=jersey_features_new,  # Jersey features from anchor frame
                                                                    foot_features=foot_features_new,  # Foot features from anchor frame
                                                                    reference_frame={
                                                                        'frame_num': current_frame_num,
                                                                        'video_path': input_path,
                                                                        'bbox': anchor_bbox if anchor_bbox else None,
                                                                        'confidence': 1.00,  # Anchor frames have 1.00 confidence (ABSOLUTE TRUTH)
                                                                        'similarity': 1.00,  # Anchor frames are ground truth (ABSOLUTE TRUTH)
                                                                        'is_anchor': True  # Mark as anchor frame
                                                                    }
                                                                )
                                                                features_log_new = []
                                                                if body_features_new is not None:
                                                                    features_log_new.append("body")
                                                                if jersey_features_new is not None:
                                                                    features_log_new.append("jersey")
                                                                if foot_features_new is not None:
                                                                    features_log_new.append("foot")
                                                                features_str_new = ", ".join(features_log_new) if features_log_new else "main"
                                                                print(f"  ðŸ“š RE-ID LEARNING: Created new player '{anchor_player_name}' in gallery from anchor frame (frame {current_frame_num}, confidence: 1.00, features: {features_str_new})")
                                                                print(f"     â†’ Anchor frames are ABSOLUTE TRUTH - player is manually tagged, features used for learning")
                                                            else:
                                                                # Update existing player with anchor frame features (high priority)
                                                                # Extract dominant color if available (variables may not exist yet if anchor frames are applied early)
                                                                dominant_color = None
                                                                try:
                                                                    if 'detection_dominant_colors' in locals() and detection_dominant_colors and detection_idx < len(detection_dominant_colors) and detection_dominant_colors[detection_idx] is not None:
                                                                        dominant_color = detection_dominant_colors[detection_idx]
                                                                except:
                                                                    pass
                                                                
                                                                # Extract uniform info (variables may not exist yet)
                                                                uniform_info = None
                                                                try:
                                                                    if 'uniform_info_list' in locals() and uniform_info_list and detection_idx < len(uniform_info_list):
                                                                        uniform_info = uniform_info_list[detection_idx]
                                                                except:
                                                                    pass
                                                                
                                                                # Extract body, jersey, and foot features for highest quality images
                                                                body_features_for_gallery = None
                                                                jersey_features_for_gallery = None
                                                                foot_features_for_gallery = None
                                                                if reid_tracker is not None and detection_idx < len(detections) and YOLO_AVAILABLE:
                                                                    try:
                                                                        # Create single detection for this player
                                                                        single_bbox = detections.xyxy[detection_idx:detection_idx+1]
                                                                        single_detection = sv.Detections(xyxy=single_bbox)
                                                                        
                                                                        # Extract body features (full bbox)
                                                                        body_features_for_gallery = reid_tracker.extract_body_features(
                                                                            frame_for_reid, single_detection)
                                                                        if body_features_for_gallery is not None and len(body_features_for_gallery) > 0:
                                                                            body_features_for_gallery = body_features_for_gallery[0]
                                                                        
                                                                        # Extract jersey features (torso region)
                                                                        jersey_features_for_gallery = reid_tracker.extract_jersey_features(
                                                                            frame_for_reid, single_detection)
                                                                        if jersey_features_for_gallery is not None and len(jersey_features_for_gallery) > 0:
                                                                            jersey_features_for_gallery = jersey_features_for_gallery[0]
                                                                            
                                                                            # Extract foot features (foot/shoe region)
                                                                            if hasattr(reid_tracker, 'extract_foot_features'):
                                                                                foot_feat = reid_tracker.extract_foot_features(frame_for_reid, single_detection)
                                                                                if foot_feat is not None and len(foot_feat) > 0:
                                                                                    foot_features_for_gallery = foot_feat[0]
                                                                    except Exception as e:
                                                                        # Don't fail if feature extraction fails
                                                                        if current_frame_num % 100 == 0:
                                                                            print(f"  âš  Could not extract body/jersey/foot features: {e}")
                                                                
                                                                # Update gallery with anchor frame features (these are ground truth, so high weight)
                                                                player_gallery.update_player(
                                                                    player_id=player_id,
                                                                    features=detection_feature.reshape(1, -1),  # Update with anchor frame features
                                                                    dominant_color=dominant_color,
                                                                    reference_frame={
                                                                        'frame_num': current_frame_num,
                                                                        'video_path': input_path,
                                                                        'bbox': anchor_bbox if anchor_bbox else (detections.xyxy[detection_idx].tolist() if detection_idx < len(detections.xyxy) else None),
                                                                        'confidence': 1.00,  # Anchor frames have 1.00 confidence
                                                                        'similarity': 1.00  # Anchor frames are ground truth
                                                                    },
                                                                    uniform_info=uniform_info,
                                                                    team=anchor_team if anchor_team else None,
                                                                    body_features=body_features_for_gallery,
                                                                    jersey_features=jersey_features_for_gallery,
                                                                    foot_features=foot_features_for_gallery
                                                                )
                                                                
                                                                # Always log anchor frame gallery updates (they're important!)
                                                                # Log what features were updated
                                                                features_log = []
                                                                if body_features_for_gallery is not None:
                                                                    features_log.append("body")
                                                                if jersey_features_for_gallery is not None:
                                                                    features_log.append("jersey")
                                                                if foot_features_for_gallery is not None:
                                                                    features_log.append("foot")
                                                                features_str = ", ".join(features_log) if features_log else "main"
                                                                print(f"  ðŸ“š RE-ID LEARNING: Updated gallery for '{anchor_player_name}' with anchor frame features (frame {current_frame_num}, confidence: 1.00, similarity: 1.00, features: {features_str})")
                                                                print(f"     â†’ Anchor frames are ABSOLUTE TRUTH - player is manually tagged, features used for learning")
                                                                
                                                                # CRITICAL: Save gallery after anchor frame update to persist features
                                                                try:
                                                                    player_gallery.save_gallery()
                                                                    if current_frame_num % 50 == 0:  # Log every 50 frames to avoid spam
                                                                        print(f"  ðŸ’¾ Saved gallery after anchor frame update (frame {current_frame_num})")
                                                                except Exception as save_error:
                                                                    if current_frame_num % 100 == 0:
                                                                        print(f"  âš  Could not save gallery after anchor update: {save_error}")
                                            except Exception as e:
                                                # Log errors more frequently so we can debug issues
                                                print(f"  âš  Could not update gallery for anchor frame '{anchor_player_name}' (frame {current_frame_num}): {e}")
                                                import traceback
                                                if current_frame_num % 100 == 0:  # Full traceback every 100 frames
                                                    traceback.print_exc()
                                        
                                        anchors_applied += 1
                                        
                                        if current_frame_num % 100 == 0:
                                            print(f"  ðŸ”’ ANCHOR FRAME: Frame {current_frame_num}, Track #{matched_track_id} = {anchor_player_name} (confidence: 1.00, matched by {match_method})")
                                    else:
                                        # Anchor frame failed to match - log for debugging
                                        # Only log if this is a real player (not "Unknown Player" from converted tracks)
                                        if anchor_player_name and not anchor_player_name.startswith('Unknown'):
                                            # DIAGNOSTIC: Always log failed anchor matches (not just every 50 frames) for first 30 frames
                                            should_log = (current_frame_num < 30) or (current_frame_num % 50 == 0)
                                            if should_log:
                                                bbox_info = f"bbox={anchor_bbox}" if anchor_bbox else "no bbox"
                                                track_id_info = f"track_id={anchor_track_id}" if anchor_track_id is not None else "no track_id"
                                                
                                                # DIAGNOSTIC: Show what detections exist
                                                detection_info = []
                                                if detections is not None and detections.tracker_id is not None:
                                                    unique_track_ids = set()
                                                    for tid in detections.tracker_id:
                                                        if tid is not None:
                                                            unique_track_ids.add(int(tid))
                                                    if unique_track_ids:
                                                        detection_info.append(f"detected tracks: {sorted(unique_track_ids)[:10]}")  # Show first 10
                                                    else:
                                                        detection_info.append("no tracks in detections")
                                                else:
                                                    detection_info.append("no detections")
                                                
                                                # DIAGNOSTIC: Show why matching failed
                                                failure_reason = []
                                                if anchor_track_id is not None:
                                                    if detections is None or detections.tracker_id is None:
                                                        failure_reason.append("no detections")
                                                    else:
                                                        track_id_found = False
                                                        for tid in detections.tracker_id:
                                                            if tid is not None and int(tid) == int(anchor_track_id):
                                                                track_id_found = True
                                                                break
                                                        if not track_id_found:
                                                            failure_reason.append(f"track_id {anchor_track_id} not found")
                                                
                                                if anchor_bbox is not None and len(anchor_bbox) >= 4:
                                                    if detections is None or len(detections) == 0:
                                                        failure_reason.append("no detections for bbox matching")
                                                    else:
                                                        # Check if any detection is near the anchor bbox
                                                        anchor_center_x = (anchor_bbox[0] + anchor_bbox[2]) / 2
                                                        anchor_center_y = (anchor_bbox[1] + anchor_bbox[3]) / 2
                                                        min_distance = float('inf')
                                                        # Compute distances if detections exist
                                                        if detections.xyxy is not None and len(detections.xyxy) > 0:
                                                            det_centers_x_temp = (detections.xyxy[:, 0] + detections.xyxy[:, 2]) / 2
                                                            det_centers_y_temp = (detections.xyxy[:, 1] + detections.xyxy[:, 3]) / 2
                                                            distances = np.sqrt((det_centers_x_temp - anchor_center_x)**2 + (det_centers_y_temp - anchor_center_y)**2)
                                                            if len(distances) > 0:
                                                                min_distance = float(np.min(distances))
                                                        # Use more lenient threshold for anchor matching (1000px instead of 500px)
                                                        # This handles cases where tracking has drifted or players moved significantly
                                                        anchor_distance_threshold = 1000.0
                                                        if min_distance > anchor_distance_threshold:
                                                            failure_reason.append(f"bbox too far (nearest: {min_distance:.1f}px > {anchor_distance_threshold:.0f}px)")
                                                        else:
                                                            # Distance is within threshold but matching still failed - this suggests IoU/area checks were too strict
                                                            failure_reason.append(f"bbox matching failed (nearest: {min_distance:.1f}px) - IoU/area checks may be too strict")
                                                            # CRITICAL: If distance is reasonable (< 1000px), we should still match - this is ground truth
                                                            # The matching logic above should have caught this, so log for debugging
                                                            if current_frame_num < 30:
                                                                print(f"  ðŸš¨ ANCHOR MATCHING BUG: Frame {current_frame_num}, {anchor_player_name} - distance {min_distance:.1f}px is within threshold but no match found! This should not happen.")
                                                
                                                # If anchor has no bbox, add helpful message
                                                if anchor_bbox is None:
                                                    failure_reason.append("no bbox (anchor frame missing bbox - recreate from CSV with 'Convert Tracks â†’ Anchor Frames')")
                                                
                                                reason_text = f" - {', '.join(failure_reason)}" if failure_reason else ""
                                                detection_text = f" ({', '.join(detection_info)})" if detection_info else ""
                                                # CRITICAL: Always log failed anchor matches - they're ground truth and MUST match
                                                print(f"  âš  ANCHOR FRAME FAILED: Frame {current_frame_num}, {anchor_player_name} - {bbox_info}, {track_id_info}{reason_text}{detection_text}")
                                                # If distance is very close (< 20px), this is a critical failure - matching should have worked
                                                if anchor_bbox is not None and len(anchor_bbox) >= 4 and detections is not None and len(detections) > 0:
                                                    anchor_center_x = (anchor_bbox[0] + anchor_bbox[2]) / 2
                                                    anchor_center_y = (anchor_bbox[1] + anchor_bbox[3]) / 2
                                                    if detections.xyxy is not None and len(detections.xyxy) > 0:
                                                        det_centers_x = (detections.xyxy[:, 0] + detections.xyxy[:, 2]) / 2
                                                        det_centers_y = (detections.xyxy[:, 1] + detections.xyxy[:, 3]) / 2
                                                        distances = np.sqrt((det_centers_x - anchor_center_x)**2 + (det_centers_y - anchor_center_y)**2)
                                                        if len(distances) > 0:
                                                            min_dist = float(np.min(distances))
                                                            if min_dist < 20:
                                                                print(f"  ðŸš¨ CRITICAL: Anchor frame has detection within {min_dist:.1f}px but matching failed! This should not happen - matching logic may be too strict.")
                            
                            if anchors_applied > 0 and current_frame_num % 100 == 0:
                                print(f"  âœ“ Applied {anchors_applied} anchor frame tag(s) at frame {current_frame_num}")
                            elif current_frame_num in anchor_frames and anchors_applied == 0:
                                # Warn if anchor frames exist but none were applied
                                total_anchors = len(anchor_frames[current_frame_num])
                                if current_frame_num % 50 == 0 or current_frame_num < 10:
                                    print(f"  âš  WARNING: Frame {current_frame_num} has {total_anchors} anchor frame(s) but NONE matched! Check bbox positions and track IDs.")
                                    # Enhanced diagnostics: Show what anchor frames exist and what detections are available
                                    if anchor_tags:
                                        for anchor in anchor_tags:
                                            anchor_player = anchor.get('player_name', 'Unknown')
                                            anchor_track_id = anchor.get('track_id')
                                            anchor_bbox = anchor.get('bbox')
                                            has_track_id = anchor_track_id is not None
                                            has_bbox = anchor_bbox is not None and len(anchor_bbox) >= 4
                                            detections_count = len(detections.xyxy) if detections is not None and hasattr(detections, 'xyxy') and detections.xyxy is not None else 0
                                            tracker_ids_count = sum(1 for tid in (detections.tracker_id if detections is not None and detections.tracker_id is not None else []) if tid is not None) if detections is not None else 0
                                            print(f"      â†’ Anchor '{anchor_player}': track_id={has_track_id}, bbox={has_bbox}, detections={detections_count}, tracks_with_id={tracker_ids_count}")
                                            if anchor_track_id is not None and detections is not None and detections.tracker_id is not None:
                                                track_id_found = int(anchor_track_id) in [int(tid) for tid in detections.tracker_id if tid is not None]
                                                print(f"         â†’ Track ID {anchor_track_id} in detections: {track_id_found}")
                                            if anchor_bbox and detections is not None and hasattr(detections, 'xyxy') and detections.xyxy is not None:
                                                # Check if any detection is near the anchor bbox
                                                anchor_x1, anchor_y1, anchor_x2, anchor_y2 = anchor_bbox[:4]
                                                anchor_center_x = (anchor_x1 + anchor_x2) / 2
                                                anchor_center_y = (anchor_y1 + anchor_y2) / 2
                                                min_distance = float('inf')
                                                for det_bbox in detections.xyxy:
                                                    det_center_x = (det_bbox[0] + det_bbox[2]) / 2
                                                    det_center_y = (det_bbox[1] + det_bbox[3]) / 2
                                                    distance = np.sqrt((det_center_x - anchor_center_x)**2 + (det_center_y - anchor_center_y)**2)
                                                    min_distance = min(min_distance, distance)
                                                print(f"         â†’ Nearest detection distance: {min_distance:.1f}px (threshold: 1000px)")
                                    # Show available detections
                                    if detections is not None and len(detections) > 0:
                                        if detections.tracker_id is not None:
                                            unique_tracks = sorted(set(int(t) for t in detections.tracker_id if t is not None))
                                            print(f"      â†’ Available tracks in frame: {unique_tracks[:20]}")  # Show first 20
                                        if detections.xyxy is not None and len(detections.xyxy) > 0:
                                            print(f"      â†’ Available detections: {len(detections.xyxy)} bboxes")
                                    else:
                                        print(f"      â†’ No detections available in this frame")
                        
                        # ðŸ›¡ï¸ SPATIAL RECOVERY: Recover protected players even if track ID changed
                        # This runs AFTER anchor frames but BEFORE Re-ID, so we can recover players before Re-ID assigns wrong names
                        # ENHANCED: Proactively maintain identity for ALL protected players in EVERY frame within protection window
                        # CRITICAL FIX: Also detect when anchor-protected players switch to NEW active tracks (track ID change without disappearance)
                        if detections is not None and len(detections) > 0 and detections.tracker_id is not None:
                            # FIRST: Check for track ID changes (player switches to new track while old track may still be active)
                            # This handles cases like Anay switching from track 1798 â†’ 1805 â†’ 1814 â†’ 1830
                            for player_name, assigned_track_id in list(player_to_track_global.items()):
                                # Only check anchor-protected players (they should maintain identity across track changes)
                                if player_name in player_anchor_protection or assigned_track_id in track_anchor_protection:
                                    # Check if assigned track is still active in current frame
                                    assigned_track_active = False
                                    for tid in detections.tracker_id:
                                        if tid is not None and int(tid) == int(assigned_track_id):
                                            assigned_track_active = True
                                            break
                                    
                                    # If assigned track is NOT active, player may have switched to a new track
                                    if not assigned_track_active:
                                        # Find closest detection to where player was last seen
                                        last_bbox = None
                                        if assigned_track_id in track_state and 'xyxy' in track_state[assigned_track_id]:
                                            last_bbox = track_state[assigned_track_id]['xyxy']
                                        elif player_name in player_anchor_protection:
                                            # Get last known bbox from protection window
                                            for prot_start, prot_end, prot_bbox, _ in player_anchor_protection[player_name]:
                                                if prot_start <= current_frame_num <= prot_end:
                                                    last_bbox = prot_bbox
                                                    break
                                        
                                        if last_bbox is not None and len(last_bbox) >= 4:
                                            last_center_x = (last_bbox[0] + last_bbox[2]) / 2
                                            last_center_y = (last_bbox[1] + last_bbox[3]) / 2
                                            
                                            # Find closest detection within recovery distance
                                            best_new_track_id = None
                                            best_distance = float('inf')
                                            
                                            for det_idx, track_id in enumerate(detections.tracker_id):
                                                if track_id is not None and det_idx < len(detections.xyxy):
                                                    # Skip if this track is already assigned to a different player with high confidence
                                                    track_id_str = str(int(track_id))
                                                    track_already_named = track_id_str in player_names and player_names[track_id_str] and player_names[track_id_str] != player_name
                                                    track_has_high_conf = False
                                                    if int(track_id) in track_name_confidence:
                                                        _, track_conf, _ = track_name_confidence[int(track_id)]
                                                        track_has_high_conf = track_conf >= 0.70
                                                    
                                                    if not track_already_named and not track_has_high_conf:
                                                        det_bbox = detections.xyxy[det_idx]
                                                        det_center_x = (det_bbox[0] + det_bbox[2]) / 2
                                                        det_center_y = (det_bbox[1] + det_bbox[3]) / 2
                                                        distance = np.sqrt((det_center_x - last_center_x)**2 + (det_center_y - last_center_y)**2)
                                                        
                                                        # Calculate IoU for high-confidence reconnection
                                                        iou = 0.0
                                                        try:
                                                            x1_inter = max(last_bbox[0], det_bbox[0])
                                                            y1_inter = max(last_bbox[1], det_bbox[1])
                                                            x2_inter = min(last_bbox[2], det_bbox[2])
                                                            y2_inter = min(last_bbox[3], det_bbox[3])
                                                            if x2_inter > x1_inter and y2_inter > y1_inter:
                                                                inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)
                                                                last_area = (last_bbox[2] - last_bbox[0]) * (last_bbox[3] - last_bbox[1])
                                                                det_area = (det_bbox[2] - det_bbox[0]) * (det_bbox[3] - det_bbox[1])
                                                                union_area = last_area + det_area - inter_area
                                                                if union_area > 0:
                                                                    iou = inter_area / union_area
                                                        except:
                                                            pass
                                                        
                                                        # Use same recovery distance as spatial recovery (350px for anchor-protected)
                                                        # ENHANCED: If IoU is very high (>0.9), allow reconnection even at longer distances (up to 500px)
                                                        # This handles cases where tracker fails to reconnect despite excellent IoU
                                                        max_recovery_distance = 500 if iou > 0.9 else 350
                                                        if distance < max_recovery_distance and distance < best_distance:
                                                            best_distance = distance
                                                            best_new_track_id = int(track_id)
                                                            # Store IoU for logging
                                                            if iou > 0.9:
                                                                if current_frame_num % 50 == 0:
                                                                    print(f"  ðŸ”— HIGH IoU RECOVERY: '{player_name}' Track #{assigned_track_id} â†’ #{track_id} (IoU: {iou:.3f}, distance: {distance:.1f}px) - forcing reconnection despite tracker failure")
                                            
                                            # Reassign player to new track if found within recovery distance
                                            if best_new_track_id is not None:
                                                # Clear old track assignment
                                                old_track_str = str(int(assigned_track_id))
                                                if old_track_str in player_names:
                                                    player_names[old_track_str] = ""
                                                if assigned_track_id in track_name_confidence:
                                                    del track_name_confidence[assigned_track_id]
                                                
                                                # Assign to new track
                                                new_track_str = str(best_new_track_id)
                                                player_names[new_track_str] = player_name
                                                player_to_track_global[player_name] = best_new_track_id
                                                
                                                # Transfer anchor protection to new track
                                                if assigned_track_id in track_anchor_protection:
                                                    prot_name, prot_start, prot_end = track_anchor_protection[assigned_track_id]
                                                    track_anchor_protection[best_new_track_id] = (prot_name, prot_start, prot_end)
                                                    # CRITICAL: Remove protection from old track to prevent conflicts
                                                    del track_anchor_protection[assigned_track_id]
                                                    # Also update track_anchor_assigned
                                                    if assigned_track_id in track_anchor_assigned:
                                                        track_anchor_assigned[best_new_track_id] = track_anchor_assigned[assigned_track_id]
                                                        # CRITICAL: Remove assignment from old track
                                                        del track_anchor_assigned[assigned_track_id]
                                                
                                                # Set high confidence (anchor-protected players maintain identity)
                                                track_name_confidence[best_new_track_id] = (player_name, 1.00, current_frame_num)
                                                
                                                print(f"  ðŸ”„ TRACK CHANGE RECOVERY: Frame {current_frame_num}, '{player_name}' switched from Track #{assigned_track_id} â†’ Track #{best_new_track_id} (distance: {best_distance:.1f}px)")
                                                print(f"     â†’ Anchor protection transferred - identity maintained across track change")
                        
                        if detections is not None and len(detections) > 0 and detections.tracker_id is not None:
                            for player_name, protection_windows in player_anchor_protection.items():
                                # Check if this player has active protection in current frame
                                for prot_start, prot_end, prot_bbox, original_track_id in protection_windows:
                                    if prot_start <= current_frame_num <= prot_end:
                                        # PROACTIVE IDENTITY MAINTENANCE: Check if player needs identity recovery/maintenance
                                        # This ensures players like Rocco are tracked continuously, not just at anchor frames
                                        # Check if player is already assigned to a track in this frame
                                        player_already_assigned = False
                                        assigned_track_id = None
                                        if player_name in player_to_track_global:
                                            assigned_track_id = player_to_track_global[player_name]
                                            # Check if assigned track is active in current frame
                                            if assigned_track_id is not None:
                                                for tid in detections.tracker_id:
                                                    if tid is not None and int(tid) == int(assigned_track_id):
                                                        player_already_assigned = True
                                                        # Ensure player name is set on this track (defensive check)
                                                        track_id_str = str(int(tid))
                                                        if track_id_str not in player_names or player_names[track_id_str] != player_name:
                                                            player_names[track_id_str] = player_name
                                                            if assigned_track_id not in track_name_confidence:
                                                                track_name_confidence[assigned_track_id] = (player_name, 1.00, current_frame_num)
                                                        break
                                        
                                        # CRITICAL: Also check if assigned track is actually in current frame
                                        # If assigned track is not in detections, player is lost and needs recovery
                                        if player_already_assigned:
                                            # Double-check: is the assigned track actually in this frame?
                                            assigned_track_found = False
                                            if assigned_track_id is not None:
                                                for det_idx_check, tid in enumerate(detections.tracker_id):
                                                    if tid is not None and int(tid) == int(assigned_track_id):
                                                        assigned_track_found = True
                                                        # CRITICAL: Update protection bbox to current position (enables spatial recovery in future frames)
                                                        if det_idx_check < len(detections.xyxy):
                                                            current_bbox = detections.xyxy[det_idx_check].tolist()
                                                            # Update protection window with current bbox
                                                            for i, (ps, pe, pb, pt) in enumerate(protection_windows):
                                                                if ps == prot_start and pe == prot_end:
                                                                    protection_windows[i] = (ps, pe, current_bbox, assigned_track_id)
                                                                    # Also update track_anchor_protection bbox if needed
                                                                    if assigned_track_id in track_anchor_protection:
                                                                        prot_name, prot_s, prot_e = track_anchor_protection[assigned_track_id]
                                                                        if prot_name == player_name:
                                                                            # Update protection window to extend if needed
                                                                            new_prot_start = min(prot_s, prot_start)
                                                                            new_prot_end = max(prot_e, prot_end)
                                                                            track_anchor_protection[assigned_track_id] = (player_name, new_prot_start, new_prot_end)
                                                                    break
                                                        break
                                            # If assigned track not found, player is lost - need recovery
                                            if not assigned_track_found:
                                                player_already_assigned = False
                                                if current_frame_num % 10 == 0:
                                                    print(f"  âš  PLAYER LOST: '{player_name}' assigned to Track #{assigned_track_id} but track not found in frame {current_frame_num} - attempting spatial recovery")
                                        
                                        # If player is not assigned (or assigned track is missing), try to recover them spatially
                                        # ENHANCED: Use last known position, travel distance, and velocity prediction
                                        if not player_already_assigned and prot_bbox is not None and len(prot_bbox) >= 4:
                                            # Find detection closest to where protected player was
                                            prot_center_x = (prot_bbox[0] + prot_bbox[2]) / 2
                                            prot_center_y = (prot_bbox[1] + prot_bbox[3]) / 2
                                            
                                            # ENHANCED: Calculate expected position based on last known velocity
                                            # Track last known position and velocity for better recovery
                                            expected_center_x = prot_center_x
                                            expected_center_y = prot_center_y
                                            
                                            # Try to get velocity from track history if available
                                            if original_track_id is not None and original_track_id in track_velocities:
                                                velocity = track_velocities[original_track_id]
                                                if velocity is not None and len(velocity) >= 2:
                                                    # Predict position based on velocity and frames since last seen
                                                    frames_since_last_seen = current_frame_num - (prot_start if prot_start <= current_frame_num else current_frame_num)
                                                    if frames_since_last_seen > 0 and fps > 0:
                                                        time_elapsed = frames_since_last_seen / fps
                                                        expected_center_x = prot_center_x + velocity[0] * time_elapsed
                                                        expected_center_y = prot_center_y + velocity[1] * time_elapsed
                                            
                                            best_recovery_track_id = None
                                            best_recovery_distance = float('inf')
                                            
                                            # Check all detections
                                            for det_idx, track_id in enumerate(detections.tracker_id):
                                                if track_id is not None and det_idx < len(detections.xyxy):
                                                    det_bbox = detections.xyxy[det_idx]
                                                    det_center_x = (det_bbox[0] + det_bbox[2]) / 2
                                                    det_center_y = (det_bbox[1] + det_bbox[3]) / 2
                                                    
                                                    # ENHANCED: Calculate distance from expected position (not just last position)
                                                    distance_from_expected = np.sqrt((det_center_x - expected_center_x)**2 + (det_center_y - expected_center_y)**2)
                                                    distance_from_last = np.sqrt((det_center_x - prot_center_x)**2 + (det_center_y - prot_center_y)**2)
                                                    # Use minimum of expected and last position distances
                                                    distance = min(distance_from_expected, distance_from_last)
                                                    
                                                    # Check if this track is already assigned to a different player
                                                    track_id_str = str(int(track_id))
                                                    track_already_named = track_id_str in player_names and player_names[track_id_str] and player_names[track_id_str] != player_name
                                                    
                                                    # Only consider if within recovery distance and not already named
                                                    # ENHANCED: Increased recovery radius for anchor-protected players (400px)
                                                    # Anchor-protected players should be recovered more aggressively
                                                    # Also check if track has high confidence assignment to another player
                                                    track_has_high_conf_assignment = False
                                                    if track_id_str in track_name_confidence:
                                                        _, track_conf, _ = track_name_confidence[int(track_id)]
                                                        if track_conf >= 0.70:  # High confidence assignment blocks recovery
                                                            track_has_high_conf_assignment = True
                                                    
                                                    # CRITICAL: Don't recover to tracks that are already assigned with high confidence
                                                    # ENHANCED: Increased recovery radius to 400px for anchor-protected players
                                                    if distance < 400 and not track_already_named and not track_has_high_conf_assignment:
                                                        if distance < best_recovery_distance:
                                                            best_recovery_distance = distance
                                                            best_recovery_track_id = int(track_id)
                                            
                                            # Recover player if we found a good match AND distance is reasonable
                                            # ENHANCED: Increased max recovery distance to 350px for anchor-protected players
                                            # Anchor-protected players (Rocco, Cameron, Ellie) should be recovered more aggressively
                                            max_recovery_distance = 350  # pixels - increased for anchor-protected players
                                            if best_recovery_track_id is not None and best_recovery_distance <= max_recovery_distance:
                                                recovery_track_str = str(best_recovery_track_id)
                                                
                                                # Calculate distance from protection center
                                                prot_center_frame = (prot_start + prot_end) // 2
                                                dist_from_center = abs(current_frame_num - prot_center_frame)
                                                
                                                # HARD PROTECTION: Force recovery immediately
                                                if dist_from_center <= ANCHOR_HARD_PROTECTION_FRAMES:
                                                    # CRITICAL: Check if player_name is already assigned to a different track
                                                    if player_name in player_to_track_global:
                                                        old_track_id = player_to_track_global[player_name]
                                                        if old_track_id != best_recovery_track_id:
                                                            # Player is on a different track - clear it first
                                                            old_track_str = str(int(old_track_id))
                                                            if old_track_str in player_names:
                                                                player_names[old_track_str] = ""  # Clear old track
                                                            if old_track_id in track_name_confidence:
                                                                del track_name_confidence[old_track_id]
                                                            if current_frame_num % 10 == 0:
                                                                print(f"  ðŸ—‘ï¸ UNIQUENESS (recovery): Cleared '{player_name}' from Track #{old_track_id} (spatial recovery to Track #{best_recovery_track_id})")
                                                    
                                                    player_names[recovery_track_str] = player_name
                                                    track_name_confidence[best_recovery_track_id] = (player_name, 1.00, current_frame_num)
                                                    # CRITICAL FIX: Ensure player_names is synced (defensive - already set above, but ensures consistency)
                                                    player_names[recovery_track_str] = player_name
                                                    player_to_track_global[player_name] = best_recovery_track_id
                                                    
                                                    # CRITICAL: Transfer anchor protection to recovered track
                                                    # This ensures Rocco maintains protection even when track ID changes (e.g., Track #3 â†’ Track #15)
                                                    track_anchor_protection[best_recovery_track_id] = (player_name, prot_start, prot_end)
                                                    
                                                    # Also update track_anchor_assigned to mark this track as permanently owned
                                                    track_anchor_assigned[best_recovery_track_id] = (player_name, current_frame_num)
                                                    
                                                    # Update protection bbox to new track's position
                                                    for det_idx, track_id in enumerate(detections.tracker_id):
                                                        if track_id == best_recovery_track_id and det_idx < len(detections.xyxy):
                                                            new_bbox = detections.xyxy[det_idx].tolist()
                                                            # Update protection window with new bbox and new track ID
                                                            for i, (ps, pe, pb, pt) in enumerate(protection_windows):
                                                                if ps == prot_start and pe == prot_end:
                                                                    protection_windows[i] = (ps, pe, new_bbox, best_recovery_track_id)
                                                                    break
                                                            break
                                                    
                                                    # ENHANCED: Log recovery with more detail
                                                    print(f"  ðŸ”„ SPATIAL RECOVERY: Frame {current_frame_num}, recovered '{player_name}' from Track #{original_track_id} â†’ Track #{best_recovery_track_id}")
                                                    print(f"     â†’ Distance: {best_recovery_distance:.1f}px (recovery radius: 350px for anchor-protected players)")
                                                    print(f"     â†’ Protection extended to end of video (frame {prot_end})")
                                                    print(f"     â†’ Anchor protection transferred to new track - identity will be maintained")
                                                
                                                # SOFT/DECAY: Still recover but allow Re-ID override with high evidence
                                                elif dist_from_center <= ANCHOR_DECAY_FRAMES:
                                                    # Only recover if track is unnamed or has low confidence
                                                    current_track_name = player_names.get(recovery_track_str, "")
                                                    if not current_track_name or current_track_name.startswith("Player "):
                                                        # CRITICAL: Check if player_name is already assigned to a different track
                                                        if player_name in player_to_track_global:
                                                            old_track_id = player_to_track_global[player_name]
                                                            if old_track_id != best_recovery_track_id:
                                                                # Player is on a different track - clear it first
                                                                old_track_str = str(int(old_track_id))
                                                                if old_track_str in player_names:
                                                                    player_names[old_track_str] = ""  # Clear old track
                                                                if old_track_id in track_name_confidence:
                                                                    del track_name_confidence[old_track_id]
                                                                if current_frame_num % 50 == 0:
                                                                    print(f"  ðŸ—‘ï¸ UNIQUENESS (recovery soft): Cleared '{player_name}' from Track #{old_track_id} (spatial recovery to Track #{best_recovery_track_id})")
                                                        
                                                        player_names[recovery_track_str] = player_name
                                                        if best_recovery_track_id not in track_name_confidence:
                                                            track_name_confidence[best_recovery_track_id] = (player_name, 0.80, current_frame_num)  # Lower confidence in decay zone
                                                        # CRITICAL FIX: Ensure player_names is synced (defensive - already set above, but ensures consistency)
                                                        player_names[recovery_track_str] = player_name
                                                        player_to_track_global[player_name] = best_recovery_track_id
                                                        if current_frame_num % 50 == 0:
                                                            print(f"  ðŸ”„ SPATIAL RECOVERY (soft): Frame {current_frame_num}, recovered '{player_name}' â†’ Track #{best_recovery_track_id} (distance: {best_recovery_distance:.1f}px)")
                                        
                                        # Only check first matching protection window (most recent)
                                        break
                        
                        # PLAYER GALLERY: Match detected players against gallery for cross-video identification
                        # DEBUG: Check conditions before attempting match
                        if frame_data.get('frame_num', 0) % 100 == 0:
                            num_detections = len(detections) if detections is not None else 0
                            gallery_available = player_gallery is not None
                            reid_available = reid_features is not None
                            # Clarify message: reid_features is None when there are no detections (expected behavior)
                            if num_detections == 0:
                                # Note: Active tracks exist from previous frames, but we can't easily count them here
                                # The progress line shows total tracks, which includes active tracks from previous frames
                                print(f"ðŸ” Frame {frame_data.get('frame_num', 'unknown')}: Gallery match check - gallery={gallery_available}, detections=0 (no Re-ID features to extract - see progress line for active track count)")
                            else:
                                # PERFORMANCE: Only log gallery match check occasionally (was every frame)
                                if frame_data.get('frame_num', 0) % 500 == 0:
                                    print(f"ðŸ” Frame {frame_data.get('frame_num', 'unknown')}: Gallery match check - gallery={gallery_available}, reid_features={reid_available}, detections={num_detections}")
                        
                        # Force gallery matching if seed_frame_interval is set and this is a seed frame
                        # CRITICAL: Only force gallery updates for high-confidence tracks
                        # This prevents wrong players from messing up gallery references while learning from consistently identified players
                        should_force_gallery_match = False
                        should_force_gallery_learning_only = False  # Only update gallery, don't assign new names
                        
                        if 'force_gallery_mapping' in locals() and force_gallery_mapping:
                            # Find tracks eligible for gallery learning:
                            # 1. Anchor frames (1.0 confidence) - ground truth
                            # 2. Confirmed identity tracks (>= 0.75 similarity, confirmed 2+ times) - consistently identified
                            # 3. High-confidence stable tracks (>= 0.80 confidence) - very high confidence
                            eligible_tracks = []
                            
                            # Check track_name_confidence for high-confidence tracks
                            if 'track_name_confidence' in locals():
                                for tid, (pname, conf, frame_num) in track_name_confidence.items():
                                    if conf >= 1.00:
                                        # Anchor frame - always eligible
                                        eligible_tracks.append((tid, pname, conf, "anchor"))
                                    elif conf >= 0.80:
                                        # Very high confidence - eligible
                                        eligible_tracks.append((tid, pname, conf, "high_conf"))
                            
                            # Check track_confirmed_identity for consistently identified tracks
                            if 'track_confirmed_identity' in locals():
                                for tid, (pname, lock_frame, lock_conf, match_count) in track_confirmed_identity.items():
                                    if lock_conf >= 0.75 and match_count >= 2:
                                        # Consistently identified with high confidence - eligible
                                        # Only add if not already in eligible_tracks (avoid duplicates)
                                        if tid not in [t[0] for t in eligible_tracks]:
                                            eligible_tracks.append((tid, pname, lock_conf, "confirmed"))
                            
                            if eligible_tracks:
                                # Force gallery learning for eligible tracks
                                should_force_gallery_learning_only = True
                                if current_frame_num % 100 == 0:
                                    anchor_count = sum(1 for t in eligible_tracks if t[3] == "anchor")
                                    confirmed_count = sum(1 for t in eligible_tracks if t[3] == "confirmed")
                                    high_conf_count = sum(1 for t in eligible_tracks if t[3] == "high_conf")
                                    print(f"  ðŸ”„ SEED FRAME: Forcing gallery learning for {len(eligible_tracks)} eligible track(s): "
                                          f"{anchor_count} anchor(s), {confirmed_count} confirmed, {high_conf_count} high-confidence")
                            else:
                                # No eligible tracks - skip seed frame forcing
                                if current_frame_num % 100 == 0:
                                    print(f"  â„¹ SEED FRAME: Skipped (no eligible tracks: need 1.0 confidence, >=0.80 confidence, or confirmed identity)")
                        
                        # Perform gallery matching if:
                        # 1. Normal conditions: player_gallery, reid_features, and detections exist
                        # 2. OR forced learning: seed_frame_interval is active and we have 1.0 confidence tracks
                        if (player_gallery is not None and reid_features is not None and len(detections) > 0) or \
                           (should_force_gallery_learning_only and player_gallery is not None and len(detections) > 0):
                            try:
                                # DEBUG: Log gallery matching attempt
                                if frame_data.get('frame_num', 0) % 100 == 0:  # Log every 100 frames to avoid spam
                                    focus_info = f" (focus mode: {len(focused_players_set)} players)" if focused_players_set else ""
                                    # PERFORMANCE: Only log gallery matching attempt occasionally (was every frame)
                                    if frame_data.get('frame_num', 0) % 500 == 0:
                                        print(f"ðŸ” Frame {frame_data.get('frame_num', 'unknown')}: Attempting gallery matching for {len(detections)} detections{focus_info}...")
                                
                                # TEAM COLOR FILTERING: Extract team assignments and dominant colors for each detection
                                # This will filter out cross-team matches and boost same-team matches
                                # OPTIMIZATION: Skip team classification in watch-only mode (expensive K-means, not needed for learning)
                                detection_teams = []  # List of team names for each detection
                                detection_dominant_colors = []  # List of HSV dominant colors for each detection
                                uniform_info_list = []  # List of uniform info dicts for each detection
                                
                                # Get full frame for uniform extraction (if ROI cropping is used)
                                frame_for_uniform = batch_frame
                                if roi_bounds is not None:
                                    full_frame = frame_data.get('full_frame', None)
                                    if full_frame is not None:
                                        frame_for_uniform = full_frame
                                
                                if watch_only:
                                    # In watch-only mode: skip team classification (expensive, not needed for learning)
                                    # Just use None for all detections - gallery matching will work without team filtering
                                    detection_teams = [None] * len(detections)
                                    detection_dominant_colors = [None] * len(detections)
                                    # Still extract uniform info for uniform-based matching
                                    for i in range(len(detections)):
                                        bbox = detections.xyxy[i]
                                        uniform_info = extract_uniform_colors(frame_for_uniform, bbox)
                                        uniform_info_list.append(uniform_info if uniform_info else {})
                                else:
                                    # PERFORMANCE: Parallelize team classification and uniform extraction
                                    # Normal mode: classify teams for filtering
                                    # Use ThreadPoolExecutor to parallelize CPU-bound operations (OpenCV releases GIL)
                                    num_detections = len(detections)
                                    if num_detections > 0:
                                        # Create a thread pool for parallel team classification (only if we have multiple detections)
                                        # For small batches, overhead isn't worth it
                                        use_parallel = num_detections >= 3 and cpu_ops_executor is not None
                                        
                                        if use_parallel:
                                            # Parallel processing for multiple detections
                                            team_futures = []
                                            uniform_futures = []
                                            for i in range(num_detections):
                                                bbox = detections.xyxy[i]
                                                # Submit team classification task
                                                team_futures.append(cpu_ops_executor.submit(
                                                    classify_player_team, batch_frame.copy(), bbox, team_colors, False, False
                                                ))
                                                # Submit uniform extraction task
                                                uniform_futures.append(cpu_ops_executor.submit(
                                                    extract_uniform_colors, frame_for_uniform.copy(), bbox
                                                ))
                                            
                                            # Collect results
                                            for i in range(num_detections):
                                                try:
                                                    team = team_futures[i].result(timeout=0.5)
                                                    detection_teams.append(team)
                                                except:
                                                    detection_teams.append(None)
                                                
                                                try:
                                                    uniform_info = uniform_futures[i].result(timeout=0.5)
                                                    uniform_info_list.append(uniform_info if uniform_info else {})
                                                except:
                                                    uniform_info_list.append({})
                                                
                                                # Extract dominant color in HSV (for color similarity boost)
                                                dominant_color_hsv = None
                                                if reid_color_features is not None and 'team_color' in reid_color_features:
                                                    # Get BGR color from color features
                                                    bgr_color = reid_color_features['team_color'][i]
                                                    if np.any(bgr_color > 0):  # Valid color
                                                        # Convert BGR to HSV
                                                        bgr_array = np.array([[bgr_color]], dtype=np.uint8)
                                                        hsv_array = cv2.cvtColor(bgr_array, cv2.COLOR_BGR2HSV)
                                                        dominant_color_hsv = hsv_array[0, 0].astype(np.float32)
                                                
                                                detection_dominant_colors.append(dominant_color_hsv)
                                        else:
                                            # Sequential processing for small batches (lower overhead)
                                            for i in range(num_detections):
                                                # Classify team for this detection
                                                bbox = detections.xyxy[i]
                                                # Disable verbose debug - console I/O is very slow on Windows
                                                debug_team = False  # Set to True only for troubleshooting
                                                team = classify_player_team(batch_frame, bbox, team_colors, debug=debug_team)
                                                detection_teams.append(team)
                                                # Debug: Log team classification failures
                                                if debug_team and team is None:
                                                    print(f"   âš  Team classification failed for Detection #0 (bbox: {bbox}) - check team_color_config.json and HSV ranges")
                                                
                                                # Extract dominant color in HSV (for color similarity boost)
                                                dominant_color_hsv = None
                                                if reid_color_features is not None and 'team_color' in reid_color_features:
                                                    # Get BGR color from color features
                                                    bgr_color = reid_color_features['team_color'][i]
                                                    if np.any(bgr_color > 0):  # Valid color
                                                        # Convert BGR to HSV
                                                        bgr_array = np.array([[bgr_color]], dtype=np.uint8)
                                                        hsv_array = cv2.cvtColor(bgr_array, cv2.COLOR_BGR2HSV)
                                                        dominant_color_hsv = hsv_array[0, 0].astype(np.float32)
                                                
                                                detection_dominant_colors.append(dominant_color_hsv)
                                                
                                                # ðŸ›¡ï¸ DRIFT PREVENTION: Store jersey color signature for this detection
                                                # This will be linked to track_id after tracker assigns IDs
                                                # (We'll update track_jersey_signatures after tracker update)
                                                
                                                # Extract uniform colors (jersey, shorts, socks) for uniform-based matching
                                                uniform_info = extract_uniform_colors(frame_for_uniform, bbox)
                                                uniform_info_list.append(uniform_info if uniform_info else {})
                                
                                # Convert to appropriate formats for match_against_gallery
                                # teams: List of strings (team names)
                                teams_list = detection_teams if detection_teams else None
                                
                                # dominant_colors: Convert list of HSV arrays to numpy array (N, 3)
                                # Handle None values by creating zero arrays
                                dominant_colors_array = None
                                if detection_dominant_colors:
                                    # Convert list of arrays (some may be None) to numpy array
                                    color_list = []
                                    for color in detection_dominant_colors:
                                        if color is not None:
                                            color_list.append(color)
                                        else:
                                            color_list.append(np.array([0.0, 0.0, 0.0], dtype=np.float32))
                                    if color_list:
                                        dominant_colors_array = np.array(color_list)
                                
                                # Match each detection against the gallery with team color filtering
                                # PERFORMANCE: Reduced team filtering log frequency (was every 100 frames)
                                if frame_data.get('frame_num', 0) % 1000 == 0:  # Reduced from 100 to 1000 frames
                                    teams_detected = [t for t in detection_teams if t is not None]
                                    colors_detected = sum(1 for c in detection_dominant_colors if c is not None)
                                    if teams_detected:
                                        unique_teams = set(teams_detected)
                                        print(f"   ðŸŽ¨ Team color filtering: {len(teams_detected)}/{len(detections)} detections have team assignments ({', '.join(unique_teams)}), {colors_detected} have color data")
                                    else:
                                        print(f"   âš  No team assignments detected for any detections (team classification may be failing)")
                                
                                # Use GUI-specified Re-ID similarity threshold (default: 0.5, user can adjust)
                                # For cross-video gallery matching, use a LOWER threshold to allow more matches
                                # Gallery matching can be more lenient since it's cross-video identification
                                # Lower threshold allows players to be recognized even with slight appearance changes
                                # Gallery similarity threshold: Use GUI-specified threshold (default: 0.40, adjustable 0.25-0.75)
                                # This threshold is for cross-video player matching and should be lower than Re-ID threshold
                                # because appearance can vary between videos (different lighting, angles, uniforms)
                                # The GUI allows users to adjust this based on their similarity score distribution
                                # Most similarities are 0.35-0.50, so default 0.40 allows legitimate matches while preventing false positives
                                gallery_similarity_threshold = gallery_similarity_threshold  # Use GUI parameter (no hardcoding)
                                
                                # JERSEY NUMBER SEARCH: Get jersey numbers for tracks (if previously assigned)
                                # This allows jersey numbers to be used as a search criterion
                                # Build reverse mapping: track_id -> jersey_number (from global jersey mapping)
                                track_to_jersey_for_search = {}
                                if 'jersey_to_track_global' in locals():
                                    for jersey, track_id in jersey_to_track_global.items():
                                        track_to_jersey_for_search[track_id] = jersey
                                
                                jersey_numbers_list = []
                                # SAFETY CHECK: tracker_id might be None if tracking hasn't assigned IDs yet
                                if detections.tracker_id is not None and len(detections.tracker_id) > 0:
                                    for track_id in detections.tracker_id:
                                        if track_id is not None:
                                            track_id_int = int(track_id)
                                            # Check if this track previously had a jersey assigned
                                            if track_id_int in track_to_jersey_for_search:
                                                jersey_numbers_list.append(str(track_to_jersey_for_search[track_id_int]))
                                            else:
                                                jersey_numbers_list.append(None)
                                        else:
                                            jersey_numbers_list.append(None)
                                else:
                                    # No tracker IDs yet - create list of None values
                                    jersey_numbers_list = [None] * len(detections) if detections else []
                                
                                # BREADCRUMBS: Get track IDs for breadcrumb matching (if available)
                                track_ids_for_breadcrumbs = None
                                if detections.tracker_id is not None and len(detections.tracker_id) > 0:
                                    track_ids_for_breadcrumbs = [int(tid) if tid is not None else None for tid in detections.tracker_id]
                                
                                # DIAGNOSTIC: Check Re-ID features before matching
                                current_frame_num = frame_data.get('frame_num', 0)
                                if current_frame_num % 500 == 0 and reid_features is not None:
                                    valid_count = 0
                                    for f in reid_features:
                                        if f is not None and isinstance(f, np.ndarray) and not np.isnan(f).any():
                                            valid_count += 1
                                    feature_shapes = [f.shape if isinstance(f, np.ndarray) else "not array" for f in reid_features[:3]]
                                    print(f"   ðŸ” DIAGNOSTIC (Frame {current_frame_num}): Re-ID features - {valid_count}/{len(reid_features)} valid, shapes: {feature_shapes}")
                                
                                # Only perform gallery matching if we have Re-ID features (or forced seed frame with features extracted)
                                if reid_features is not None and len(reid_features) > 0:
                                    gallery_matches = reid_tracker.match_against_gallery(
                                        features=reid_features,
                                        gallery=player_gallery,
                                        dominant_colors=dominant_colors_array,
                                        teams=teams_list,
                                        jersey_numbers=jersey_numbers_list if jersey_numbers_list else None,  # Pass jersey numbers for search/boost
                                        similarity_threshold=gallery_similarity_threshold,  # Use GUI parameter
                                        current_frame_num=frame_data.get('frame_num', None),  # Only boost if detection is also from early frames
                                        track_ids=track_ids_for_breadcrumbs,  # BREADCRUMBS: Pass track IDs for breadcrumb matching
                                        uniform_info_list=uniform_info_list if uniform_info_list else None,  # UNIFORM VARIANTS: Pass uniform info for uniform-based matching
                                        exclude_players=players_in_anchor_frames_set,  # EXCLUDE anchor players from matching (they're already identified)
                                        # OPTIMIZATION: Only match against active/focused players for faster processing
                                        include_only_players=include_only_players_set  # Match against active players only (faster Re-ID)
                                        )
                                    
                                    # SEED FRAME: Log when forced gallery learning is active
                                    if should_force_gallery_learning_only:
                                        matches_count = sum(1 for m in gallery_matches if m[0] is not None)
                                        if matches_count > 0:
                                            print(f"  âœ“ Seed frame gallery learning: {matches_count}/{len(gallery_matches)} detections matched (for gallery updates only, no new assignments)")
                                        else:
                                            print(f"  â„¹ Seed frame gallery learning: No matches found (similarity threshold: {gallery_similarity_threshold:.2f})")
                                else:
                                    # No Re-ID features available - create empty matches
                                    gallery_matches = [(None, None, 0.0)] * len(detections)
                                    if should_force_gallery_learning_only:
                                        print(f"  âš  Seed frame gallery learning skipped: No Re-ID features available")
                                
                                # Store matches temporarily - we'll apply them after tracking assigns IDs
                                # Track IDs don't exist yet at this point in the pipeline
                                gallery_match_cache = {}  # detection_index -> (player_name, jersey_number, team, similarity)
                                
                                # Get ALL potential matches for each detection (for jersey constraint logic)
                                # We'll store top 5 matches per detection in case #1 choice has jersey conflict
                                all_matches_per_detection = {}  # detection_index -> [(player_name, jersey, similarity), ...]
                                
                                matches_found = 0
                                best_similarities = []  # Track best similarity scores for diagnostics
                                all_match_details = []  # Store all (player_name, similarity) for debugging
                                
                                # DIAGNOSTIC: Check if gallery_matches is empty or all None
                                current_frame_num = frame_data.get('frame_num', 0)
                                if len(gallery_matches) == 0:
                                    print(f"   âš  DIAGNOSTIC (Frame {current_frame_num}): gallery_matches is empty (no matches returned from match_against_gallery)")
                                elif all(m[0] is None for m in gallery_matches):
                                    # Only print this diagnostic occasionally to avoid spam
                                    if current_frame_num % 500 == 0:
                                        print(f"   âš  DIAGNOSTIC (Frame {current_frame_num}): All gallery_matches are None (all detections returned (None, None, 0.0))")
                                        # Check if gallery players have features
                                        if player_gallery:
                                            stats = player_gallery.get_stats()
                                            players_with_features = sum(1 for p in player_gallery.players.values() if p.features is not None)
                                            print(f"      â†’ Gallery has {stats.get('total_players', 0)} players, {players_with_features} with features")
                                        
                                        # Check Re-ID features
                                        if reid_features is not None:
                                            valid_count = sum(1 for f in reid_features if f is not None and isinstance(f, np.ndarray) and f.size > 0 and not (np.isnan(f).any() if isinstance(f, np.ndarray) else False))
                                            print(f"      â†’ Re-ID features: {valid_count}/{len(reid_features)} valid")
                                            if len(reid_features) > 0:
                                                first_feature = reid_features[0]
                                                if isinstance(first_feature, np.ndarray):
                                                    print(f"      â†’ First feature shape: {first_feature.shape}, norm: {np.linalg.norm(first_feature):.4f}")
                                                else:
                                                    print(f"      â†’ First feature type: {type(first_feature)}")
                                        
                                        # Check gallery features for first player
                                        if player_gallery and len(player_gallery.players) > 0:
                                            first_player = list(player_gallery.players.values())[0]
                                            if first_player.features is not None:
                                                gallery_feat = np.array(first_player.features)
                                                print(f"      â†’ First gallery player ({first_player.name}) features shape: {gallery_feat.shape}, norm: {np.linalg.norm(gallery_feat):.4f}")
                                            else:
                                                print(f"      â†’ First gallery player ({first_player.name}) has no features")
                                
                                for i, (player_id, player_name, similarity) in enumerate(gallery_matches):
                                    # Track all similarity scores and details (skip NaN)
                                    if not np.isnan(similarity) and similarity > 0:
                                        best_similarities.append(similarity)
                                        all_match_details.append((player_id or "unknown", similarity))
                                    
                                    # Get jersey number and team for this player from gallery
                                    jersey_number = None
                                    player_team = None
                                    if player_id is not None:
                                        player_profile = player_gallery.get_player(player_id)
                                        if player_profile:
                                            if player_profile.jersey_number:
                                                jersey_number = player_profile.jersey_number
                                            if player_profile.team:
                                                player_team = player_profile.team
                                    
                                    if player_name is not None and not np.isnan(similarity):
                                        # CRITICAL FIX: Store matches even if below threshold (for fallback matching)
                                        # We'll still check threshold when applying, but having them available helps
                                        # FOCUS MODE: Only store matches for focused players
                                        if focused_players_set is None or player_name in focused_players_set:
                                            # Store this match with jersey number and team
                                            # Store even if below threshold - we'll use best available match
                                            gallery_match_cache[i] = (player_name, jersey_number, player_team, similarity)
                                            if similarity >= gallery_similarity_threshold:
                                                matches_found += 1
                                                # DEBUG: Log if same player is matching to multiple detections (potential issue)
                                                if frame_data.get('frame_num', 0) % 100 == 0 and player_name == "Jax Derryberry":
                                                    print(f"  ðŸ” DEBUG: Detection #{i} matched to '{player_name}' (similarity: {similarity:.3f}, threshold: {gallery_similarity_threshold:.3f})")
                                        else:
                                            # Skip this match - player not in focus list
                                            if frame_data.get('frame_num', 0) % 500 == 0:  # Log occasionally
                                                pass  # Silently skip non-focused players
                                        
                                        # Also get ALL potential matches for this detection for jersey conflict resolution
                                        # Get top 5 matches from gallery (with team color filtering)
                                        # OPTIMIZATION: Only get alternative matches if we don't already have a good match stored
                                        # This avoids redundant matching and diagnostic spam
                                        needs_alternative_matches = (
                                            i not in all_matches_per_detection or
                                            (i in gallery_match_cache and gallery_match_cache[i][3] < gallery_similarity_threshold)
                                        )
                                        if needs_alternative_matches and i not in all_matches_per_detection:
                                            detection_feature = reid_features[i]
                                            detection_team = detection_teams[i] if i < len(detection_teams) else None
                                            # Get dominant color for this detection (single HSV array, not list)
                                            detection_color = None
                                            if i < len(detection_dominant_colors):
                                                detection_color = detection_dominant_colors[i]
                                            # Get jersey number for this detection (if track previously had one)
                                            detection_jersey = None
                                            if (detections.tracker_id is not None and 
                                                i < len(detections.tracker_id) and 
                                                detections.tracker_id[i] is not None):
                                                track_id_int = int(detections.tracker_id[i])
                                                if track_id_int in track_to_jersey_for_search:
                                                    detection_jersey = str(track_to_jersey_for_search[track_id_int])
                                            
                                            # ADAPTIVE THRESHOLD: 30 seconds worth of frames (adjusts for fps)
                                            # At 30fps: 30s * 30fps = 900 frames
                                            # At 120fps: 30s * 120fps = 3600 frames
                                            early_frame_threshold_adaptive = int(30 * fps) if fps > 0 else 1000
                                            # ===== ADVANCED RECOGNITION: Try graph-based matching first =====
                                            graph_match = None
                                            # CRITICAL FIX: Use 'i' (loop variable) instead of 'detection_idx' which is not defined in this scope
                                            if graph_tracker is not None and i < len(reid_features) and not np.isnan(reid_features[i]).any():
                                                try:
                                                    detection_feature = reid_features[i]
                                                    x1, y1, x2, y2 = detections.xyxy[i]
                                                    center_x = (x1 + x2) / 2.0
                                                    center_y = (y1 + y2) / 2.0
                                                    
                                                    # Get jersey number and team if available
                                                    detected_jersey = jersey_numbers.get(track_id_int) if track_id_int in jersey_numbers else None
                                                    detection_team = detection_teams[i] if i < len(detection_teams) else None
                                                    
                                                    # Try graph-based matching
                                                    graph_matches = graph_tracker.find_matching_players(
                                                        features=detection_feature,
                                                        jersey_number=detected_jersey,
                                                        team=detection_team,
                                                        position=(center_x, center_y),
                                                        field_size=(width, height),
                                                        similarity_threshold=gallery_similarity_threshold
                                                    )
                                                    
                                                    if graph_matches:
                                                        best_graph_match = graph_matches[0]
                                                        graph_node_id, graph_similarity = best_graph_match
                                                        graph_node = graph_tracker.nodes[graph_node_id]
                                                        
                                                        # If graph node has a player_id, use it
                                                        if graph_node.player_id:
                                                            graph_match = (
                                                                graph_node.player_id,
                                                                graph_node.jersey_number,
                                                                graph_node.team,
                                                                graph_similarity
                                                            )
                                                except Exception as e:
                                                    if current_frame_num % 1000 == 0:
                                                        print(f"  âš  Graph matching error: {e}")
                                            
                                            # Use graph match if available, otherwise fall back to gallery matching
                                            if graph_match and player_gallery:
                                                # Graph match found - use it
                                                player_id_from_graph, jersey_from_graph, team_from_graph, graph_sim = graph_match
                                                # Get player name from gallery
                                                if player_gallery.has_player(player_id_from_graph):
                                                    profile = player_gallery.get_player(player_id_from_graph)
                                                    all_candidate_matches = [(profile.name, jersey_from_graph, team_from_graph, graph_sim)]
                                                else:
                                                    # Graph match but no gallery entry - fall through to normal matching
                                                    # Get alternative matches with reasonable threshold (lower than primary to get more candidates, but still quality-filtered)
                                                    # Use 0.30 threshold to get viable alternatives while filtering out obviously bad matches
                                                    alternative_threshold = max(0.30, gallery_similarity_threshold - 0.20)  # At least 0.30, or 0.20 below primary threshold
                                                    all_candidate_matches = player_gallery.match_player(
                                                        features=detection_feature,
                                                        similarity_threshold=alternative_threshold,  # Quality threshold for alternative matches
                                                        dominant_color=detection_color,
                                                        team=detection_team,
                                                        jersey_number=detection_jersey,  # Use jersey number for search/boost
                                                        return_all=True,  # Return all similarities
                                                        early_frame_range=(0, early_frame_threshold_adaptive),  # Boost players tagged in first 30 seconds
                                                        early_frame_boost=0.10,  # 10% proportional boost for early-frame tags (only if similarity >= 0.5)
                                                        current_frame_num=frame_data.get('frame_num', None),  # Only boost if detection is also from early frames
                                                        filter_module=reid_tracker.filter_module if reid_tracker and hasattr(reid_tracker, 'filter_module') else None,  # Pass filter module for quality checks
                                                        suppress_diagnostics=True,  # Suppress diagnostic output for alternative matching
                                                        exclude_players=players_in_anchor_frames_set,  # EXCLUDE anchor players (they're already identified)
                                                        # CRITICAL: Set include_only_players to None to allow matching against ALL gallery players
                                                        # This enables Re-ID to identify untagged players (e.g., 5 players in video, 3 tagged, Re-ID finds the other 2)
                                                        # We exclude anchor-protected players to prevent overwriting their identities
                                                        include_only_players=include_only_players_set  # OPTIMIZATION: Only match against active/focused players
                                                    )
                                            else:
                                                # ENHANCED: Normal gallery matching with hard negative mining integration
                                                # Get alternative matches with reasonable threshold (lower than primary to get more candidates, but still quality-filtered)
                                                # Use 0.30 threshold to get viable alternatives while filtering out obviously bad matches
                                                alternative_threshold = max(0.30, gallery_similarity_threshold - 0.20)  # At least 0.30, or 0.20 below primary threshold
                                                all_candidate_matches = player_gallery.match_player(
                                                    features=detection_feature,
                                                    similarity_threshold=alternative_threshold,  # Quality threshold for alternative matches
                                                    dominant_color=detection_color,
                                                    team=detection_team,
                                                    jersey_number=detection_jersey,  # Use jersey number for search/boost
                                                    return_all=True,  # Return all similarities
                                                    early_frame_range=(0, early_frame_threshold_adaptive),  # Boost players tagged in first 30 seconds
                                                    early_frame_boost=0.10,  # 10% proportional boost for early-frame tags (only if similarity >= 0.5)
                                                    current_frame_num=frame_data.get('frame_num', None),  # Only boost if detection is also from early frames
                                                    hard_negative_miner=hard_negative_miner,  # Pass hard negative miner for similarity adjustment
                                                    track_id=track_id_int,  # Pass track ID for hard negative mining
                                                    filter_module=reid_tracker.filter_module if reid_tracker and hasattr(reid_tracker, 'filter_module') else None,  # Pass filter module for quality checks
                                                    suppress_diagnostics=True,  # Suppress diagnostic output for alternative matching
                                                    exclude_players=players_in_anchor_frames_set,  # EXCLUDE anchor players from matching
                                                    # CRITICAL: Set include_only_players to None to allow matching against ALL gallery players
                                                    # This enables Re-ID to identify untagged players (e.g., 5 players in video, 3 tagged, Re-ID finds the other 2)
                                                    # We exclude anchor-protected players to prevent overwriting their identities
                                                    include_only_players=None  # Allow matching against all gallery players (not just those in anchor frames)
                                                )
                                            
                                            # ===== ADVANCED RECOGNITION: Adjust similarity with hard negatives =====
                                            if hard_negative_miner is not None and all_candidate_matches and player_gallery:
                                                try:
                                                    # Adjust top match similarity using hard negatives
                                                    if len(all_candidate_matches) > 0:
                                                        top_match = all_candidate_matches[0]
                                                        if isinstance(top_match, (list, tuple)) and len(top_match) >= 4:
                                                            player_name_top, jersey_top, team_top, sim_top = top_match[:4]
                                                            player_id_top = player_name_top.lower().replace(' ', '_')
                                                            
                                                            # Get player feature from gallery
                                                            if player_gallery.has_player(player_id_top):
                                                                player_profile = player_gallery.get_player(player_id_top)
                                                                if player_profile.features is not None:
                                                                    player_feature = np.array(player_profile.features)
                                                                    
                                                                    # Adjust similarity
                                                                    adjusted_sim = hard_negative_miner.adjust_similarity_with_negatives(
                                                                        player_feature,
                                                                        detection_feature,
                                                                        player_id_top,
                                                                        sim_top
                                                                    )
                                                                    
                                                                    # Update match with adjusted similarity
                                                                    all_candidate_matches[0] = (player_name_top, jersey_top, team_top, adjusted_sim)
                                                except Exception as e:
                                                    if current_frame_num % 1000 == 0:
                                                        print(f"  âš  Hard negative adjustment error: {e}")
                                            # Store top 5 with jersey numbers and teams
                                            top_matches = []
                                            if all_candidate_matches is not None and isinstance(all_candidate_matches, (list, tuple)):
                                                for match_item in all_candidate_matches[:5]:
                                                    if isinstance(match_item, (list, tuple)) and len(match_item) >= 3:
                                                        pid, pname, sim = match_item[0], match_item[1], match_item[2]
                                                        pprofile = player_gallery.get_player(pid)
                                                        pjersey = pprofile.jersey_number if pprofile and pprofile.jersey_number else None
                                                        pteam = pprofile.team if pprofile and pprofile.team else None
                                                        top_matches.append((pname, pjersey, pteam, sim))
                                            all_matches_per_detection[i] = top_matches
                                
                                # CRITICAL: Frame-level validation - prevent same player name appearing multiple times in same frame
                                # This is impossible (one person can't be in multiple places) and indicates false matches
                                current_frame_num = frame_data.get('frame_num', 0)
                                frame_player_matches = {}  # player_name -> [detection_indices]
                                
                                # First pass: collect all matches for this frame
                                for i, (player_id, player_name, similarity) in enumerate(gallery_matches):
                                    # CRITICAL FIX: Handle case where player_name might be a list
                                    if isinstance(player_name, list) and len(player_name) > 0:
                                        player_name = str(player_name[0])
                                    elif player_name:
                                        player_name = str(player_name)
                                    else:
                                        player_name = None
                                    
                                    if player_name and isinstance(player_name, str) and player_name.strip():
                                        if player_name not in frame_player_matches:
                                            frame_player_matches[player_name] = []
                                        frame_player_matches[player_name].append(i)
                                
                                # Second pass: reject duplicate matches (keep only highest similarity)
                                duplicate_rejections = 0
                                for player_name, detection_indices in frame_player_matches.items():
                                    if len(detection_indices) > 1:
                                        # Same player matched multiple times - keep only the best match
                                        # Sort by similarity (highest first)
                                        matches_with_sim = [(idx, gallery_matches[idx][2]) for idx in detection_indices]
                                        matches_with_sim.sort(key=lambda x: x[1], reverse=True)
                                        
                                        # Keep only the best match, reject others
                                        best_idx = matches_with_sim[0][0]
                                        for idx, _ in matches_with_sim[1:]:
                                            # Reject this match by setting to None
                                            gallery_matches[idx] = (None, None, 0.0)
                                            duplicate_rejections += 1
                                        
                                        if current_frame_num % 100 == 0:  # Log occasionally
                                            print(f"   âš  Frame {current_frame_num}: Rejected {len(matches_with_sim) - 1} duplicate match(es) for '{player_name}' (kept best: similarity={gallery_matches[best_idx][2]:.2f})")
                                
                                # PERFORMANCE: Reduced logging frequency to improve processing speed
                                # Detailed similarity scores are expensive to compute and print
                                if current_frame_num % 1000 == 0:  # Reduced from 200 to 1000 frames (much less frequent)
                                    total_matches_in_cache = len(gallery_match_cache)
                                    print(f"   ðŸ” Frame {current_frame_num}: Found {matches_found} gallery matches above threshold (out of {len(detections)} detections)")
                                    if duplicate_rejections > 0:
                                        print(f"      â†’ Rejected {duplicate_rejections} duplicate player match(es) in this frame")
                                    print(f"      â†’ Total matches in cache: {total_matches_in_cache} (includes below-threshold matches for fallback)")
                                    if player_gallery is None:
                                        print(f"      âš  WARNING: Player Gallery is None - gallery matching is disabled!")
                                    elif stats := player_gallery.get_stats():
                                        if stats['total_players'] == 0:
                                            print(f"      âš  WARNING: Player Gallery is empty ({stats['total_players']} players) - no matches possible!")
                                        else:
                                            print(f"      â„¹ Gallery has {stats['total_players']} players, threshold={gallery_similarity_threshold:.2f}")
                                    if best_similarities:
                                        max_sim = max(best_similarities)
                                        avg_sim = sum(best_similarities) / len(best_similarities)
                                        min_sim = min(best_similarities)
                                        print(f"      Similarity scores: max={max_sim:.3f}, avg={avg_sim:.3f}, min={min_sim:.3f} (threshold={gallery_similarity_threshold:.2f})")
                                        # Show top matches only (reduce spam)
                                        sorted_matches = sorted(all_match_details, key=lambda x: x[1], reverse=True)
                                        if len(sorted_matches) > 0:
                                            print(f"      Top matches: {', '.join([f'{name}({sim:.2f})' for name, sim in sorted_matches[:5]])}")
                                    else:
                                        print(f"      No similarity scores computed for {len(detections)} detections")
                                        # DIAGNOSTIC: Check why no similarities were computed
                                        if len(gallery_matches) > 0:
                                            # Check if all matches are None or below threshold
                                            all_none = all(m[0] is None for m in gallery_matches)
                                            all_below_threshold = all(m[2] < gallery_similarity_threshold for m in gallery_matches if m[0] is not None)
                                            if all_none:
                                                print(f"      â†’ DIAGNOSTIC: All matches returned (None, None, 0.0) - gallery players may not have features")
                                            elif all_below_threshold:
                                                max_sim_found = max((m[2] for m in gallery_matches if m[0] is not None), default=0.0)
                                                print(f"      â†’ DIAGNOSTIC: All similarities below threshold (max: {max_sim_found:.3f} < {gallery_similarity_threshold:.2f})")
                                    
                                    # PERFORMANCE: Disabled detailed similarity score printing (too expensive)
                                    # This was calling match_player() again just for logging, causing 2x computation
                                    # If needed for debugging, enable only when explicitly requested
                                    # DETAILED DIAGNOSTICS disabled for performance
                                
                            except Exception as e:
                                print(f"âš  Player Gallery matching failed at frame {frame_data.get('frame_num', 'unknown')}: {e}")
                                import traceback
                                traceback.print_exc()

                        # EXPANSION IOU: Apply motion prediction to detections before tracker update
                        # This helps all trackers (ByteTrack, OC-SORT, etc.) match fast-moving players better
                        # Expansion IOU does NOT require Re-ID - it works with motion prediction alone
                        # CRITICAL: Skip Expansion IOU for tracks in drift bypass mode (they have corrupted velocity estimates)
                        if use_expansion_iou and ADVANCED_TRACKING_UTILS_AVAILABLE and detections is not None and len(detections) > 0:
                            try:
                                # Expand detection boxes based on predicted motion from previous frame tracks
                                expanded_xyxy = detections.xyxy.copy()
                                for i in range(len(detections)):
                                    # CRITICAL: Skip Expansion IOU for tracks in drift bypass mode
                                    # These tracks have corrupted velocity estimates that cause drift
                                    if detections.tracker_id is not None and i < len(detections.tracker_id):
                                        track_id = detections.tracker_id[i]
                                        if track_id is not None:
                                            track_id_int = int(track_id)
                                            if track_id_int in track_drift_bypass and track_drift_bypass[track_id_int]:
                                                # Skip Expansion IOU for this track - use detection position directly
                                                continue  # Keep original bbox, don't expand
                                    
                                    x1, y1, x2, y2 = detections.xyxy[i]
                                    det_bbox = (float(x1), float(y1), float(x2), float(y2))
                                    
                                    # Calculate detection center
                                    det_cx = (x1 + x2) / 2.0
                                    det_cy = (y1 + y2) / 2.0
                                    det_w = x2 - x1
                                    det_h = y2 - y1
                                    
                                    # Find closest previous track by position (for velocity estimation)
                                    best_velocity = None
                                    min_distance = float('inf')
                                    
                                    for track_id, prev_track in previous_frame_tracks.items():
                                        prev_center = prev_track['center']
                                        prev_cx, prev_cy = prev_center
                                        
                                        # Calculate distance to previous track position
                                        distance = np.sqrt((det_cx - prev_cx)**2 + (det_cy - prev_cy)**2)
                                        
                                        # If close enough and we have velocity, use it
                                        if distance < min_distance and distance < 200:  # 200 pixel threshold
                                            if track_id in track_velocities:
                                                best_velocity = track_velocities[track_id]
                                                min_distance = distance
                                    
                                    # If we found a velocity, expand the detection box based on predicted motion
                                    # CRITICAL: Skip expansion if the track has severe drift (corrupted velocity)
                                    if best_velocity is not None:
                                        # Check if this velocity belongs to a severely drifting track
                                        # We can't use tracker_id here (not assigned yet), but we can check if the velocity
                                        # belongs to a track that has drifted recently by checking track_drift_count
                                        skip_expansion = False
                                        for track_id, prev_track in previous_frame_tracks.items():
                                            if track_id in track_velocities and track_velocities[track_id] == best_velocity:
                                                # This velocity belongs to this track - check if it's severely drifting
                                                if track_id in track_drift_count:
                                                    drift_count = track_drift_count[track_id]
                                                    if drift_count >= 20:  # Skip expansion for tracks with 20+ drifts
                                                        skip_expansion = True
                                                        if current_frame_num % 100 == 0:
                                                            print(f"  ðŸ”§ EXPANSION IOU SKIPPED: Track #{track_id} has drifted {drift_count} times - skipping velocity-based expansion")
                                                        break
                                        
                                        if not skip_expansion:
                                            vx, vy = best_velocity
                                            
                                            # Predict where detection should be based on motion
                                            motion_magnitude = np.sqrt(vx**2 + vy**2)
                                            expansion_factor = 0.1  # 10% expansion per unit of motion
                                            expansion = motion_magnitude * expansion_factor
                                            
                                            # Expand box based on motion
                                            expanded_w = det_w + expansion
                                            expanded_h = det_h + expansion
                                            
                                            # Update expanded box (centered on detection)
                                            expanded_xyxy[i][0] = det_cx - expanded_w / 2.0
                                            expanded_xyxy[i][1] = det_cy - expanded_h / 2.0
                                            expanded_xyxy[i][2] = det_cx + expanded_w / 2.0
                                            expanded_xyxy[i][3] = det_cy + expanded_h / 2.0
                                
                                # Update detections with expanded boxes
                                detections.xyxy = expanded_xyxy
                            except Exception as e:
                                # If Expansion IOU fails, continue with original detections
                                if current_frame_num % 500 == 0:
                                    print(f"âš  Expansion IOU application failed: {e}")

                        # Update tracker (must be sequential for tracking continuity)
                        # Handle both ByteTrack and OC-SORT APIs
                        try:
                            # CRITICAL: Before tracker update, check for tracks in drift bypass mode
                            # For these tracks, we want to ensure the tracker uses detection positions directly
                            # Since we can't directly control tracker internals, we'll rely on drift correction after tracker update
                            # But we can log which tracks are in bypass mode for debugging
                            if len(track_drift_bypass) > 0 and current_frame_num % 100 == 0:
                                bypassed_tracks = [tid for tid in track_drift_bypass.keys() if track_drift_bypass[tid]]
                                if bypassed_tracks:
                                    print(f"  ðŸ”§ Pre-tracker: {len(bypassed_tracks)} track(s) in drift bypass mode: {sorted(bypassed_tracks)}")
                            
                            # Reset OC-SORT error counter on successful frame (before tracker update)
                            if OCSORT_AVAILABLE and hasattr(tracker, '__class__') and tracker.__class__.__name__ == 'OCSortTracker':
                                if detections is not None and len(detections) > 0:
                                    # Successful frame with detections - reset error counter
                                    if ocsort_keyerror_count > 0:
                                        ocsort_keyerror_count = 0
                                        ocsort_last_keyerror_frame = -1
                            
                            # Safety check: ensure detections is valid before
                            # passing to tracker
                            if detections is None or len(detections) == 0:
                                # Empty detections - tracker can still update
                                # (to maintain lost tracks)
                                pass
                            elif hasattr(detections, 'xyxy') and (detections.xyxy is None or len(detections.xyxy) == 0):
                                # Invalid detections - skip tracker update
                                print(
                                    f"âš  Skipping tracker update at frame {
                                        frame_data['frame_num']}: invalid detections (xyxy is empty)")
                                detections = sv.Detections.empty()

                            # Store detections count before tracker update (for both OC-SORT and ByteTrack)
                            detections_before_tracker = len(detections) if detections is not None else 0
                            
                            # DIAGNOSTIC: Track which tracks were active before tracker update (for detecting track loss)
                            # CRITICAL: Check tracks from track_state (persistent across frames) not just current detections
                            # This catches tracks that were active in previous frames but not in current detections
                            active_tracks_before = set()
                            if detections is not None and detections.tracker_id is not None:
                                for tid in detections.tracker_id:
                                    if tid is not None:
                                        active_tracks_before.add(int(tid))
                            
                            # Also include tracks from track_state that were active recently (within last 10 frames)
                            # This helps detect tracks that just disappeared
                            if 'track_state' in locals():
                                for tid, state in track_state.items():
                                    if 'last_seen_frame' in state:
                                        frames_since_seen = current_frame_num - state['last_seen_frame']
                                        if frames_since_seen <= 10:  # Track was active within last 10 frames
                                            active_tracks_before.add(int(tid))
                            
                            # DIAGNOSTIC: For anchor-protected players, track their assigned tracks before update
                            anchor_tracks_before = {}
                            for player_name, assigned_tid in player_to_track_global.items():
                                if player_name in player_anchor_protection or assigned_tid in track_anchor_protection:
                                    anchor_tracks_before[player_name] = assigned_tid

                            if OCSORT_AVAILABLE and hasattr(
                                    tracker, '__class__') and tracker.__class__.__name__ == 'OCSortTracker':
                                # OC-SORT uses update() method
                                # OC-SORT can throw KeyError when accessing non-existent track IDs
                                # This happens when tracker internal state gets
                                # out of sync
                                
                                # Debug logging BEFORE tracker update for OC-SORT
                                if current_frame_num <= 10:
                                    conf_before = f"conf={detections.confidence.min():.3f}-{detections.confidence.max():.3f}" if detections is not None and detections.confidence is not None and len(detections.confidence) > 0 else "no conf"
                                    print(f"ðŸ” Frame {current_frame_num}: Before OC-SORT tracker.update(): {detections_before_tracker} detections, {conf_before}")
                                
                                try:
                                    # Update tracker (BoxMOT wrapper handles both update() and update_with_detections())
                                    if hasattr(tracker, 'update') and not isinstance(tracker, type):  # Check if it's a method, not a class
                                        # BoxMOT wrapper needs frame for appearance features
                                        if hasattr(tracker, '__class__') and 'BoxMOT' in tracker.__class__.__name__:
                                            detections = tracker.update(detections, batch_frame)
                                        else:
                                            detections = tracker.update(detections)
                                    else:
                                        detections = tracker.update(detections)
                                    
                                    # Debug logging AFTER tracker update for OC-SORT
                                    if current_frame_num <= 10:
                                        detections_after = len(detections) if detections is not None else 0
                                        tracker_ids_after = sum(1 for tid in (detections.tracker_id if detections is not None and detections.tracker_id is not None else []) if tid is not None) if detections is not None else 0
                                        print(f"ðŸ” Frame {current_frame_num}: After OC-SORT tracker.update(): {detections_after} detections, {tracker_ids_after} with tracker_id")
                                        if detections_before_tracker > 0 and tracker_ids_after == 0:
                                            print(f"  âš  WARNING: OC-SORT tracker received {detections_before_tracker} detections but assigned 0 tracker IDs!")
                                            if detections is not None and detections.confidence is not None and len(detections.confidence) > 0:
                                                print(f"     â†’ Confidence range: {detections.confidence.min():.3f} - {detections.confidence.max():.3f}")
                                            if detections is not None and detections.tracker_id is not None:
                                                none_count = sum(1 for tid in detections.tracker_id if tid is None)
                                                print(f"     â†’ Tracker returned {len(detections.tracker_id)} tracker_ids, {none_count} are None")
                                    
                                    # Track statistics for summary
                                    if 'csv_export_stats' in locals():
                                        csv_export_stats['tracker_stats']['frames_processed'] += 1
                                        if detections is not None and len(detections) > 0:
                                            csv_export_stats['tracker_stats']['frames_with_detections'] += 1
                                            csv_export_stats['tracker_stats']['total_detections'] += len(detections)
                                            if detections.tracker_id is not None:
                                                valid_ids = sum(1 for tid in detections.tracker_id if tid is not None)
                                                csv_export_stats['tracker_stats']['total_tracker_ids_assigned'] += valid_ids
                                                if valid_ids > 0:
                                                    csv_export_stats['tracker_stats']['frames_with_tracker_ids'] += 1
                                                elif len(detections) > 0:
                                                    csv_export_stats['tracker_stats']['frames_tracker_failed'] += 1
                                    
                                    # DIAGNOSTIC: Check if anchor-protected players lost their tracks (OC-SORT)
                                    active_tracks_after = set()
                                    if detections is not None and detections.tracker_id is not None:
                                        for tid in detections.tracker_id:
                                            if tid is not None:
                                                active_tracks_after.add(int(tid))
                                    
                                    # Check for lost anchor-protected tracks
                                    for player_name, assigned_tid in anchor_tracks_before.items():
                                        if assigned_tid not in active_tracks_after:
                                            # Track was lost - diagnose why
                                            was_in_before = assigned_tid in active_tracks_before
                                            detection_count = len(detections) if detections is not None else 0
                                            
                                            # Check if there are detections near where the player was
                                            last_bbox = None
                                            if assigned_tid in track_state and 'xyxy' in track_state[assigned_tid]:
                                                last_bbox = track_state[assigned_tid]['xyxy']
                                            
                                            nearby_detections = 0
                                            kalman_prediction_error = None
                                            max_iou_with_nearby = 0.0
                                            nearest_detection_distance = None
                                            if last_bbox is not None and detections is not None and len(detections) > 0:
                                                last_center_x = (last_bbox[0] + last_bbox[2]) / 2
                                                last_center_y = (last_bbox[1] + last_bbox[3]) / 2
                                                
                                                # Check if Kalman filter made a prediction and how far off it was
                                                if use_enhanced_kalman and ENHANCED_TRACKING_AVAILABLE and assigned_tid in enhanced_kalman_filters:
                                                    try:
                                                        kf = enhanced_kalman_filters[assigned_tid]
                                                        kalman_predicted = kf.predict(dt=1.0)
                                                        if kalman_predicted is not None and len(kalman_predicted) >= 2:
                                                            kalman_pred_x, kalman_pred_y = kalman_predicted[0], kalman_predicted[1]
                                                            kalman_error = np.sqrt((kalman_pred_x - last_center_x)**2 + (kalman_pred_y - last_center_y)**2)
                                                            kalman_prediction_error = kalman_error
                                                    except Exception as e:
                                                        if current_frame_num % 200 == 0:
                                                            print(f"  âš  Kalman prediction check failed for Track #{assigned_tid}: {e}")
                                                
                                                # Calculate IoU with nearby detections to see why matching failed
                                                for det_idx in range(len(detections)):
                                                    if detections.xyxy is not None and det_idx < len(detections.xyxy):
                                                        det_bbox = detections.xyxy[det_idx]
                                                        det_center_x = (det_bbox[0] + det_bbox[2]) / 2
                                                        det_center_y = (det_bbox[1] + det_bbox[3]) / 2
                                                        distance = np.sqrt((det_center_x - last_center_x)**2 + (det_center_y - last_center_y)**2)
                                                        if distance < 350:  # Within recovery distance
                                                            nearby_detections += 1
                                                            # Calculate IoU between last bbox and this detection
                                                            try:
                                                                # IoU calculation
                                                                x1_inter = max(last_bbox[0], det_bbox[0])
                                                                y1_inter = max(last_bbox[1], det_bbox[1])
                                                                x2_inter = min(last_bbox[2], det_bbox[2])
                                                                y2_inter = min(last_bbox[3], det_bbox[3])
                                                                
                                                                if x2_inter > x1_inter and y2_inter > y1_inter:
                                                                    inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)
                                                                    last_area = (last_bbox[2] - last_bbox[0]) * (last_bbox[3] - last_bbox[1])
                                                                    det_area = (det_bbox[2] - det_bbox[0]) * (det_bbox[3] - det_bbox[1])
                                                                    union_area = last_area + det_area - inter_area
                                                                    if union_area > 0:
                                                                        iou = inter_area / union_area
                                                                        max_iou_with_nearby = max(max_iou_with_nearby, iou)
                                                                
                                                                if nearest_detection_distance is None or distance < nearest_detection_distance:
                                                                    nearest_detection_distance = distance
                                                            except Exception:
                                                                pass  # IoU calculation failed, ignore
                                            
                                            # Log diagnostic info
                                            print(f"  ðŸ” TRACK LOSS DIAGNOSTIC: Frame {current_frame_num}, '{player_name}' lost Track #{assigned_tid}")
                                            print(f"     â†’ Was active before: {was_in_before}, Active after: False")
                                            print(f"     â†’ Detections in frame: {detection_count}, Nearby detections (<350px): {nearby_detections}")
                                            if detections is not None and detections.confidence is not None and len(detections.confidence) > 0:
                                                print(f"     â†’ Detection confidence range: {detections.confidence.min():.3f} - {detections.confidence.max():.3f} (track_thresh: {track_thresh:.3f})")
                                            # Use adjusted_match_thresh if available (actual value used by tracker), otherwise use match_thresh
                                            actual_match_thresh = adjusted_match_thresh if 'adjusted_match_thresh' in locals() else match_thresh
                                            print(f"     â†’ Match threshold: {actual_match_thresh:.2f} (GUI setting: {match_thresh:.2f}), Track buffer: {track_buffer_scaled} frames (~{track_buffer_scaled/fps:.1f}s)")
                                            if kalman_prediction_error is not None:
                                                print(f"     â†’ Kalman prediction error: {kalman_prediction_error:.1f}px from last known position")
                                                if kalman_prediction_error > 100:
                                                    print(f"        âš  LARGE KALMAN ERROR: Prediction is {kalman_prediction_error:.1f}px off - filter may need tuning")
                                            elif use_enhanced_kalman and ENHANCED_TRACKING_AVAILABLE:
                                                if assigned_tid not in enhanced_kalman_filters:
                                                    print(f"     â†’ Kalman filter: Not initialized for Track #{assigned_tid}")
                                                else:
                                                    print(f"     â†’ Kalman filter: Initialized but prediction error not calculated")
                                            if nearby_detections > 0:
                                                if nearest_detection_distance is not None:
                                                    print(f"     â†’ Nearest detection: {nearest_detection_distance:.1f}px away")
                                            if max_iou_with_nearby > 0:
                                                print(f"     â†’ Max IoU with nearby detection: {max_iou_with_nearby:.3f} (threshold: {actual_match_thresh:.2f})")
                                                if max_iou_with_nearby < actual_match_thresh:
                                                    print(f"        âš  IoU {max_iou_with_nearby:.3f} < threshold {actual_match_thresh:.2f} - bbox shapes may not overlap enough despite proximity")
                                                else:
                                                    print(f"        âœ“ IoU {max_iou_with_nearby:.3f} >= threshold {actual_match_thresh:.2f} - matching should have succeeded (tracker may use different threshold or other criteria)")
                                            print(f"     â†’ Possible causes:")
                                            if nearby_detections > 0:
                                                actual_match_thresh = adjusted_match_thresh if 'adjusted_match_thresh' in locals() else match_thresh
                                                if max_iou_with_nearby > 0:
                                                    if max_iou_with_nearby < actual_match_thresh:
                                                        print(f"        â€¢ Detection exists nearby but IoU ({max_iou_with_nearby:.3f}) < threshold ({actual_match_thresh:.2f}) - bbox shapes don't overlap enough")
                                                    else:
                                                        print(f"        â€¢ IoU ({max_iou_with_nearby:.3f}) >= threshold ({actual_match_thresh:.2f}) but tracker still failed - tracker may use stricter internal threshold or other matching criteria")
                                                else:
                                                    print(f"        â€¢ Detection exists nearby but IoU matching failed (match_thresh={actual_match_thresh:.2f} may be too strict, GUI setting: {match_thresh:.2f})")
                                            else:
                                                print(f"        â€¢ No detection nearby (YOLO may have missed the player, or player moved >350px)")
                                            if detections is not None and detections.confidence is not None and len(detections.confidence) > 0:
                                                if detections.confidence.min() < track_thresh:
                                                    print(f"        â€¢ Low detection confidence (min: {detections.confidence.min():.3f} < track_thresh: {track_thresh:.3f})")
                                            print(f"        â€¢ Track buffer expired (track not matched for {track_buffer_scaled} frames)")
                                            if kalman_prediction_error is not None and kalman_prediction_error > 50:
                                                print(f"        â€¢ âš  Kalman filter prediction error: {kalman_prediction_error:.1f}px (filter may need tuning or player made sudden direction change)")
                                            else:
                                                print(f"        â€¢ Motion prediction failure (Kalman filter predicted wrong position)")
                                    
                                    # Track statistics for summary
                                    if 'csv_export_stats' in locals():
                                        csv_export_stats['tracker_stats']['frames_processed'] += 1
                                        if detections is not None and len(detections) > 0:
                                            csv_export_stats['tracker_stats']['frames_with_detections'] += 1
                                            csv_export_stats['tracker_stats']['total_detections'] += len(detections)
                                            if detections.tracker_id is not None:
                                                valid_ids = sum(1 for tid in detections.tracker_id if tid is not None and tid != -1)
                                                csv_export_stats['tracker_stats']['total_tracker_ids_assigned'] += valid_ids
                                                if valid_ids > 0:
                                                    csv_export_stats['tracker_stats']['frames_with_tracker_ids'] += 1
                                                elif len(detections) > 0:
                                                    csv_export_stats['tracker_stats']['frames_tracker_failed'] += 1
                                    
                                    # ðŸ”— POST-TRACKER HIGH-IoU RECONNECTION: Force reconnection for lost tracks with very high IoU
                                    # This handles cases where tracker fails to reconnect despite excellent IoU (e.g., 0.978)
                                    # The tracker may use additional criteria beyond IoU, but we can force reconnection for obvious matches
                                    if len(detections) > 0 and detections.tracker_id is not None and 'track_state' in locals():
                                        for lost_track_id, track_state_data in track_state.items():
                                            # Check if this track was recently lost (within last 5 frames)
                                            if 'last_seen_frame' in track_state_data:
                                                frames_since_lost = current_frame_num - track_state_data['last_seen_frame']
                                                if 1 <= frames_since_lost <= 5:  # Recently lost (1-5 frames ago)
                                                    # Check if this track has a player name (anchor-protected or previously assigned)
                                                    lost_track_str = str(int(lost_track_id))
                                                    lost_player_name = player_names.get(lost_track_str, None)
                                                    
                                                    # Only force reconnection for named tracks (anchor-protected or previously identified)
                                                    if lost_player_name and lost_player_name and not lost_player_name.startswith("#"):
                                                        # Get last known bbox
                                                        last_bbox = track_state_data.get('xyxy', None)
                                                        if last_bbox is not None and len(last_bbox) >= 4:
                                                            last_center_x = (last_bbox[0] + last_bbox[2]) / 2
                                                            last_center_y = (last_bbox[1] + last_bbox[3]) / 2
                                                            
                                                            # Find detection with highest IoU
                                                            best_det_idx = None
                                                            best_iou = 0.0
                                                            best_distance = float('inf')
                                                            
                                                            for det_idx in range(len(detections)):
                                                                if detections.xyxy is not None and det_idx < len(detections.xyxy):
                                                                    det_bbox = detections.xyxy[det_idx]
                                                                    det_center_x = (det_bbox[0] + det_bbox[2]) / 2
                                                                    det_center_y = (det_bbox[1] + det_bbox[3]) / 2
                                                                    distance = np.sqrt((det_center_x - last_center_x)**2 + (det_center_y - last_center_y)**2)
                                                                    
                                                                    # Calculate IoU
                                                                    try:
                                                                        x1_inter = max(last_bbox[0], det_bbox[0])
                                                                        y1_inter = max(last_bbox[1], det_bbox[1])
                                                                        x2_inter = min(last_bbox[2], det_bbox[2])
                                                                        y2_inter = min(last_bbox[3], det_bbox[3])
                                                                        if x2_inter > x1_inter and y2_inter > y1_inter:
                                                                            inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)
                                                                            last_area = (last_bbox[2] - last_bbox[0]) * (last_bbox[3] - last_bbox[1])
                                                                            det_area = (det_bbox[2] - det_bbox[0]) * (det_bbox[3] - det_bbox[1])
                                                                            union_area = last_area + det_area - inter_area
                                                                            if union_area > 0:
                                                                                iou = inter_area / union_area
                                                                                # Prefer high IoU (>0.9) and close distance (<50px)
                                                                                if iou > 0.9 and distance < 50:
                                                                                    if iou > best_iou or (iou == best_iou and distance < best_distance):
                                                                                        best_iou = iou
                                                                                        best_distance = distance
                                                                                        best_det_idx = det_idx
                                                                    except:
                                                                        pass
                                                            
                                                            # Force reconnection if IoU is very high (>0.9)
                                                            if best_det_idx is not None and best_iou > 0.9:
                                                                new_track_id = detections.tracker_id[best_det_idx]
                                                                if new_track_id is not None:
                                                                    # Check if new track is unassigned or has low confidence
                                                                    new_track_str = str(int(new_track_id))
                                                                    new_track_name = player_names.get(new_track_str, "")
                                                                    new_track_has_high_conf = False
                                                                    if int(new_track_id) in track_name_confidence:
                                                                        _, new_track_conf, _ = track_name_confidence[int(new_track_id)]
                                                                        new_track_has_high_conf = new_track_conf >= 0.70
                                                                    
                                                                    # Only force reconnection if new track is unassigned or has low confidence
                                                                    if (not new_track_name or new_track_name.startswith("#") or not new_track_has_high_conf):
                                                                        # Force reconnection by assigning player name to new track
                                                                        player_names[new_track_str] = lost_player_name
                                                                        player_to_track_global[lost_player_name] = int(new_track_id)
                                                                        
                                                                        # Transfer anchor protection if applicable
                                                                        if lost_track_id in track_anchor_protection:
                                                                            prot_name, prot_start, prot_end = track_anchor_protection[lost_track_id]
                                                                            track_anchor_protection[int(new_track_id)] = (prot_name, prot_start, prot_end)
                                                                            del track_anchor_protection[lost_track_id]
                                                                        
                                                                        if lost_track_id in track_anchor_assigned:
                                                                            track_anchor_assigned[int(new_track_id)] = track_anchor_assigned[lost_track_id]
                                                                            del track_anchor_assigned[lost_track_id]
                                                                        
                                                                        # Set high confidence
                                                                        track_name_confidence[int(new_track_id)] = (lost_player_name, 1.00, current_frame_num)
                                                                        
                                                                        # Clear old track
                                                                        if lost_track_str in player_names:
                                                                            player_names[lost_track_str] = ""
                                                                        if lost_track_id in track_name_confidence:
                                                                            del track_name_confidence[lost_track_id]
                                                                        
                                                                        if current_frame_num % 10 == 0:
                                                                            print(f"  ðŸ”— FORCED RECONNECTION: '{lost_player_name}' Track #{lost_track_id} â†’ #{new_track_id} (IoU: {best_iou:.3f}, distance: {best_distance:.1f}px) - tracker failed but IoU is excellent")
                                    
                                    # ðŸš« POST-TRACKING BALL FILTER: Remove tracks that look like balls after tracking
                                    # Even with pre-filtering, some balls may get track IDs
                                    # Filter based on size, aspect ratio, and movement (stationary = likely ball)
                                    if len(detections) > 0 and detections.tracker_id is not None:
                                        ball_track_filter_mask = np.ones(len(detections), dtype=bool)
                                        
                                        for i in range(len(detections)):
                                            if detections.tracker_id[i] is None:
                                                continue
                                            
                                            track_id = int(detections.tracker_id[i])
                                            x1, y1, x2, y2 = detections.xyxy[i]
                                            bbox_width = x2 - x1
                                            bbox_height = y2 - y1
                                            bbox_area = bbox_width * bbox_height
                                            aspect_ratio = bbox_height / bbox_width if bbox_width > 0 else 0
                                            center_x = (x1 + x2) / 2
                                            center_y = (y1 + y2) / 2
                                            
                                            # Check movement history
                                            if track_id not in track_movement_history:
                                                track_movement_history[track_id] = []
                                            
                                            # Add current position to history
                                            track_movement_history[track_id].append((current_frame_num, center_x, center_y))
                                            # Keep only last 30 frames of history
                                            if len(track_movement_history[track_id]) > 30:
                                                track_movement_history[track_id] = track_movement_history[track_id][-30:]
                                            
                                            # Calculate movement distance
                                            movement_distance = 0.0
                                            if len(track_movement_history[track_id]) > 1:
                                                first_pos = track_movement_history[track_id][0]
                                                last_pos = track_movement_history[track_id][-1]
                                                movement_distance = np.sqrt((last_pos[1] - first_pos[1])**2 + (last_pos[2] - first_pos[2])**2)
                                            
                                            # BALL FILTERING CRITERIA (post-tracking):
                                            # 1. Small area (< 3000 pixels)
                                            # 2. Circular aspect ratio (0.8-1.2)
                                            # 3. Short height (< 80 pixels)
                                            # 4. Stationary (movement < 50 pixels over 30 frames)
                                            
                                            is_likely_ball = False
                                            
                                            # Criterion 1: Small and circular
                                            if bbox_area < 3000 and 0.8 <= aspect_ratio <= 1.2:
                                                is_likely_ball = True
                                            
                                            # Criterion 2: Small and short
                                            if bbox_area < 3000 and bbox_height < 80:
                                                is_likely_ball = True
                                            
                                            # Criterion 3: Stationary and small (ball doesn't move)
                                            if len(track_movement_history[track_id]) >= 10:  # Need at least 10 frames of history
                                                if movement_distance < 50 and bbox_area < 4000:
                                                    # Stationary and small = likely a ball
                                                    is_likely_ball = True
                                            
                                            # Criterion 4: Low aspect ratio (not tall enough to be a player)
                                            if aspect_ratio < 1.3 and (bbox_area < 4000 or bbox_height < 90):
                                                is_likely_ball = True
                                            
                                            if is_likely_ball:
                                                ball_track_filter_mask[i] = False
                                                if current_frame_num % 500 == 0:
                                                    print(f"ðŸš« POST-TRACKING: Filtered ball track #{track_id}: area={bbox_area:.0f}, height={bbox_height:.0f}, aspect={aspect_ratio:.2f}, movement={movement_distance:.1f}px")
                                        
                                        # Apply post-tracking filter
                                        if not np.all(ball_track_filter_mask):
                                            filtered_count = np.sum(~ball_track_filter_mask)
                                            if current_frame_num % 500 == 0 or current_frame_num <= 10:
                                                print(f"ðŸš« POST-TRACKING BALL FILTER: Removed {filtered_count} ball track(s) from {len(detections)} total tracks")
                                            
                                            if np.any(ball_track_filter_mask):
                                                detections = detections[ball_track_filter_mask]
                                            else:
                                                detections = sv.Detections.empty()
                                    
                                except KeyError as key_err:
                                    # OC-SORT internal state issue - track ID doesn't exist
                                    # This can happen when tracker state gets corrupted
                                    current_frame = frame_data['frame_num']
                                    
                                    # Track consecutive KeyErrors
                                    if current_frame == ocsort_last_keyerror_frame + 1:
                                        # Consecutive error - increment counter
                                        ocsort_keyerror_count += 1
                                    else:
                                        # Non-consecutive error - reset counter
                                        ocsort_keyerror_count = 1
                                    
                                    ocsort_last_keyerror_frame = current_frame
                                    
                                    # If too many consecutive errors, reinitialize tracker
                                    if ocsort_keyerror_count >= ocsort_keyerror_threshold:
                                        print(f"ðŸš¨ OC-SORT: {ocsort_keyerror_count} consecutive KeyErrors detected - reinitializing tracker to recover from corruption")
                                        try:
                                            # Reinitialize tracker with stored parameters
                                            if 'ocsort_tracker_params' in locals():
                                                params = ocsort_tracker_params
                                                tracker = OCSortTracker(
                                                    track_activation_threshold=params['activation_thresh'],
                                                    minimum_matching_threshold=params['adjusted_match_thresh'],
                                                    lost_track_buffer=params['track_buffer_scaled'],
                                                    min_track_length=params['min_track_length'],
                                                    max_age=params['track_buffer_scaled'] * 3,
                                                    iou_threshold=params['adjusted_match_thresh']
                                                )
                                            else:
                                                # Fallback: use current values if params not stored
                                                tracker = OCSortTracker(
                                                    track_activation_threshold=activation_thresh,
                                                    minimum_matching_threshold=adjusted_match_thresh,
                                                    lost_track_buffer=track_buffer_scaled,
                                                    min_track_length=min_track_length,
                                                    max_age=track_buffer_scaled * 3,
                                                    iou_threshold=adjusted_match_thresh
                                                )
                                            ocsort_keyerror_count = 0  # Reset counter
                                            ocsort_last_keyerror_frame = -1
                                            print(f"âœ“ OC-SORT tracker reinitialized successfully")
                                        except Exception as reinit_err:
                                            print(f"âš  Could not reinitialize OC-SORT tracker: {reinit_err}")
                                            # Continue with corrupted tracker (better than crashing)
                                    
                                    # Log error (less verbose if we're about to reinitialize)
                                    if ocsort_keyerror_count < ocsort_keyerror_threshold:
                                        print(f"âš  OC-SORT KeyError at frame {current_frame}: {key_err} - tracker state may be corrupted ({ocsort_keyerror_count}/{ocsort_keyerror_threshold} consecutive errors)")
                                    
                                    # Create empty detections to allow tracker to recover
                                    detections = sv.Detections.empty()
                                    # The tracker should recover on next successful frame (or after reinitialization)
                            else:
                                # ByteTrack uses update_with_detections() method
                                # BoxMOT wrapper uses update() with frame
                                
                                # Debug logging BEFORE tracker update for ByteTrack
                                if current_frame_num <= 10:
                                    conf_before = f"conf={detections.confidence.min():.3f}-{detections.confidence.max():.3f}" if detections is not None and detections.confidence is not None and len(detections.confidence) > 0 else "no conf"
                                    print(f"ðŸ” Frame {current_frame_num}: Before ByteTrack tracker.update(): {detections_before_tracker} detections, {conf_before}")
                                
                                if hasattr(tracker, '__class__') and 'BoxMOT' in tracker.__class__.__name__:
                                    detections = tracker.update(detections, batch_frame)
                                elif hasattr(tracker, 'update_with_detections'):
                                    detections = tracker.update_with_detections(detections)
                                else:
                                    detections = tracker.update(detections)
                            
                                # Debug logging AFTER tracker update for ByteTrack
                                if current_frame_num <= 10:
                                    detections_after_tracker = len(detections) if detections is not None else 0
                                    tracker_ids_after = sum(1 for tid in (detections.tracker_id if detections is not None and detections.tracker_id is not None else []) if tid is not None and tid != -1) if detections is not None else 0
                                    print(f"ðŸ” Frame {current_frame_num}: After ByteTrack tracker.update(): {detections_after_tracker} detections, {tracker_ids_after} with valid tracker_id (non--1)")
                                    # CRITICAL DEBUG: Show actual tracker_id values immediately after tracker update
                                    if detections is not None and detections.tracker_id is not None:
                                        actual_ids = [int(tid) if tid is not None else None for tid in detections.tracker_id]
                                        print(f"ðŸ” Frame {current_frame_num}: After ByteTrack - actual tracker_id values: {actual_ids}")
                                        # Check if all IDs are -1 (warmup period)
                                        if all(tid == -1 for tid in actual_ids if tid is not None):
                                            print(f"  âš  WARNING: ByteTrack returned all -1 IDs (warmup period or configuration issue)")
                                            print(f"     â†’ This is normal for first few frames, but should resolve by frame 5-10")
                                            print(f"     â†’ If this persists, ByteTrack may need lower thresholds or more lenient configuration")
                                            # Show detection confidence to help diagnose
                                            if detections.confidence is not None and len(detections.confidence) > 0:
                                                print(f"     â†’ Detection confidence range: {detections.confidence.min():.3f} - {detections.confidence.max():.3f}")
                                                print(f"     â†’ ByteTrack activation threshold: {activation_thresh if 'activation_thresh' in locals() else 'unknown'}")
                                    if detections_before_tracker > 0 and tracker_ids_after == 0:
                                        print(f"  âš  WARNING: ByteTrack tracker received {detections_before_tracker} detections but assigned 0 tracker IDs!")
                                        if detections is not None and detections.confidence is not None and len(detections.confidence) > 0:
                                            print(f"     â†’ Confidence range: {detections.confidence.min():.3f} - {detections.confidence.max():.3f}")
                                        if detections is not None and detections.tracker_id is not None:
                                            none_count = sum(1 for tid in detections.tracker_id if tid is None)
                                            print(f"     â†’ Tracker returned {len(detections.tracker_id)} tracker_ids, {none_count} are None")
                                
                                # DIAGNOSTIC: Check if anchor-protected players lost their tracks (ByteTrack)
                                active_tracks_after = set()
                                if detections is not None and detections.tracker_id is not None:
                                    for tid in detections.tracker_id:
                                        if tid is not None and tid != -1:
                                            active_tracks_after.add(int(tid))
                                
                                # Check for lost anchor-protected tracks
                                for player_name, assigned_tid in anchor_tracks_before.items():
                                    if assigned_tid not in active_tracks_after:
                                        # Track was lost - diagnose why
                                        was_in_before = assigned_tid in active_tracks_before
                                        detection_count = len(detections) if detections is not None else 0
                                        
                                        # Check if there are detections near where the player was
                                        last_bbox = None
                                        if assigned_tid in track_state and 'xyxy' in track_state[assigned_tid]:
                                            last_bbox = track_state[assigned_tid]['xyxy']
                                        
                                        nearby_detections = 0
                                        kalman_prediction_error = None
                                        max_iou_with_nearby = 0.0
                                        nearest_detection_distance = None
                                        if last_bbox is not None and detections is not None and len(detections) > 0:
                                            last_center_x = (last_bbox[0] + last_bbox[2]) / 2
                                            last_center_y = (last_bbox[1] + last_bbox[3]) / 2
                                            
                                            # Check if Kalman filter made a prediction and how far off it was
                                            if use_enhanced_kalman and ENHANCED_TRACKING_AVAILABLE and assigned_tid in enhanced_kalman_filters:
                                                try:
                                                    kf = enhanced_kalman_filters[assigned_tid]
                                                    kalman_predicted = kf.predict(dt=1.0)
                                                    if kalman_predicted is not None and len(kalman_predicted) >= 2:
                                                        kalman_pred_x, kalman_pred_y = kalman_predicted[0], kalman_predicted[1]
                                                        kalman_error = np.sqrt((kalman_pred_x - last_center_x)**2 + (kalman_pred_y - last_center_y)**2)
                                                        kalman_prediction_error = kalman_error
                                                except Exception as e:
                                                    if current_frame_num % 200 == 0:
                                                        print(f"  âš  Kalman prediction check failed for Track #{assigned_tid}: {e}")
                                            
                                            # Calculate IoU with nearby detections to see why matching failed
                                            for det_idx in range(len(detections)):
                                                if detections.xyxy is not None and det_idx < len(detections.xyxy):
                                                    det_bbox = detections.xyxy[det_idx]
                                                    det_center_x = (det_bbox[0] + det_bbox[2]) / 2
                                                    det_center_y = (det_bbox[1] + det_bbox[3]) / 2
                                                    distance = np.sqrt((det_center_x - last_center_x)**2 + (det_center_y - last_center_y)**2)
                                                    if distance < 350:  # Within recovery distance
                                                        nearby_detections += 1
                                                        # Calculate IoU between last bbox and this detection
                                                        try:
                                                            # IoU calculation
                                                            x1_inter = max(last_bbox[0], det_bbox[0])
                                                            y1_inter = max(last_bbox[1], det_bbox[1])
                                                            x2_inter = min(last_bbox[2], det_bbox[2])
                                                            y2_inter = min(last_bbox[3], det_bbox[3])
                                                            
                                                            if x2_inter > x1_inter and y2_inter > y1_inter:
                                                                inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)
                                                                last_area = (last_bbox[2] - last_bbox[0]) * (last_bbox[3] - last_bbox[1])
                                                                det_area = (det_bbox[2] - det_bbox[0]) * (det_bbox[3] - det_bbox[1])
                                                                union_area = last_area + det_area - inter_area
                                                                if union_area > 0:
                                                                    iou = inter_area / union_area
                                                                    max_iou_with_nearby = max(max_iou_with_nearby, iou)
                                                            
                                                            if nearest_detection_distance is None or distance < nearest_detection_distance:
                                                                nearest_detection_distance = distance
                                                        except Exception:
                                                            pass  # IoU calculation failed, ignore
                                        
                                        # Log diagnostic info
                                        print(f"  ðŸ” TRACK LOSS DIAGNOSTIC: Frame {current_frame_num}, '{player_name}' lost Track #{assigned_tid}")
                                        print(f"     â†’ Was active before: {was_in_before}, Active after: False")
                                        print(f"     â†’ Detections in frame: {detection_count}, Nearby detections (<350px): {nearby_detections}")
                                        if detections is not None and detections.confidence is not None and len(detections.confidence) > 0:
                                            print(f"     â†’ Detection confidence range: {detections.confidence.min():.3f} - {detections.confidence.max():.3f} (track_thresh: {track_thresh:.3f})")
                                        # Use adjusted_match_thresh if available (actual value used by tracker), otherwise use match_thresh
                                        actual_match_thresh = adjusted_match_thresh if 'adjusted_match_thresh' in locals() else match_thresh
                                        print(f"     â†’ Match threshold: {actual_match_thresh:.2f} (GUI setting: {match_thresh:.2f}), Track buffer: {track_buffer_scaled} frames (~{track_buffer_scaled/fps:.1f}s)")
                                        if kalman_prediction_error is not None:
                                            print(f"     â†’ Kalman prediction error: {kalman_prediction_error:.1f}px from last known position")
                                            if kalman_prediction_error > 100:
                                                print(f"        âš  LARGE KALMAN ERROR: Prediction is {kalman_prediction_error:.1f}px off - filter may need tuning")
                                        elif use_enhanced_kalman and ENHANCED_TRACKING_AVAILABLE:
                                            if assigned_tid not in enhanced_kalman_filters:
                                                print(f"     â†’ Kalman filter: Not initialized for Track #{assigned_tid}")
                                            else:
                                                print(f"     â†’ Kalman filter: Initialized but prediction error not calculated")
                                        if nearby_detections > 0:
                                            if nearest_detection_distance is not None:
                                                print(f"     â†’ Nearest detection: {nearest_detection_distance:.1f}px away")
                                            if max_iou_with_nearby > 0:
                                                print(f"     â†’ Max IoU with nearby detection: {max_iou_with_nearby:.3f} (threshold: {actual_match_thresh:.2f})")
                                                if max_iou_with_nearby < actual_match_thresh:
                                                    print(f"        âš  IoU {max_iou_with_nearby:.3f} < threshold {actual_match_thresh:.2f} - bbox shapes may not overlap enough despite proximity")
                                                else:
                                                    print(f"        âœ“ IoU {max_iou_with_nearby:.3f} >= threshold {actual_match_thresh:.2f} - matching should have succeeded (tracker may use different threshold or other criteria)")
                                        print(f"     â†’ Possible causes:")
                                        if nearby_detections > 0:
                                            if max_iou_with_nearby > 0:
                                                if max_iou_with_nearby < actual_match_thresh:
                                                    print(f"        â€¢ Detection exists nearby but IoU ({max_iou_with_nearby:.3f}) < threshold ({actual_match_thresh:.2f}) - bbox shapes don't overlap enough")
                                                else:
                                                    print(f"        â€¢ IoU ({max_iou_with_nearby:.3f}) >= threshold ({actual_match_thresh:.2f}) but tracker still failed - tracker may use stricter internal threshold or other matching criteria")
                                            else:
                                                print(f"        â€¢ Detection exists nearby but IoU matching failed (match_thresh={actual_match_thresh:.2f} may be too strict, GUI setting: {match_thresh:.2f})")
                                        else:
                                            print(f"        â€¢ No detection nearby (YOLO may have missed the player, or player moved >350px)")
                                        if detections is not None and detections.confidence is not None and len(detections.confidence) > 0:
                                            if detections.confidence.min() < track_thresh:
                                                print(f"        â€¢ Low detection confidence (min: {detections.confidence.min():.3f} < track_thresh: {track_thresh:.3f})")
                                        print(f"        â€¢ Track buffer expired (track not matched for {track_buffer_scaled} frames)")
                                        print(f"        â€¢ Motion prediction failure (Kalman filter predicted wrong position)")
                                
                                # Track statistics for summary
                                if 'csv_export_stats' in locals():
                                    csv_export_stats['tracker_stats']['frames_processed'] += 1
                                    if detections is not None and len(detections) > 0:
                                        csv_export_stats['tracker_stats']['frames_with_detections'] += 1
                                        csv_export_stats['tracker_stats']['total_detections'] += len(detections)
                                        if detections.tracker_id is not None:
                                            valid_ids = sum(1 for tid in detections.tracker_id if tid is not None and tid != -1)
                                            csv_export_stats['tracker_stats']['total_tracker_ids_assigned'] += valid_ids
                                            if valid_ids > 0:
                                                csv_export_stats['tracker_stats']['frames_with_tracker_ids'] += 1
                                            elif len(detections) > 0:
                                                csv_export_stats['tracker_stats']['frames_tracker_failed'] += 1
                            
                            # OPTICAL FLOW: Apply motion prediction AFTER tracker update
                            # NOTE: This is applied AFTER tracker to refine positions, but can cause drift if flow is incorrect
                            # If bboxes are drifting, consider disabling optical flow or reducing flow_scale
                            if use_optical_flow and batch_frame is not None:
                                try:
                                    # Convert current frame to grayscale for optical flow
                                    current_gray = cv2.cvtColor(batch_frame, cv2.COLOR_BGR2GRAY)
                                    
                                    # Apply optical flow prediction if we have previous frame and detections
                                    if prev_frame_gray is not None and detections is not None and len(detections) > 0:
                                        # Store original positions for drift detection
                                        original_xyxy = detections.xyxy.copy() if len(detections) > 0 else None
                                        
                                        # Pass remove_net flag for net-aware flow masking
                                        # Base flow scale 0.8, will be adapted based on flow magnitude
                                        detections = predict_detections_with_optical_flow(
                                            detections, prev_frame_gray, current_gray, 
                                            flow_scale=0.8, remove_net=remove_net
                                        )
                                        
                                        # Check for excessive drift (bboxes moving too far)
                                        if original_xyxy is not None and len(detections) > 0 and len(original_xyxy) == len(detections.xyxy):
                                            max_drift = 0.0
                                            for i in range(len(detections)):
                                                orig = original_xyxy[i]
                                                new = detections.xyxy[i]
                                                # Calculate center point drift
                                                orig_center_y = (orig[1] + orig[3]) / 2
                                                new_center_y = (new[1] + new[3]) / 2
                                                drift_y = abs(new_center_y - orig_center_y)
                                                max_drift = max(max_drift, drift_y)
                                            
                                            # If drift is excessive (>100 pixels), warn and revert
                                            if max_drift > 100:
                                                if current_frame_num <= 10 or current_frame_num % 100 == 0:
                                                    print(f"âš  Frame {current_frame_num}: Optical flow caused excessive drift ({max_drift:.1f}px) - reverting to original positions")
                                                detections.xyxy = original_xyxy
                                    
                                    # Update previous frame for next iteration (even if no detections)
                                    # This ensures optical flow can be computed on the next frame
                                    prev_frame_gray = current_gray.copy()
                                except Exception as e:
                                    # If optical flow fails, continue with original detections
                                    # Update previous frame anyway to maintain continuity
                                    if batch_frame is not None:
                                        try:
                                            current_gray = cv2.cvtColor(batch_frame, cv2.COLOR_BGR2GRAY)
                                            prev_frame_gray = current_gray.copy()
                                        except:
                                            pass
                                    if current_frame_num % 500 == 0:
                                        print(f"âš  Optical flow prediction failed at frame {current_frame_num}: {e}")
                            
                            # Store detections before tracker for comparison and drift correction
                            # CRITICAL: Store original detection positions before tracker modifies them
                            detections_before_tracker_debug = detections
                            original_detections_for_drift = None
                            if detections is not None and len(detections) > 0 and detections.xyxy is not None:
                                # Deep copy to preserve original positions
                                import copy
                                original_detections_for_drift = copy.deepcopy(detections)
                            
                            # Debug logging for first few frames - check detections after tracker update
                            if current_frame_num <= 10:
                                detections_after_tracker = len(detections) if detections is not None else 0
                                tracker_ids_after = sum(1 for tid in (detections.tracker_id if detections is not None and detections.tracker_id is not None else []) if tid is not None) if detections is not None else 0
                                print(f"ðŸ” Frame {current_frame_num}: After tracker.update(): {detections_after_tracker} detections, {tracker_ids_after} with tracker_id")
                                if detections_before_tracker > 0 and detections_after_tracker == 0:
                                    print(f"  âš  WARNING: Tracker consumed {detections_before_tracker} detections but returned 0! This may be due to low confidence or tracker warmup.")
                            
                            # EXPANSION IOU: Update track velocities and positions after tracker update
                            # This provides motion prediction for the next frame
                            if use_expansion_iou and ADVANCED_TRACKING_UTILS_AVAILABLE and detections is not None and len(detections) > 0:
                                try:
                                    tracker_ids = detections.tracker_id if detections.tracker_id is not None else [None] * len(detections)
                                    
                                    # Update track positions and calculate velocities
                                    current_frame_tracks = {}
                                    for i, (xyxy, track_id) in enumerate(zip(detections.xyxy, tracker_ids)):
                                        if track_id is not None:
                                            x1, y1, x2, y2 = xyxy
                                            center_x = (x1 + x2) / 2.0
                                            center_y = (y1 + y2) / 2.0
                                            
                                            # Store current track position
                                            current_frame_tracks[track_id] = {
                                                'bbox': (float(x1), float(y1), float(x2), float(y2)),
                                                'center': (center_x, center_y),
                                                'frame': current_frame_num
                                            }
                                            
                                            # Calculate velocity if we have previous frame data
                                            # CRITICAL: Skip velocity updates for severely drifting tracks (20+ drifts)
                                            # These tracks have corrupted velocity estimates that cause drift
                                            skip_velocity_update = False
                                            if track_id in track_drift_count:
                                                drift_count = track_drift_count[track_id]
                                                if drift_count >= 20:  # Skip velocity updates for tracks with 20+ drifts
                                                    skip_velocity_update = True
                                                    # Clear velocity to prevent Expansion IOU from using corrupted estimates
                                                    if track_id in track_velocities:
                                                        del track_velocities[track_id]
                                                    if current_frame_num % 100 == 0:
                                                        print(f"  ðŸ”§ VELOCITY UPDATE SKIPPED: Track #{track_id} has drifted {drift_count} times - clearing corrupted velocity")
                                            
                                            if not skip_velocity_update and track_id in previous_frame_tracks:
                                                prev_track = previous_frame_tracks[track_id]
                                                prev_bbox = prev_track['bbox']
                                                prev_frame = prev_track['frame']
                                                frame_delta = current_frame_num - prev_frame
                                                
                                                if frame_delta > 0:
                                                    velocity = calculate_track_velocity(
                                                        (x1, y1, x2, y2),
                                                        prev_bbox,
                                                        frame_delta
                                                    )
                                                    track_velocities[track_id] = velocity
                                            
                                            # Update position history (keep last 5 frames)
                                            if track_id not in track_positions_history:
                                                # deque is already imported at the top of the file
                                                track_positions_history[track_id] = deque(maxlen=5)
                                            track_positions_history[track_id].append((
                                                current_frame_num, center_x, center_y, (x1, y1, x2, y2)
                                            ))
                                            
                                            # Update track_last_seen for Expansion IOU cleanup
                                            track_last_seen[track_id] = current_frame_num
                                    
                                    # Update previous_frame_tracks for next iteration
                                    previous_frame_tracks = current_frame_tracks.copy()
                                    
                                    # Clean up velocities for tracks that haven't been seen recently
                                    tracks_to_remove = []
                                    for track_id in track_velocities:
                                        if track_id not in current_frame_tracks:
                                            # Track disappeared - keep velocity for a few frames in case it reappears
                                            if track_id in track_last_seen:
                                                frames_since_seen = current_frame_num - track_last_seen[track_id]
                                                if frames_since_seen > 30:  # Remove after 30 frames
                                                    tracks_to_remove.append(track_id)
                                            else:
                                                tracks_to_remove.append(track_id)
                                    
                                    for track_id in tracks_to_remove:
                                        if track_id in track_velocities:
                                            del track_velocities[track_id]
                                        if track_id in track_positions_history:
                                            del track_positions_history[track_id]
                                    
                                except Exception as e:
                                    # If velocity tracking fails, continue without it
                                    if current_frame_num % 500 == 0:
                                        print(f"âš  Expansion IOU velocity tracking failed: {e}")
                            
                            # HOTA-GUIDED TRACKING: Collect tracking data for quality monitoring
                            if hota_guided_tracker is not None and detections is not None and len(detections) > 0:
                                try:
                                    # Convert detections to HOTA format: (track_id, x1, y1, x2, y2)
                                    hota_detections = []
                                    tracker_ids = detections.tracker_id if detections.tracker_id is not None else [None] * len(detections)
                                    for i, (xyxy, track_id) in enumerate(zip(detections.xyxy, tracker_ids)):
                                        if track_id is not None:
                                            x1, y1, x2, y2 = xyxy
                                            hota_detections.append((int(track_id), float(x1), float(y1), float(x2), float(y2)))
                                    
                                    if hota_detections:
                                        hota_guided_tracker.add_frame_data(current_frame_num, hota_detections)
                                except Exception as e:
                                    # Silently fail - HOTA monitoring is optional
                                    if current_frame_num % 500 == 0:
                                        print(f"âš  HOTA data collection failed: {e}")
                            
                            # HOTA-GUIDED TRACKING: Periodic evaluation and Re-ID threshold adjustment
                            if hota_guided_tracker is not None and reid_tracker is not None and current_frame_num % 200 == 0 and current_frame_num > 100:
                                try:
                                    # Calculate recent HOTA (use anchor frames as ground truth if available)
                                    gt_tracks = None
                                    if anchor_frames:
                                        # Convert anchor frames to HOTA format
                                        gt_tracks = {}
                                        for frame_num, anchors in anchor_frames.items():
                                            if anchors is None or not isinstance(anchors, list):
                                                continue
                                            if abs(frame_num - current_frame_num) <= 100:  # Within window
                                                for anchor in anchors:
                                                    track_id = anchor.get('track_id', 0)
                                                    bbox = anchor.get('bbox', [0, 0, 0, 0])
                                                    if track_id not in gt_tracks:
                                                        gt_tracks[track_id] = []
                                                    gt_tracks[track_id].append((frame_num, float(bbox[0]), float(bbox[1]), float(bbox[2]), float(bbox[3])))
                                    
                                    # Calculate ALL metrics in real-time (HOTA, MOTA, IDF1)
                                    recent_metrics = hota_guided_tracker.calculate_recent_metrics(gt_tracks)
                                    
                                    # Adjust Re-ID threshold based on ALL metrics (HOTA, MOTA, IDF1)
                                    if hasattr(reid_tracker, 'similarity_threshold'):
                                        current_threshold = reid_tracker.similarity_threshold
                                        new_threshold = hota_guided_tracker.suggest_reid_threshold_adjustment(
                                            current_threshold, recent_metrics
                                        )
                                        
                                        if abs(new_threshold - current_threshold) > 0.05:
                                            reid_tracker.similarity_threshold = new_threshold
                                            print(f"ðŸ“Š Real-time metrics adjustment (Frame {current_frame_num}): Re-ID threshold {current_threshold:.2f} â†’ {new_threshold:.2f}")
                                            print(f"   HOTA: {recent_metrics.get('HOTA', 0):.3f}, MOTA: {recent_metrics.get('MOTA', 0):.3f}, IDF1: {recent_metrics.get('IDF1', 0):.3f}")
                                            print(f"   AssA: {recent_metrics.get('AssA', 0):.3f}, IDSW: {recent_metrics.get('IDSW', 0)}")
                                    
                                    # Report quality every 500 frames with ALL metrics
                                    if current_frame_num % 500 == 0:
                                        report = hota_guided_tracker.get_tracking_quality_report()
                                        if report.get('suggestions') or current_frame_num % 1000 == 0:
                                            print(f"ðŸ“Š Real-Time Metrics Report (Frame {current_frame_num}):")
                                            metrics = report['recent_metrics']
                                            print(f"   HOTA: {metrics.get('HOTA', 0):.3f} (DetA: {metrics.get('DetA', 0):.3f}, AssA: {metrics.get('AssA', 0):.3f})")
                                            print(f"   MOTA: {metrics.get('MOTA', 0):.3f} (FN: {metrics.get('FN', 0)}, FP: {metrics.get('FP', 0)}, IDSW: {metrics.get('IDSW', 0)})")
                                            print(f"   IDF1: {metrics.get('IDF1', 0):.3f} (IDP: {metrics.get('IDP', 0):.3f}, IDR: {metrics.get('IDR', 0):.3f})")
                                            if report.get('suggestions'):
                                                print(f"   Route Corrections:")
                                                for suggestion in report['suggestions']:
                                                    print(f"   â†’ {suggestion}")
                                except Exception as e:
                                    # Silently fail - HOTA monitoring is optional
                                    if current_frame_num % 500 == 0:
                                        print(f"âš  HOTA evaluation failed: {e}")
                        except (KeyError, ValueError, AttributeError) as e:
                            # Handle specific tracker errors (KeyError from
                            # OC-SORT, etc.)
                            error_type = type(e).__name__
                            error_msg = str(e)
                            detections_info = f"detections={
                                len(detections) if detections is not None else 'None'}"
                            if detections is not None and hasattr(
                                    detections, 'xyxy'):
                                detections_info += f", xyxy={
                                    len(
                                        detections.xyxy) if detections.xyxy is not None else 'None'}"
                            print(
                                f"âš  Tracker update failed at frame {
                                    frame_data['frame_num']}: {error_type}: {error_msg} ({detections_info})")
                            # CRITICAL: When tracker fails, we need to create empty detections to avoid jitter
                            # Keeping detections without tracker IDs causes ID loss and jitter
                            # The tracker will recover on the next successful
                            # frame
                            detections = sv.Detections.empty()
                        except Exception as e:
                            # Catch any other unexpected errors
                            error_type = type(e).__name__
                            error_msg = str(e)
                            detections_info = f"detections={
                                len(detections) if detections is not None else 'None'}"
                            if detections is not None and hasattr(
                                    detections, 'xyxy'):
                                detections_info += f", xyxy={
                                    len(
                                        detections.xyxy) if detections.xyxy is not None else 'None'}"
                            print(
                                f"âš  Tracker update failed at frame {
                                    frame_data['frame_num']}: {error_type}: {error_msg} ({detections_info})")
                            # Create empty detections to allow tracker to
                            # recover
                            detections = sv.Detections.empty()

                        # Safety check: ensure detections is not None after
                        # tracker update
                        if detections is None:
                            detections = sv.Detections.empty()
                        
                        # ðŸ›¡ï¸ DRIFT DETECTION AND CORRECTION: Prevent bboxes from drifting away from players
                        # This is critical for maintaining accurate tracking - bboxes must stay aligned with actual detections
                        # ENHANCED: More aggressive drift prevention to stop recurring drift issues
                        if detections is not None and len(detections) > 0 and detections.xyxy is not None:
                            # Use stored original detection positions (before tracker modifications)
                            if 'original_detections_for_drift' in locals() and original_detections_for_drift is not None:
                                original_detections = original_detections_for_drift
                                if len(original_detections) > 0 and original_detections.xyxy is not None and len(original_detections.xyxy) > 0:
                                    # Match current detections to original by position (nearest neighbor)
                                    corrected_xyxy = detections.xyxy.copy()
                                    drift_corrections = 0
                                    tracks_with_drift = set()  # Track which tracks are drifting repeatedly
                                    
                                    # Get frame dimensions for bounds checking
                                    frame_width = batch_frame.shape[1] if batch_frame is not None else 3840
                                    frame_height = batch_frame.shape[0] if batch_frame is not None else 2160
                                    
                                    for i, current_bbox in enumerate(detections.xyxy):
                                        if i >= len(original_detections.xyxy):
                                            continue
                                        
                                        current_center_x = (current_bbox[0] + current_bbox[2]) / 2.0
                                        current_center_y = (current_bbox[1] + current_bbox[3]) / 2.0
                                        
                                        # Find corresponding original detection (by index or nearest)
                                        original_bbox = original_detections.xyxy[i]
                                        original_center_x = (original_bbox[0] + original_bbox[2]) / 2.0
                                        original_center_y = (original_bbox[1] + original_bbox[3]) / 2.0
                                        
                                        # Calculate drift distance
                                        drift_x = abs(current_center_x - original_center_x)
                                        drift_y = abs(current_center_y - original_center_y)
                                        drift_distance = np.sqrt(drift_x**2 + drift_y**2)
                                        
                                        # ENHANCED: More aggressive drift threshold (reduced from 100px to 50px)
                                        # This prevents even moderate drift from accumulating - critical for preventing bbox drift
                                        MAX_DRIFT_PIXELS = 50  # Maximum allowed drift before correction (very aggressive to prevent drift)
                                        
                                        # Check if bbox has drifted too far OR is outside frame bounds
                                        needs_correction = False
                                        correction_reason = ""
                                        
                                        # CRITICAL: For severely drifting tracks (50+ drifts), use much lower drift threshold
                                        # This catches drift earlier before it becomes severe
                                        drift_threshold = MAX_DRIFT_PIXELS
                                        if detections.tracker_id is not None and i < len(detections.tracker_id):
                                            track_id = detections.tracker_id[i]
                                            if track_id is not None:
                                                track_id_int = int(track_id)
                                                drift_count = track_drift_count.get(track_id_int, 0)
                                                # For tracks with 100+ drifts, use 5px threshold (extremely aggressive - tracker state is completely corrupted)
                                                # For tracks with 50+ drifts, use 10px threshold (very aggressive)
                                                # For tracks with 30+ drifts, use 15px threshold (very aggressive)
                                                # For tracks with 20+ drifts, use 25px threshold (aggressive)
                                                # For tracks with 10+ drifts, use 35px threshold (moderate)
                                                if drift_count >= 100:
                                                    drift_threshold = 5.0  # Extremely aggressive for completely corrupted tracks (100+ drifts)
                                                elif drift_count >= 50:
                                                    drift_threshold = 10.0  # Very aggressive for severely drifting tracks
                                                elif drift_count >= 30:
                                                    drift_threshold = 15.0  # Very aggressive for frequently drifting tracks (30-49 drifts)
                                                elif drift_count >= 20:
                                                    drift_threshold = 25.0  # Aggressive for frequently drifting tracks
                                                elif drift_count >= 10:
                                                    drift_threshold = 35.0  # Moderate for occasionally drifting tracks
                                        
                                        # Check for off-screen drift with different thresholds based on drift count
                                        # CRITICAL: For tracks with 100+ drifts, use much tighter off-screen threshold to catch drift earlier
                                        off_screen_threshold = 200  # Default: 200px buffer
                                        if drift_count >= 100:
                                            off_screen_threshold = 50  # Very aggressive for critical tracks (100+ drifts) - catch drift 50px before frame edge
                                        elif drift_count >= 50:
                                            off_screen_threshold = 100  # Aggressive for severe tracks (50-99 drifts) - catch drift 100px before frame edge
                                        
                                        if drift_distance > drift_threshold:
                                            needs_correction = True
                                            correction_reason = f"drift {drift_distance:.1f}px > {drift_threshold:.1f}px"
                                        elif current_center_x < -off_screen_threshold or current_center_x > frame_width + off_screen_threshold or \
                                             current_center_y < -off_screen_threshold or current_center_y > frame_height + off_screen_threshold:
                                            needs_correction = True
                                            correction_reason = f"bbox drifted off-screen (threshold: {off_screen_threshold}px)"
                                        
                                        if needs_correction:
                                            # Reset bbox to original detection position
                                            corrected_xyxy[i] = original_bbox.copy()
                                            drift_corrections += 1
                                            
                                            # Track which track IDs are drifting (for potential tracker reset)
                                            if detections.tracker_id is not None and i < len(detections.tracker_id):
                                                track_id = detections.tracker_id[i]
                                                if track_id is not None:
                                                    track_id_int = int(track_id)
                                                    tracks_with_drift.add(track_id_int)
                                                    
                                                    # Track drift history
                                                    if track_id_int not in track_drift_count:
                                                        track_drift_count[track_id_int] = 0
                                                        track_drift_frames[track_id_int] = []
                                                    track_drift_count[track_id_int] += 1
                                                    track_drift_frames[track_id_int].append(current_frame_num)
                                                    
                                                    # Keep only recent drift history (last 100 frames)
                                                    track_drift_frames[track_id_int] = [f for f in track_drift_frames[track_id_int] if current_frame_num - f <= 100]
                                                    
                                                    # Warn if track is drifting repeatedly
                                                    if track_drift_count[track_id_int] >= MAX_DRIFT_CORRECTIONS_BEFORE_WARNING:
                                                        recent_drifts = len([f for f in track_drift_frames[track_id_int] if current_frame_num - f <= 20])
                                                        if recent_drifts >= 3:  # Drifted 3+ times in last 20 frames
                                                            if current_frame_num % 50 == 0:  # Log every 50 frames to avoid spam
                                                                print(f"  âš  RECURRING DRIFT: Track #{track_id_int} has drifted {track_drift_count[track_id_int]} times (recent: {recent_drifts} in last 20 frames)")
                                                                print(f"     â†’ Tracker may need adjustment or this track should use detection positions directly")
                                            
                                            # Log drift correction (first 10 frames, then every 50 frames to catch recurring issues)
                                            if current_frame_num <= 10 or current_frame_num % 50 == 0:
                                                track_id_str = f"Track #{detections.tracker_id[i]}" if detections.tracker_id is not None and i < len(detections.tracker_id) and detections.tracker_id[i] is not None else "Unknown"
                                                print(f"  ðŸ›¡ï¸ DRIFT CORRECTION: Frame {current_frame_num}, {track_id_str} - {correction_reason}, reset to original detection")
                                    
                                    # Apply corrections if any were made
                                    if drift_corrections > 0:
                                        detections.xyxy = corrected_xyxy
                                        
                                        # ENHANCED: If same tracks drift repeatedly, log warning and consider tracker adjustment
                                        if current_frame_num <= 10 or current_frame_num % 50 == 0:
                                            print(f"  ðŸ›¡ï¸ DRIFT CORRECTION: Frame {current_frame_num} - corrected {drift_corrections} bbox(es) that drifted too far from detections")
                                            if len(tracks_with_drift) > 0:
                                                print(f"     â†’ Tracks with drift: {sorted(tracks_with_drift)} - tracker may need adjustment")
                                        
                                        # CRITICAL: After correcting drift, ensure tracker uses corrected positions
                                        # Update tracker_id positions in detections to match corrected bboxes
                                        # This ensures the tracker's next update uses the corrected positions
                                        if detections.tracker_id is not None:
                                            for i, track_id in enumerate(detections.tracker_id):
                                                if track_id is not None and i < len(corrected_xyxy):
                                                    # The corrected bbox is already in detections.xyxy
                                                    # The tracker will use these corrected positions on the next frame
                                                    pass  # Position already corrected in detections.xyxy
                                        
                                        # CRITICAL FIX: For tracks that drift repeatedly, disable temporal smoothing AND use detection positions directly
                                        # ENHANCED: Apply bypass earlier (at 3 drifts instead of 5) to catch problem tracks even earlier
                                        # Tracks that drift >3 times should use detection positions directly (bypass tracker prediction)
                                        problem_tracks = set()
                                        severe_drift_tracks = set()
                                        for track_id_int in tracks_with_drift:
                                            drift_count = track_drift_count.get(track_id_int, 0)
                                            # Lower threshold: apply bypass at 3 drifts (was 5) to catch problem tracks earlier
                                            if drift_count > 3:
                                                problem_tracks.add(track_id_int)
                                                # Mark this track to bypass temporal smoothing AND use detection positions directly
                                                if track_id_int not in track_drift_bypass:
                                                    track_drift_bypass[track_id_int] = True
                                                    print(f"  ðŸ”§ DRIFT BYPASS: Track #{track_id_int} will bypass temporal smoothing (drifted {drift_count} times, threshold: 3)")
                                            
                                            # CRITICAL: For tracks that drift 10+ times, mark as severe drift
                                            # These tracks have a fundamental issue with the tracker's predictions
                                            if drift_count >= 10:
                                                severe_drift_tracks.add(track_id_int)
                                                if current_frame_num % 50 == 0:  # Log every 50 frames to avoid spam
                                                    if drift_count >= 100:
                                                        print(f"  ðŸš¨ CRITICAL DRIFT: Track #{track_id_int} has drifted {drift_count} times - tracker state is completely corrupted")
                                                        print(f"     â†’ Using 5px drift threshold and detection positions directly (no tracker prediction)")
                                                    else:
                                                        print(f"  ðŸš¨ SEVERE DRIFT: Track #{track_id_int} has drifted {drift_count} times - tracker predictions are unreliable")
                                                        print(f"     â†’ This track will use detection positions directly (no tracker prediction)")
                                                    
                                                    # CRITICAL: For tracks in bypass mode, we need to prevent the tracker from using its predictions
                                                    # The tracker's internal state may have bad predictions that cause drift
                                                    # Solution: After drift correction, ensure the corrected position is used by the tracker
                                                    # This is done by updating detections.xyxy with the corrected position (already done above)
                                                    # But we also need to clear the tracker's velocity/prediction for this track
                                                    # Since we can't directly access tracker internals, we'll rely on the corrected detections.xyxy
                                                    # and ensure temporal smoothing is bypassed (which is already handled below)
                                                    # NOTE: The adaptive drift threshold (implemented above) will catch drift earlier for these tracks
                                        
                                        # Clean up bypass flags for tracks that haven't drifted recently
                                        tracks_to_cleanup_bypass = []
                                        for track_id_int in track_drift_bypass:
                                            if track_id_int not in tracks_with_drift:
                                                # Track hasn't drifted in this frame - check if it's been stable
                                                if track_id_int in track_drift_frames:
                                                    recent_drifts = len([f for f in track_drift_frames[track_id_int] if current_frame_num - f <= 50])
                                                    if recent_drifts == 0:  # No drifts in last 50 frames
                                                        tracks_to_cleanup_bypass.append(track_id_int)
                                        for track_id_int in tracks_to_cleanup_bypass:
                                            del track_drift_bypass[track_id_int]
                                            if current_frame_num % 50 == 0:
                                                print(f"  âœ… DRIFT BYPASS CLEARED: Track #{track_id_int} has been stable - re-enabling temporal smoothing")
                        
                        # ðŸŽ¯ ERRATIC TRACK FILTER: Remove tracks that jump around erratically
                        # This filters out tracks that aren't following a consistent object (false positives, noise)
                        if detections is not None and len(detections) > 0 and detections.tracker_id is not None:
                            erratic_track_indices = []
                            tracker_ids = detections.tracker_id if detections.tracker_id is not None else [None] * len(detections)
                            
                            for det_idx, (xyxy, track_id) in enumerate(zip(detections.xyxy, tracker_ids)):
                                if track_id is None:
                                    continue
                                
                                track_id_int = int(track_id)
                                x1, y1, x2, y2 = xyxy
                                center_x = (x1 + x2) / 2.0
                                center_y = (y1 + y2) / 2.0
                                
                                # Initialize history for this track
                                if track_id_int not in track_position_history_for_erratic:
                                    track_position_history_for_erratic[track_id_int] = deque(maxlen=10)  # Keep last 10 positions
                                
                                # Add current position
                                track_position_history_for_erratic[track_id_int].append((center_x, center_y, current_frame_num))
                                
                                # Check if track is erratic (need at least 5 positions to judge)
                                if len(track_position_history_for_erratic[track_id_int]) >= 5:
                                    positions = list(track_position_history_for_erratic[track_id_int])
                                    
                                    # Calculate position variance (how much the track jumps around)
                                    x_coords = [p[0] for p in positions]
                                    y_coords = [p[1] for p in positions]
                                    x_variance = np.var(x_coords) if len(x_coords) > 1 else 0
                                    y_variance = np.var(y_coords) if len(y_coords) > 1 else 0
                                    total_variance = x_variance + y_variance
                                    
                                    # Calculate maximum jump distance between consecutive frames
                                    max_jump = 0.0
                                    for i in range(1, len(positions)):
                                        prev_x, prev_y, _ = positions[i-1]
                                        curr_x, curr_y, _ = positions[i]
                                        jump = np.sqrt((curr_x - prev_x)**2 + (curr_y - prev_y)**2)
                                        max_jump = max(max_jump, jump)
                                    
                                    # Calculate average movement consistency
                                    movements = []
                                    for i in range(1, len(positions)):
                                        prev_x, prev_y, prev_frame = positions[i-1]
                                        curr_x, curr_y, curr_frame = positions[i]
                                        frame_diff = curr_frame - prev_frame
                                        if frame_diff > 0:
                                            movement = np.sqrt((curr_x - prev_x)**2 + (curr_y - prev_y)**2) / frame_diff
                                            movements.append(movement)
                                    
                                    avg_movement = np.mean(movements) if movements else 0
                                    movement_std = np.std(movements) if len(movements) > 1 else 0
                                    
                                    # ERATIC DETECTION CRITERIA:
                                    # 1. High position variance (jumping around) AND
                                    # 2. Large maximum jump (sudden position changes) OR
                                    # 3. Inconsistent movement (high std relative to mean)
                                    is_eratic = False
                                    erratic_reason = []
                                    
                                    # High variance threshold: > 50000 pixels^2 (about 224 pixels std in each direction)
                                    if total_variance > 50000:
                                        is_eratic = True
                                        erratic_reason.append(f"high_variance({total_variance:.0f})")
                                    
                                    # Large jump threshold: > 300 pixels in one frame (impossible for a player)
                                    if max_jump > 300:
                                        is_eratic = True
                                        erratic_reason.append(f"large_jump({max_jump:.0f}px)")
                                    
                                    # Inconsistent movement: std > 2x mean (very erratic)
                                    if avg_movement > 0 and movement_std > 2.0 * avg_movement:
                                        is_eratic = True
                                        erratic_reason.append(f"inconsistent_movement(std={movement_std:.1f}, mean={avg_movement:.1f})")
                                    
                                    if is_eratic:
                                        erratic_track_indices.append(det_idx)
                                        if current_frame_num % 100 == 0:  # Log periodically
                                            print(f"  ðŸš« ERRATIC TRACK FILTER: Track #{track_id_int} marked as erratic at frame {current_frame_num}: {', '.join(erratic_reason)}")
                                            print(f"     â†’ Position variance: {total_variance:.0f}, Max jump: {max_jump:.0f}px, Movement std: {movement_std:.1f}")
                                        
                                        # Clear history for this track (will rebuild if it stabilizes)
                                        if track_id_int in track_position_history_for_erratic:
                                            track_position_history_for_erratic[track_id_int].clear()
                            
                            # Remove erratic tracks
                            if len(erratic_track_indices) > 0:
                                keep_mask = np.ones(len(detections), dtype=bool)
                                keep_mask[erratic_track_indices] = False
                                success, detections = safe_apply_detection_mask(
                                    detections, keep_mask,
                                    frame_num=current_frame_num
                                )
                                if current_frame_num % 100 == 0:
                                    print(f"  ðŸŽ¯ ERRATIC TRACK FILTER: Removed {len(erratic_track_indices)} erratic track(s) at frame {current_frame_num}")
                            
                            # Clean up old track histories (tracks that haven't been seen in 100 frames)
                            tracks_to_cleanup = []
                            for tid in track_position_history_for_erratic:
                                if len(track_position_history_for_erratic[tid]) > 0:
                                    last_frame = track_position_history_for_erratic[tid][-1][2]
                                    if current_frame_num - last_frame > 100:
                                        tracks_to_cleanup.append(tid)
                            for tid in tracks_to_cleanup:
                                del track_position_history_for_erratic[tid]
                        
                        # Debug logging for first few frames - check detections after tracker
                        if current_frame_num <= 10:
                            detections_after_tracker = len(detections) if detections is not None else 0
                            tracker_ids_after = sum(1 for tid in (detections.tracker_id if detections is not None and detections.tracker_id is not None else []) if tid is not None) if detections is not None else 0
                            print(f"ðŸ” Frame {current_frame_num}: After tracker: {detections_after_tracker} detections, {tracker_ids_after} with tracker_id")
                            # CRITICAL DEBUG: Show actual tracker_id values to catch when they become -1
                            if detections is not None and detections.tracker_id is not None:
                                actual_ids = [int(tid) if tid is not None else None for tid in detections.tracker_id]
                                print(f"ðŸ” Frame {current_frame_num}: After tracker - actual tracker_id values: {actual_ids}")
                        
                        # ðŸŽ¯ ANCHOR FRAMES: Apply frame-specific player tags with 1.00 confidence (HIGHEST PRIORITY)
                        # CRITICAL: Process anchor frames AFTER tracker runs so track IDs are available for matching
                        # Anchor frames are ground truth tags from Setup Wizard or Gallery Seeder - apply BEFORE gallery matching
                        # GROK'S FIX: Enhanced ID transfer from seed/gallery to YOLO/Re-ID with IoU matching and confidence refresh
                        if anchor_frames:
                            # ðŸ”„ PERIODIC ANCHOR REFRESH: Re-map anchors every 100 frames for repeatability
                            # This ensures anchor protection stays accurate even if tracks drift
                            if current_frame_num > 0 and current_frame_num % 100 == 0 and len(anchor_frames) > 0:
                                # Find all anchor frames within the last 100 frames (for refresh context)
                                refresh_window_start = max(0, current_frame_num - 100)
                                refresh_anchors = []
                                for frame_num in range(refresh_window_start, current_frame_num + 1):
                                    if frame_num in anchor_frames:
                                        refresh_anchors.extend(anchor_frames[frame_num])
                                
                                if len(refresh_anchors) > 0 and detections is not None and len(detections) > 0:
                                    refreshed_count = 0
                                    for anchor in refresh_anchors:
                                        anchor_player_name = anchor.get('player_name')
                                        anchor_bbox = anchor.get('bbox')
                                        if not anchor_player_name or anchor_bbox is None or len(anchor_bbox) < 4:
                                            continue
                                        
                                        # Find best matching detection using IoU
                                        anchor_x1, anchor_y1, anchor_x2, anchor_y2 = anchor_bbox[:4]
                                        anchor_area = (anchor_x2 - anchor_x1) * (anchor_y2 - anchor_y1)
                                        anchor_center_x = (anchor_x1 + anchor_x2) / 2
                                        anchor_center_y = (anchor_y1 + anchor_y2) / 2
                                        
                                        best_match_idx = None
                                        best_iou = 0.0
                                        for det_idx, (det_x1, det_y1, det_x2, det_y2) in enumerate(detections.xyxy):
                                            inter_x1 = max(anchor_x1, det_x1)
                                            inter_y1 = max(anchor_y1, det_y1)
                                            inter_x2 = min(anchor_x2, det_x2)
                                            inter_y2 = min(anchor_y2, det_y2)
                                            if inter_x2 > inter_x1 and inter_y2 > inter_y1:
                                                inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)
                                                det_area = (det_x2 - det_x1) * (det_y2 - det_y1)
                                                union_area = anchor_area + det_area - inter_area
                                                iou = inter_area / union_area if union_area > 0 else 0.0
                                                if iou > best_iou and iou > 0.3:  # Lower threshold for refresh
                                                    best_iou = iou
                                                    best_match_idx = det_idx
                                        
                                        # If found a match, refresh the anchor assignment
                                        if best_match_idx is not None and detections.tracker_id is not None and best_match_idx < len(detections.tracker_id):
                                            track_id = detections.tracker_id[best_match_idx]
                                            if track_id is not None:
                                                track_id_int = int(track_id)
                                                track_id_str = str(track_id_int)
                                                
                                                # Refresh anchor assignment
                                                if track_id_str not in player_names or player_names[track_id_str] != anchor_player_name:
                                                    player_names[track_id_str] = anchor_player_name
                                                    track_name_confidence[track_id_int] = (anchor_player_name, 1.00, current_frame_num)
                                                    track_anchor_assigned[track_id_int] = (anchor_player_name, current_frame_num)
                                                    refreshed_count += 1
                                    
                                    if refreshed_count > 0:
                                        print(f"  ðŸ”„ PERIODIC ANCHOR REFRESH: Refreshed {refreshed_count} anchor assignment(s) at frame {current_frame_num}")
                            
                            # DEBUG: Log anchor frame state (first 10 frames and every 100 frames)
                            if current_frame_num <= 10 or current_frame_num % 100 == 0:
                                print(f"ðŸ” DEBUG Frame {current_frame_num}: Anchor frames loaded - {len(anchor_frames)} frame(s) with anchors")
                                if detections is not None and detections.tracker_id is not None:
                                    active_track_ids = [int(tid) for tid in detections.tracker_id if tid is not None]
                                    print(f"ðŸ” DEBUG Frame {current_frame_num}: YOLO raw track IDs: {active_track_ids}")
                                    # CRITICAL DEBUG: Show actual tracker_id values BEFORE anchor processing
                                    actual_ids_before_anchor = [int(tid) if tid is not None else None for tid in detections.tracker_id]
                                    print(f"ðŸ” DEBUG Frame {current_frame_num}: BEFORE anchor processing - tracker_id values: {actual_ids_before_anchor}")
                            # Log anchor frame summary on first frame only
                            if current_frame_num == 0:
                                total_anchor_frames = len(anchor_frames)
                                total_anchor_tags = sum(len(anchors) if isinstance(anchors, list) else 0 for anchors in anchor_frames.values())
                                anchor_players = set()
                                for anchors in anchor_frames.values():
                                    if isinstance(anchors, list):
                                        for anchor in anchors:
                                            player_name = anchor.get('player_name')
                                            if player_name:
                                                anchor_players.add(player_name)
                                print(f"  ðŸŽ¯ ANCHOR FRAMES LOADED: {total_anchor_frames} frame(s) with {total_anchor_tags} tag(s) for players: {', '.join(sorted(anchor_players))}")
                            
                            # Initialize anchor_tags to empty list (will be populated if current frame has anchors)
                            anchor_tags = []
                            anchors_applied = 0
                            
                            if current_frame_num in anchor_frames:
                                anchor_tags = anchor_frames[current_frame_num]
                                if anchor_tags is None or not isinstance(anchor_tags, list):
                                    anchor_tags = []
                                if len(anchor_tags) > 0:
                                    print(f"  ðŸŽ¯ ANCHOR FRAME: Frame {current_frame_num} has {len(anchor_tags)} anchor tag(s) - applying protection")
                                    # DIAGNOSTIC: Log anchor frame details
                                    for anchor in anchor_tags:
                                        anchor_name = anchor.get('player_name', 'Unknown')
                                        # Handle malformed anchor names (lists, etc.) - clean for display AND fix the anchor dict
                                        if isinstance(anchor_name, list):
                                            anchor_name = anchor_name[0].strip() if len(anchor_name) > 0 and isinstance(anchor_name[0], str) else 'Unknown'
                                            # Also fix the anchor dict
                                            anchor['player_name'] = anchor_name
                                        elif not isinstance(anchor_name, str):
                                            anchor_name = str(anchor_name).strip() if anchor_name else 'Unknown'
                                            # Also fix the anchor dict
                                            anchor['player_name'] = anchor_name
                                        else:
                                            anchor_name = anchor_name.strip() if anchor_name else 'Unknown'
                                            # Ensure anchor dict has cleaned name
                                            anchor['player_name'] = anchor_name
                                        anchor_tid = anchor.get('track_id')
                                        anchor_bbox = anchor.get('bbox')
                                        print(f"     â†’ '{anchor_name}': track_id={anchor_tid}, bbox={'present' if anchor_bbox else 'missing'}")
                            
                            # Process anchor frames if we have any for this frame
                            # NOTE: The full anchor frame processing logic (bbox matching, track ID matching, etc.) 
                            # is in the original block at lines 9059-9986. For now, we'll process a simplified version here
                            # that focuses on track ID matching (which is now available after tracker runs)
                            if len(anchor_tags) > 0 and detections is not None and detections.tracker_id is not None:
                                # Import the anchor frame processing logic from the original location
                                # For now, we'll do a simplified version that matches by track_id
                                for anchor in anchor_tags:
                                    anchor_player_name = anchor.get('player_name')
                                    anchor_track_id = anchor.get('track_id')
                                    anchor_bbox = anchor.get('bbox')
                                    
                                    if not anchor_player_name:
                                        continue
                                    
                                    # Initialize matching variables (used later for confidence boost)
                                    best_match_idx = None
                                    best_iou = 0.0
                                    best_distance = float('inf')
                                    
                                    # Try to match by track_id first (now available after tracker runs)
                                    matched_track_id = None
                                    if anchor_track_id is not None:
                                        anchor_track_id_int = int(anchor_track_id)
                                        # Check if this track_id exists in current detections
                                        for tid in detections.tracker_id:
                                            if tid is not None and int(tid) == anchor_track_id_int:
                                                matched_track_id = anchor_track_id_int
                                                break
                                    
                                    # GROK'S FIX: If track_id match failed, try IoU-based bbox matching (more robust than distance)
                                    # ENHANCED: Also try distance-based matching if IoU fails (for drifted bboxes)
                                    if matched_track_id is None and anchor_bbox is not None and len(anchor_bbox) >= 4 and len(detections.xyxy) > 0:
                                        anchor_x1, anchor_y1, anchor_x2, anchor_y2 = anchor_bbox[:4]
                                        anchor_area = (anchor_x2 - anchor_x1) * (anchor_y2 - anchor_y1)
                                        anchor_center_x = (anchor_x1 + anchor_x2) / 2
                                        anchor_center_y = (anchor_y1 + anchor_y2) / 2
                                        
                                        best_match_idx = None
                                        best_iou = 0.0
                                        best_distance = float('inf')
                                        
                                        # IoU-based matching (more robust than distance)
                                        for det_idx, (det_x1, det_y1, det_x2, det_y2) in enumerate(detections.xyxy):
                                            # Calculate IoU
                                            inter_x1 = max(anchor_x1, det_x1)
                                            inter_y1 = max(anchor_y1, det_y1)
                                            inter_x2 = min(anchor_x2, det_x2)
                                            inter_y2 = min(anchor_y2, det_y2)
                                            
                                            if inter_x2 > inter_x1 and inter_y2 > inter_y1:
                                                inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)
                                                det_area = (det_x2 - det_x1) * (det_y2 - det_y1)
                                                union_area = anchor_area + det_area - inter_area
                                                iou = inter_area / union_area if union_area > 0 else 0.0
                                                
                                                # ENHANCED: Lower IoU threshold when anchor has no track_id (more lenient matching)
                                                # If anchor has track_id=None, use lower threshold (0.3) to account for drift
                                                # If anchor has track_id, use higher threshold (0.7) for strict matching
                                                iou_threshold = 0.3 if anchor_track_id is None else 0.7
                                                
                                                if iou > best_iou and iou > iou_threshold:
                                                    best_iou = iou
                                                    best_match_idx = det_idx
                                            
                                            # ENHANCED: Also calculate distance for fallback matching if IoU fails
                                            # This helps match anchor frames even when bboxes are drifted
                                            det_center_x = (det_x1 + det_x2) / 2
                                            det_center_y = (det_y1 + det_y2) / 2
                                            distance = np.sqrt((det_center_x - anchor_center_x)**2 + (det_center_y - anchor_center_y)**2)
                                            if distance < best_distance:
                                                best_distance = distance
                                        
                                        # If IoU matching failed but we have a close detection, use distance-based matching
                                        if best_match_idx is None and best_distance < 500:  # 500px max distance for anchor matching
                                            # Find the detection with best_distance
                                            for det_idx, (det_x1, det_y1, det_x2, det_y2) in enumerate(detections.xyxy):
                                                det_center_x = (det_x1 + det_x2) / 2
                                                det_center_y = (det_y1 + det_y2) / 2
                                                distance = np.sqrt((det_center_x - anchor_center_x)**2 + (det_center_y - anchor_center_y)**2)
                                                if abs(distance - best_distance) < 1.0:  # Found the closest detection
                                                    best_match_idx = det_idx
                                                    if current_frame_num <= 10 or current_frame_num % 100 == 0:
                                                        print(f"  ðŸ” DEBUG: Matched anchor '{anchor_player_name}' via distance={best_distance:.1f}px (IoU failed, using distance fallback)")
                                                    break
                                        
                                        if best_match_idx is not None and detections.tracker_id is not None and best_match_idx < len(detections.tracker_id):
                                            track_id = detections.tracker_id[best_match_idx]
                                            if track_id is not None:
                                                matched_track_id = int(track_id)
                                                if current_frame_num <= 10 or (best_iou > 0 and current_frame_num % 100 == 0):
                                                    match_method = f"IoU={best_iou:.3f}" if best_iou > 0 else f"distance={best_distance:.1f}px"
                                                    print(f"  ðŸ” DEBUG: Matched anchor '{anchor_player_name}' to Track #{matched_track_id} via {match_method}")
                                        elif current_frame_num <= 10 or current_frame_num % 100 == 0:
                                            # Diagnostic: why did matching fail?
                                            print(f"  âš  ANCHOR MATCH FAILED: '{anchor_player_name}' - best IoU={best_iou:.3f}, best distance={best_distance:.1f}px")
                                            print(f"     â†’ Anchor bbox: [{anchor_x1:.1f}, {anchor_y1:.1f}, {anchor_x2:.1f}, {anchor_y2:.1f}]")
                                            if len(detections.xyxy) > 0:
                                                sample_det = detections.xyxy[0]
                                                print(f"     â†’ Sample detection: [{sample_det[0]:.1f}, {sample_det[1]:.1f}, {sample_det[2]:.1f}, {sample_det[3]:.1f}]")
                                    
                                    # GROK'S FIX: Apply anchor frame if we found a match - with Re-ID confidence boost
                                    if matched_track_id is not None:
                                        track_id_str = str(matched_track_id)
                                        player_names[track_id_str] = anchor_player_name
                                        track_name_confidence[matched_track_id] = (anchor_player_name, 1.00, current_frame_num)
                                        track_anchor_assigned[matched_track_id] = (anchor_player_name, current_frame_num)
                                        player_to_track_global[anchor_player_name] = matched_track_id
                                        
                                        # GROK'S FIX: Update Re-ID tracker confidence for this track (max confidence for anchors)
                                        if reid_tracker is not None and hasattr(reid_tracker, 'update_confidence'):
                                            try:
                                                reid_tracker.update_confidence(matched_track_id, 1.0)
                                                if current_frame_num <= 10:
                                                    print(f"  ðŸ” DEBUG: Updated Re-ID confidence for Track #{matched_track_id} to 1.0 (anchor)")
                                            except:
                                                pass  # Re-ID tracker might not have this method
                                        
                                        # ðŸŽ¯ CONFIDENCE BOOST NEAR ANCHORS: Boost confidence for detections near anchor bboxes
                                        # This improves repeatability by giving higher confidence to tracks near anchor points
                                        if best_match_idx is not None and best_iou > 0:
                                            # Boost confidence based on IoU with anchor bbox
                                            # Formula: conf = min(1.0, conf + 0.2 * iou)
                                            confidence_boost = min(0.2, 0.2 * best_iou)  # Max boost of 0.2
                                            if matched_track_id in track_name_confidence:
                                                current_conf_data = track_name_confidence[matched_track_id]
                                                if isinstance(current_conf_data, tuple) and len(current_conf_data) >= 2:
                                                    current_conf = current_conf_data[1]
                                                    boosted_conf = min(1.0, current_conf + confidence_boost)
                                                    if boosted_conf > current_conf and current_frame_num % 100 == 0:
                                                        print(f"  ðŸŽ¯ CONFIDENCE BOOST: Track #{matched_track_id} '{anchor_player_name}' near anchor (IoU={best_iou:.3f}): {current_conf:.2f} â†’ {boosted_conf:.2f}")
                                                    # Update confidence (keep same structure)
                                                    track_name_confidence[matched_track_id] = (anchor_player_name, boosted_conf, current_frame_num)
                                        
                                        # CRITICAL: Set protection window extending to END of video
                                        # Anchor-protected players should maintain protection for entire video
                                        protection_start = max(0, current_frame_num - ANCHOR_DECAY_FRAMES)
                                        # Extend protection to end of video (or very large number if total_frames not known yet)
                                        protection_end = total_frames if 'total_frames' in locals() and total_frames > 0 else current_frame_num + 10000
                                        track_anchor_protection[matched_track_id] = (anchor_player_name, protection_start, protection_end)
                                        
                                        # DEBUG: Log ID transfer
                                        if current_frame_num <= 10 or current_frame_num % 100 == 0:
                                            print(f"  âœ… ANCHOR APPLIED: Frame {current_frame_num}, Track #{matched_track_id} = '{anchor_player_name}' (confidence: 1.00)")
                                            print(f"  ðŸ” DEBUG: Post-anchor IDs - Track #{matched_track_id}: '{anchor_player_name}'")
                                        
                                        anchors_applied += 1
                                        # Track successful anchor match
                                        if 'anchor_frame_stats' in locals():
                                            if anchor_player_name not in anchor_frame_stats['successful_matches']:
                                                anchor_frame_stats['successful_matches'][anchor_player_name] = 0
                                            anchor_frame_stats['successful_matches'][anchor_player_name] += 1
                                            anchor_frame_stats['protected_tracks'].add(matched_track_id)
                                    else:
                                        # DEBUG: Log failed matches (ALWAYS log for anchor protection debugging)
                                        print(f"  âš  ANCHOR MATCH FAILED: '{anchor_player_name}' (track_id={anchor_track_id}, bbox={'present' if anchor_bbox else 'missing'}) - NO PROTECTION APPLIED")
                                        print(f"     â†’ This player will NOT be protected from Re-ID override")
                                        print(f"     â†’ Possible causes:")
                                        if anchor_track_id is None:
                                            print(f"        â€¢ Anchor has no track_id - matching relies on bbox IoU/distance")
                                        if anchor_bbox is None:
                                            print(f"        â€¢ Anchor has no bbox - cannot match via IoU or distance")
                                        if detections is None or len(detections) == 0:
                                            print(f"        â€¢ No detections in this frame")
                                        elif detections.tracker_id is None:
                                            print(f"        â€¢ Tracker hasn't assigned IDs yet")
                                        else:
                                            active_tids = [int(tid) for tid in detections.tracker_id if tid is not None]
                                            print(f"        â€¢ Active track IDs: {active_tids}")
                                            if anchor_track_id is not None:
                                                print(f"        â€¢ Anchor expected track_id={anchor_track_id}, but it's not in active tracks")
                                        
                                        # Track failed anchor match
                                        if 'anchor_frame_stats' in locals():
                                            if anchor_player_name not in anchor_frame_stats['failed_matches']:
                                                anchor_frame_stats['failed_matches'][anchor_player_name] = 0
                                            anchor_frame_stats['failed_matches'][anchor_player_name] += 1
                            
                            # GROK'S FIX: Confidence refresh rule - boost confidence for anchored tracks
                            # This ensures anchored tracks maintain high confidence even as they continue
                            if detections is not None and detections.tracker_id is not None:
                                for det_idx, track_id in enumerate(detections.tracker_id):
                                    if track_id is not None:
                                        track_id_int = int(track_id)
                                        # Check if this track is anchored
                                        if track_id_int in track_anchor_assigned:
                                            # Boost confidence for anchored tracks (refresh rule)
                                            if track_id_int in track_name_confidence:
                                                # Handle both tuple format and legacy float format
                                                conf_data = track_name_confidence[track_id_int]
                                                if isinstance(conf_data, tuple) and len(conf_data) >= 2:
                                                    player_name, current_conf, _ = conf_data
                                                elif isinstance(conf_data, (int, float)):
                                                    # Legacy format: just a float confidence value
                                                    # Get player name from track_anchor_assigned or player_names
                                                    track_id_str = str(track_id_int)
                                                    player_name = None
                                                    if track_id_int in track_anchor_assigned:
                                                        player_name, _ = track_anchor_assigned[track_id_int]
                                                    elif track_id_str in player_names:
                                                        player_name = player_names[track_id_str]
                                                    else:
                                                        continue  # Can't boost without player name
                                                    current_conf = float(conf_data)
                                                else:
                                                    continue  # Skip if format is unexpected
                                                
                                                # Boost confidence by 0.2 (capped at 1.0) for anchored tracks
                                                boosted_conf = min(1.0, current_conf + 0.2)
                                                track_name_confidence[track_id_int] = (player_name, boosted_conf, current_frame_num)
                                                
                                                # Also update detection confidence if available
                                                if hasattr(detections, 'confidence') and detections.confidence is not None and det_idx < len(detections.confidence):
                                                    detections.confidence[det_idx] = max(detections.confidence[det_idx], boosted_conf)
                                                
                                                if current_frame_num <= 10:
                                                    print(f"  ðŸ” DEBUG: Confidence refresh - Track #{track_id_int} '{player_name}': {current_conf:.2f} â†’ {boosted_conf:.2f} (anchored)")
                        
                        # DEBUG: Log post-anchor state (first 10 frames)
                        if current_frame_num <= 10 and detections is not None and detections.tracker_id is not None:
                            post_anchor_ids = {}
                            for tid in detections.tracker_id:
                                if tid is not None:
                                    tid_str = str(int(tid))
                                    post_anchor_ids[int(tid)] = player_names.get(tid_str, "Unknown")
                            print(f"ðŸ” DEBUG Frame {current_frame_num}: Post-anchor track IDs: {post_anchor_ids}")
                        
                        # APPLY GALLERY MATCHES: Now that tracker has assigned IDs, apply cached gallery matches
                        # CRITICAL: Only apply ONCE per track - never overwrite an existing real name
                        # CRITICAL: NEVER override anchor-protected tracks - anchor frames are ground truth
                        # JERSEY CONSTRAINT: Enforce uniqueness - only one player per jersey number GLOBALLY (across entire video)
                        if 'gallery_match_cache' in locals() and len(gallery_match_cache) > 0 and detections.tracker_id is not None:
                            # Track which track IDs are currently active (visible in this frame)
                            active_track_ids = set(int(tid) for tid in detections.tracker_id if tid is not None)
                            
                            # ENHANCEMENT: Breadcrumb-based re-ID - collect Re-ID features before occlusions
                            current_frame_num = frame_data.get('frame_num', 0)
                            for track_id_int in active_track_ids:
                                # Update last seen time
                                track_last_seen[track_id_int] = current_frame_num
                                
                                # Collect breadcrumbs (Re-ID features) for tracks with names
                                track_id_str = str(track_id_int)
                                if track_id_str in player_names and reid_tracker is not None:
                                    # Find detection index for this track
                                    detection_idx = None
                                    for idx, tid in enumerate(detections.tracker_id):
                                        if tid == track_id_int:
                                            detection_idx = idx
                                            break
                                    
                                    if detection_idx is not None and detection_idx < len(detections.xyxy):
                                        try:
                                            # Extract Re-ID feature for breadcrumb
                                            single_bbox = detections.xyxy[detection_idx:detection_idx+1]
                                            single_detection = sv.Detections(xyxy=single_bbox)
                                            
                                            # Get frame for Re-ID
                                            frame_for_reid = batch_frame
                                            original_frame = frame_data.get('original_frame_for_learning', None)
                                            if original_frame is not None:
                                                frame_for_reid = original_frame
                                            
                                            # Extract feature
                                            breadcrumb_feature = reid_tracker.extract_features(
                                                frame_for_reid, single_detection, team_colors, ball_colors)
                                            
                                            if breadcrumb_feature is not None and len(breadcrumb_feature) > 0:
                                                # Initialize breadcrumb deque if needed
                                                if track_id_int not in track_reid_breadcrumbs:
                                                    track_reid_breadcrumbs[track_id_int] = deque(maxlen=BREADCRUMB_HISTORY)
                                                
                                                # Store breadcrumb (feature, frame_num, bbox)
                                                bbox = detections.xyxy[detection_idx].tolist()
                                                track_reid_breadcrumbs[track_id_int].append((
                                                    breadcrumb_feature[0],  # Feature vector
                                                    current_frame_num,      # Frame number
                                                    bbox                     # Bounding box
                                                ))
                                        except Exception as e:
                                            # Silently fail - breadcrumbs are optional
                                            pass
                            
                            # Clean up old breadcrumbs (remove tracks that haven't been seen in a while)
                            tracks_to_remove = []
                            for track_id_int, last_seen_frame in track_last_seen.items():
                                if current_frame_num - last_seen_frame > BREADCRUMB_MAX_AGE_FRAMES:
                                    tracks_to_remove.append(track_id_int)
                            for track_id_int in tracks_to_remove:
                                if track_id_int in track_reid_breadcrumbs:
                                    del track_reid_breadcrumbs[track_id_int]
                                if track_id_int in track_last_seen:
                                    del track_last_seen[track_id_int]
                            
                            # Build reverse mapping: track_id -> jersey_number for currently assigned tracks
                            track_to_jersey = {}
                            for jersey, track_id in jersey_to_track_global.items():
                                track_to_jersey[track_id] = jersey
                            
                            # TEAM PERSISTENCE: Build mapping of track_id -> team for currently assigned tracks
                            # This allows us to enforce team persistence (Gray track stays Gray)
                            # CRITICAL: Only include tracks with non-None teams - None teams break persistence logic
                            track_to_team_persistent = {}
                            for track_id, team in track_to_team_global.items():
                                if team is not None:  # Only store persistent teams that are actually assigned
                                    track_to_team_persistent[track_id] = team
                            
                            # ðŸŽ¯ IDENTITY TRACKER: Clear inactive tracks
                            if identity_tracker is not None:
                                identity_tracker.clear_inactive_tracks(list(active_track_ids), current_frame_num)
                            
                            # PLAYER-TO-TRACK RELEASE: Clear mappings for inactive tracks to allow reconnection
                            # This handles cases where Rocco Track #3 disappears, then reappears as Track #15
                            # CRITICAL: Clear player_to_track_global for inactive tracks (but preserve anchor protection for spatial recovery)
                            players_to_clear = []
                            for player_name, assigned_track_id in list(player_to_track_global.items()):
                                if assigned_track_id not in active_track_ids:
                                    # Track is no longer active - check if we should clear the mapping
                                    # CRITICAL: NEVER clear anchor-protected players - they must maintain their identity
                                    is_permanently_protected = assigned_track_id in track_anchor_assigned
                                    # Also check if player has anchor protection windows active
                                    has_active_protection = False
                                    if player_name in player_anchor_protection:
                                        for prot_start, prot_end, _, _ in player_anchor_protection[player_name]:
                                            if prot_start <= current_frame_num <= prot_end:
                                                has_active_protection = True
                                                break
                                    
                                    # Only clear if track is NOT permanently protected AND has no active protection
                                    if not is_permanently_protected and not has_active_protection:
                                        players_to_clear.append((player_name, assigned_track_id))
                                    else:
                                        # Anchor-protected player - keep mapping for spatial recovery
                                        if frame_data.get('frame_num', 0) % 100 == 0:
                                            print(f"  ðŸ›¡ï¸ PRESERVED: '{player_name}' on inactive Track #{assigned_track_id} - anchor-protected, keeping for spatial recovery")
                            
                            # Clear inactive track mappings to allow reconnection (only non-anchor-protected)
                            for player_name, old_track_id in players_to_clear:
                                del player_to_track_global[player_name]
                                old_track_str = str(int(old_track_id))
                                if old_track_str in player_names:
                                    player_names[old_track_str] = ""  # Clear name from old track
                                if frame_data.get('frame_num', 0) % 50 == 0:
                                    print(f"  ðŸ”„ TRACK LOST: Cleared '{player_name}' from inactive Track #{old_track_id} (allowing reconnection)")
                            
                            # JERSEY RELEASE: Allow jersey reassignment if the original track is no longer active
                            # This handles cases where Rocco #6 Track #3 disappears, then reappears as Track #15
                            jerseys_available_for_reassignment = set()
                            for jersey, assigned_track_id in list(jersey_to_track_global.items()):
                                # If the track that owns this jersey is not currently visible...
                                if assigned_track_id not in active_track_ids:
                                    # Check if the track has a name in player_names (it's a real assignment)
                                    assigned_pid_str = str(int(assigned_track_id))
                                    if assigned_pid_str in player_names:
                                        # Track is inactive but was previously named - allow reassignment
                                        jerseys_available_for_reassignment.add(jersey)
                                        if frame_data.get('frame_num', 0) % 100 == 0:
                                            print(f"  ðŸ”“ Jersey #{jersey} available for reassignment (Track #{assigned_track_id} inactive)")
                            
                            # JERSEY PERSISTENCE: If a track had a jersey before and is still active, prefer matches with that same jersey
                            # This prevents Rocco #6 from being reassigned to Gunnar #5 due to Re-ID confusion
                            for detection_idx, match_data in gallery_match_cache.items():
                                if detection_idx < len(detections.tracker_id):
                                    track_id = detections.tracker_id[detection_idx]
                                    if track_id is not None:
                                        track_id_int = int(track_id)
                                        
                                        # ðŸ›¡ï¸ CRITICAL: Skip anchor-protected tracks - anchor frames are ground truth
                                        # Check if this track is protected by anchor frames
                                        is_anchor_protected = False
                                        if track_id_int in track_anchor_protection:
                                            prot_name, prot_start, prot_end = track_anchor_protection[track_id_int]
                                            if prot_start <= current_frame_num <= prot_end:
                                                is_anchor_protected = True
                                                if frame_data.get('frame_num', 0) % 100 == 0:
                                                    print(f"  ðŸ›¡ï¸ GALLERY BLOCKED: Track #{track_id_int} is anchor-protected for '{prot_name}' - skipping gallery match")
                                        
                                        # Also check if track has 1.00 confidence (anchor frame assignment)
                                        if not is_anchor_protected and track_id_int in track_name_confidence:
                                            _, confidence, _ = track_name_confidence[track_id_int]
                                            if confidence >= 1.00:
                                                is_anchor_protected = True
                                                if frame_data.get('frame_num', 0) % 100 == 0:
                                                    print(f"  ðŸ›¡ï¸ GALLERY BLOCKED: Track #{track_id_int} has anchor frame confidence (1.00) - skipping gallery match")
                                        
                                        # SEED FRAME: Also check if this track is eligible for gallery learning (high-confidence)
                                        is_eligible_for_learning = False
                                        if should_force_gallery_learning_only:
                                            # Check if track is in eligible_tracks (from seed frame logic)
                                            if 'eligible_tracks' in locals():
                                                eligible_track_ids = [t[0] for t in eligible_tracks]
                                                if track_id_int in eligible_track_ids:
                                                    is_eligible_for_learning = True
                                                    if frame_data.get('frame_num', 0) % 100 == 0:
                                                        # Find track info from eligible_tracks
                                                        track_info = next((t for t in eligible_tracks if t[0] == track_id_int), None)
                                                        if track_info:
                                                            track_type = track_info[3]
                                                            print(f"  ðŸŽ“ SEED FRAME LEARNING: Track #{track_id_int} eligible for gallery update (type: {track_type}, confidence: {track_info[2]:.2f})")
                                        
                                        if is_anchor_protected or is_eligible_for_learning:
                                            # ðŸŽ“ RE-ID LEARNING: Update gallery from anchor-protected tracks (ground truth)
                                            # Also update from high-confidence tracks during seed frames
                                            # These are locked anchors or consistently identified - we know they're correct, so use them for gallery learning
                                            if player_gallery is not None and reid_features is not None and detection_idx < len(reid_features):
                                                try:
                                                    protected_feature = reid_features[detection_idx]
                                                    
                                                    # Only update if feature is valid (no NaN)
                                                    if not np.isnan(protected_feature).any():
                                                        # Get the protected player name
                                                        protected_player_name = None
                                                        if track_id_int in track_anchor_protection:
                                                            protected_player_name, _, _ = track_anchor_protection[track_id_int]
                                                        elif track_id_int in track_name_confidence:
                                                            protected_player_name, _, _ = track_name_confidence[track_id_int]
                                                        elif track_id_int in track_confirmed_identity:
                                                            # For confirmed identity tracks, get name from track_confirmed_identity
                                                            protected_player_name, _, _, _ = track_confirmed_identity[track_id_int]
                                                        
                                                        # CRITICAL: Extract player name from list format if needed
                                                        # Sometimes player_name is stored as ['Name', 'Team', ''] instead of 'Name'
                                                        if isinstance(protected_player_name, list) and len(protected_player_name) > 0:
                                                            protected_player_name = str(protected_player_name[0]).strip()
                                                        elif protected_player_name:
                                                            protected_player_name = str(protected_player_name).strip()
                                                        
                                                        if protected_player_name:
                                                            # CRITICAL: Exclude Ellie Hill and Cameron Hill from gallery updates for this video
                                                            # These players are likely incorrectly matched in this video
                                                            protected_player_name_clean = extract_player_name(protected_player_name)
                                                            excluded_players = ['Ellie Hill', 'Cameron Hill']
                                                            if protected_player_name_clean in excluded_players:
                                                                if frame_data.get('frame_num', 0) % 100 == 0:
                                                                    print(f"  ðŸš« GALLERY UPDATE SKIPPED: '{protected_player_name_clean}' excluded from gallery updates for this video (likely incorrect matches)")
                                                                # Skip gallery update for excluded players
                                                            else:
                                                                # Find player ID in gallery (use normalized name for matching)
                                                                player_id = None
                                                                for pid, profile in player_gallery.players.items():
                                                                    # Normalize both names for comparison
                                                                    profile_name_clean = extract_player_name(profile.name)
                                                                    if profile_name_clean == protected_player_name_clean:
                                                                        player_id = pid
                                                                        break
                                                                
                                                                # If not found, create new player with normalized name
                                                                if player_id is None:
                                                                    player_id = protected_player_name_clean.lower().replace(" ", "_")
                                                                    player_gallery.add_player(
                                                                        name=protected_player_name_clean,
                                                                        player_id=player_id
                                                                    )
                                                                
                                                                if player_id is not None:
                                                                    # Get dominant color if available
                                                                    learned_dominant_color = None
                                                                    if 'detection_dominant_colors' in locals() and detection_idx < len(detection_dominant_colors):
                                                                        learned_dominant_color = detection_dominant_colors[detection_idx]
                                                                    
                                                                    # Extract uniform info
                                                                    uniform_info_for_update = None
                                                                    if 'uniform_info_list' in locals() and detection_idx < len(uniform_info_list):
                                                                        uniform_info_for_update = uniform_info_list[detection_idx]
                                                                    
                                                                    # Extract body and jersey features for highest quality images
                                                                    body_features_for_gallery = None
                                                                    jersey_features_for_gallery = None
                                                                    if reid_tracker is not None and detection_idx < len(detections) and YOLO_AVAILABLE:
                                                                        try:
                                                                            # Get frame for Re-ID
                                                                            frame_for_reid = batch_frame
                                                                            original_frame = frame_data.get('original_frame_for_learning', None)
                                                                            if original_frame is not None:
                                                                                frame_for_reid = original_frame
                                                                            elif roi_bounds is not None:
                                                                                full_frame = frame_data.get('full_frame', None)
                                                                                if full_frame is not None:
                                                                                    frame_for_reid = full_frame
                                                                            
                                                                            # Create single detection for this player
                                                                            single_bbox = detections.xyxy[detection_idx:detection_idx+1]
                                                                            single_detection = sv.Detections(xyxy=single_bbox)
                                                                            
                                                                            # Extract body features (full bbox) - anchor-protected tracks are high quality
                                                                            body_features_for_gallery = reid_tracker.extract_body_features(
                                                                                frame_for_reid, single_detection)
                                                                            if body_features_for_gallery is not None and len(body_features_for_gallery) > 0:
                                                                                body_features_for_gallery = body_features_for_gallery[0]
                                                                            
                                                                            # Extract jersey features (torso region) - anchor-protected tracks are high quality
                                                                            jersey_features_for_gallery = reid_tracker.extract_jersey_features(
                                                                                frame_for_reid, single_detection)
                                                                            if jersey_features_for_gallery is not None and len(jersey_features_for_gallery) > 0:
                                                                                jersey_features_for_gallery = jersey_features_for_gallery[0]
                                                                        except Exception as e:
                                                                            # Don't fail if feature extraction fails
                                                                            if frame_data.get('frame_num', 0) % 200 == 0:
                                                                                print(f"  âš  Could not extract body/jersey features for anchor-protected track: {e}")
                                                                    
                                                                    # Update gallery with anchor-protected track features (ground truth - highest priority)
                                                                    player_gallery.update_player(
                                                                        player_id=player_id,
                                                                        features=protected_feature.reshape(1, -1),
                                                                        dominant_color=learned_dominant_color,
                                                                        reference_frame={
                                                                            'frame_num': frame_data.get('frame_num', 0),
                                                                            'video_path': input_path,
                                                                            'bbox': detections.xyxy[detection_idx_for_learning].tolist() if detection_idx_for_learning is not None and detection_idx_for_learning < len(detections.xyxy) else None,
                                                                            'similarity': 1.00,  # Anchor-protected tracks are ground truth
                                                                            'confidence': 1.00,  # Highest confidence
                                                                            'is_anchor': True  # Mark as anchor
                                                                        },
                                                                        uniform_info=uniform_info_for_update,
                                                                        body_features=body_features_for_gallery,
                                                                        jersey_features=jersey_features_for_gallery
                                                                    )
                                                                    
                                                                    if frame_data.get('frame_num', 0) % 100 == 0:
                                                                        print(f"  ðŸŽ“ GALLERY UPDATE: Updated gallery for '{protected_player_name}' from anchor-protected Track #{track_id_int} (ground truth, similarity: 1.00)")
                                                except Exception as e:
                                                    # Silently fail - learning is optional
                                                    if frame_data.get('frame_num', 0) % 200 == 0:
                                                        print(f"  âš  Could not update gallery from anchor-protected track: {e}")
                                            
                                            continue  # Skip this track - anchor protection takes priority
                                        
                                        # Get persistent team for this track (if any)
                                        persistent_team = track_to_team_persistent.get(track_id_int, None)
                                        
                                        # If this track previously had a jersey, boost matches with that jersey in all_matches
                                        if track_id_int in track_to_jersey and detection_idx in all_matches_per_detection:
                                            previous_jersey = track_to_jersey[track_id_int]
                                            # Find match with previous jersey and boost it to the top
                                            for i, alt_match in enumerate(all_matches_per_detection[detection_idx]):
                                                # Unpack alternative match (pname, pjersey, pteam, sim)
                                                if len(alt_match) == 4:
                                                    alt_name, alt_jersey, alt_team, alt_sim = alt_match
                                                else:
                                                    # Backward compatibility: old format (pname, pjersey, sim)
                                                    alt_name, alt_jersey, alt_sim = alt_match[:3]
                                                    alt_team = None
                                                
                                                # CRITICAL FIX: Check global player constraint BEFORE boosting
                                                # If this player is already assigned to a different active track, DON'T boost
                                                # This prevents "Jax Derryberry" from being boosted for all tracks
                                                player_already_assigned = False
                                                if alt_name and alt_name in player_to_track_global:
                                                    assigned_track = player_to_track_global[alt_name]
                                                    if assigned_track != track_id_int and assigned_track in active_track_ids:
                                                        # Player is already on a different active track - don't boost
                                                        player_already_assigned = True
                                                        if frame_data.get('frame_num', 0) % 100 == 0:
                                                            print(f"  ðŸš« BLOCKED BOOST: {alt_name} already on Track #{assigned_track}, skipping boost for Track #{track_id_int}")
                                                
                                                # Check if this match has the same jersey AND same team (if track has persistent team)
                                                jersey_match = (alt_jersey == previous_jersey)
                                                # ENHANCED: Normalize team names for comparison (handles "Gray"/"Blue" vs "team1"/"team2")
                                                team_match = (not persistent_team or teams_match(alt_team, persistent_team, team_colors) if team_colors else alt_team == persistent_team)
                                                is_coach_match = alt_name and any(coach.lower() in alt_name.lower() for coach in coach_names)
                                                
                                                # CRITICAL FIX: Require higher similarity (0.5) AND gallery threshold for jersey persistence boost
                                                # Also require that player is NOT already assigned to a different active track
                                                # This prevents weak matches from being boosted incorrectly (e.g., all tracks getting "Jax Derryberry")
                                                if not player_already_assigned and jersey_match and team_match and not is_coach_match and alt_sim >= 0.5 and alt_sim >= gallery_similarity_threshold:
                                                    # Boost this match by moving it to the front
                                                    all_matches_per_detection[detection_idx].pop(i)
                                                    all_matches_per_detection[detection_idx].insert(0, (alt_name, alt_jersey, alt_team, alt_sim))
                                                    # Also update the primary match in gallery_match_cache
                                                    gallery_match_cache[detection_idx] = (alt_name, alt_jersey, alt_team, alt_sim)
                                                    if frame_data.get('frame_num', 0) % 100 == 0:
                                                        persistence_info = f" (jersey persistence"
                                                        if persistent_team:
                                                            persistence_info += f" + team persistence: {persistent_team}"
                                                        persistence_info += ")"
                                                        print(f"  ðŸ”„ Track #{track_id}: Boosted {alt_name} #{alt_jersey}{persistence_info}")
                                                    break
                                        
                                        # TEAM PERSISTENCE: If track has persistent team but no jersey match, still prefer same team
                                        if persistent_team and detection_idx in all_matches_per_detection:
                                            # Find match with same team and boost it
                                            for i, alt_match in enumerate(all_matches_per_detection[detection_idx]):
                                                if len(alt_match) == 4:
                                                    alt_name, alt_jersey, alt_team, alt_sim = alt_match
                                                else:
                                                    alt_name, alt_jersey, alt_sim = alt_match[:3]
                                                    alt_team = None
                                                
                                                # CRITICAL FIX: Check global player constraint BEFORE boosting
                                                # If this player is already assigned to a different active track, DON'T boost
                                                player_already_assigned = False
                                                if alt_name and alt_name in player_to_track_global:
                                                    assigned_track = player_to_track_global[alt_name]
                                                    if assigned_track != track_id_int and assigned_track in active_track_ids:
                                                        # Player is already on a different active track - don't boost
                                                        player_already_assigned = True
                                                        if frame_data.get('frame_num', 0) % 100 == 0:
                                                            print(f"  ðŸš« BLOCKED BOOST (team): {alt_name} already on Track #{assigned_track}, skipping boost for Track #{track_id_int}")
                                                
                                                # Check if this match has the same team (and not a coach)
                                                is_coach_match = alt_name and any(coach.lower() in alt_name.lower() for coach in coach_names)
                                                # ENHANCED: Normalize team names for comparison (handles "Gray"/"Blue" vs "team1"/"team2")
                                                team_matches = teams_match(alt_team, persistent_team, team_colors) if team_colors else (alt_team == persistent_team)
                                                # CRITICAL FIX: Require higher similarity (0.5) AND gallery threshold for team persistence boost
                                                # Also require that player is NOT already assigned to a different active track
                                                # This prevents weak matches from being boosted incorrectly
                                                if not player_already_assigned and team_matches and not is_coach_match and alt_sim >= 0.5 and alt_sim >= gallery_similarity_threshold:
                                                    # Boost this match (but not as much as jersey+team match)
                                                    # Move to second position if not already first
                                                    if i > 0:
                                                        all_matches_per_detection[detection_idx].pop(i)
                                                        all_matches_per_detection[detection_idx].insert(1, (alt_name, alt_jersey, alt_team, alt_sim))
                                                        if frame_data.get('frame_num', 0) % 100 == 0:
                                                            print(f"  ðŸ”’ Track #{track_id}: Boosted {alt_name} (team persistence: {persistent_team})")
                                                    break
                            
                            # ðŸ›¡ï¸ PROTECTION VERIFICATION: Log any anchor-protected tracks before gallery matching
                            # This helps diagnose if protection is being bypassed
                            current_frame_verify = frame_data.get('frame_num', 0)
                            if current_frame_verify % 500 == 0 or current_frame_verify < 10:
                                anchor_protected_tracks = []
                                for tid, (pname, pstart, pend) in track_anchor_protection.items():
                                    if pstart <= current_frame_verify <= pend:
                                        anchor_protected_tracks.append((tid, pname, pstart, pend))
                                if anchor_protected_tracks:
                                    print(f"  ðŸ” PROTECTION VERIFY: Frame {current_frame_verify} - {len(anchor_protected_tracks)} anchor-protected track(s): {[(t, n) for t, n, _, _ in anchor_protected_tracks]}")
                                
                                confidence_1_tracks = []
                                for tid, (pname, conf, _) in track_name_confidence.items():
                                    if conf >= 1.00:
                                        confidence_1_tracks.append((tid, pname, conf))
                                if confidence_1_tracks:
                                    print(f"  ðŸ” CONFIDENCE VERIFY: Frame {current_frame_verify} - {len(confidence_1_tracks)} track(s) with confidence 1.00: {[(t, n) for t, n, _ in confidence_1_tracks]}")
                            
                            # ðŸ• TEMPORAL CONFIDENCE: Frame look-ahead and look-back for player identity reaffirmation
                            # This helps maintain player identity across occlusions and track changes
                            # Store current frame player identity for temporal analysis
                            current_frame_num = frame_data.get('frame_num', 0)
                            if current_frame_num not in temporal_player_history:
                                temporal_player_history[current_frame_num] = {}
                            
                            # Store player identity from current frame (before applying gallery matches)
                            for track_id_int in active_track_ids:
                                track_id_str = str(track_id_int)
                                player_name = player_names.get(track_id_str, "")
                                confidence = 0.0
                                if track_id_int in track_name_confidence:
                                    _, confidence, _ = track_name_confidence[track_id_int]
                                
                                # Get bbox and other info
                                bbox = None
                                team = None
                                jersey = None
                                if detections is not None and detections.tracker_id is not None:
                                    for det_idx, tid in enumerate(detections.tracker_id):
                                        if tid is not None and int(tid) == track_id_int:
                                            if det_idx < len(detections.xyxy):
                                                bbox = detections.xyxy[det_idx].tolist()
                                            break
                                
                                if track_id_int in track_to_team_global:
                                    team = track_to_team_global[track_id_int]
                                if track_id_int in track_to_jersey:
                                    jersey = track_to_jersey[track_id_int]
                                
                                if player_name or confidence > 0:
                                    temporal_player_history[current_frame_num][track_id_int] = {
                                        'player_name': player_name,
                                        'confidence': confidence,
                                        'bbox': bbox,
                                        'team': team,
                                        'jersey': jersey
                                    }
                            
                            # Clean up old temporal history (keep last 5 seconds)
                            frames_to_cleanup = [f for f in temporal_player_history.keys() 
                                                 if current_frame_num - f > MAX_TEMPORAL_HISTORY]
                            for f in frames_to_cleanup:
                                if f in temporal_player_history:
                                    del temporal_player_history[f]
                            
                            # Apply matches with GLOBAL jersey and team constraints
                            for detection_idx, match_data in gallery_match_cache.items():
                                # Unpack match data (player_name, jersey_number, team, similarity)
                                if len(match_data) == 4:
                                    player_name, jersey_number, player_team, similarity = match_data
                                else:
                                    # Backward compatibility: old format (player_name, jersey_number, similarity)
                                    player_name, jersey_number, similarity = match_data[:3]
                                    player_team = None
                                if detection_idx < len(detections.tracker_id):
                                    track_id = detections.tracker_id[detection_idx]
                                    if track_id is not None:
                                        pid_str = str(int(track_id))
                                        track_id_int = int(track_id)
                                        
                                        # ðŸ• TEMPORAL CONFIDENCE: Look back and ahead to reaffirm player identity
                                        # This helps maintain identity across occlusions and track changes
                                        temporal_confidence_boost = 0.0
                                        temporal_evidence_count = 0
                                        
                                        # LOOK BACK: Check previous frames to see if this player was on this track or nearby
                                        for lookback_frame in range(max(0, current_frame_num - TEMPORAL_LOOK_BACK_FRAMES), current_frame_num):
                                            if lookback_frame in temporal_player_history:
                                                frame_data_lookback = temporal_player_history[lookback_frame]
                                                
                                                # Check if this track had the same player in previous frame
                                                if track_id_int in frame_data_lookback:
                                                    prev_data = frame_data_lookback[track_id_int]
                                                    if prev_data['player_name'] == player_name:
                                                        # Same player on same track - strong temporal evidence
                                                        temporal_confidence_boost += 0.15
                                                        temporal_evidence_count += 1
                                                        if current_frame_num % 100 == 0:
                                                            print(f"  ðŸ• TEMPORAL (look-back): Track #{track_id_int} '{player_name}' found in frame {lookback_frame} (confidence: {prev_data['confidence']:.2f})")
                                                
                                                # Check if this player was on a nearby track (track change after occlusion)
                                                for prev_track_id, prev_data in frame_data_lookback.items():
                                                    if prev_data['player_name'] == player_name and prev_track_id != track_id_int:
                                                        # Same player, different track - possible track change
                                                        # Check if bboxes are close (spatial continuity)
                                                        if prev_data['bbox'] is not None and detection_idx < len(detections.xyxy):
                                                            current_bbox = detections.xyxy[detection_idx]
                                                            prev_bbox = prev_data['bbox']
                                                            prev_center_x = (prev_bbox[0] + prev_bbox[2]) / 2
                                                            prev_center_y = (prev_bbox[1] + prev_bbox[3]) / 2
                                                            current_center_x = (current_bbox[0] + current_bbox[2]) / 2
                                                            current_center_y = (current_bbox[1] + current_bbox[3]) / 2
                                                            distance = np.sqrt((current_center_x - prev_center_x)**2 + (current_center_y - prev_center_y)**2)
                                                            
                                                            # If close spatially, this is likely the same player (track changed after occlusion)
                                                            if distance < 300:  # 300px threshold for track change recovery
                                                                temporal_confidence_boost += 0.10
                                                                temporal_evidence_count += 1
                                                                if current_frame_num % 100 == 0:
                                                                    print(f"  ðŸ• TEMPORAL (look-back): Track #{track_id_int} '{player_name}' likely same as Track #{prev_track_id} in frame {lookback_frame} (distance: {distance:.1f}px)")
                                                        
                                                        break  # Found match, no need to check other tracks
                                        
                                        # LOOK AHEAD: Check future frames (if available) to reaffirm identity
                                        # Note: This only works if frames are buffered/processed ahead of time
                                        for lookahead_frame in range(current_frame_num + 1, min(current_frame_num + TEMPORAL_LOOK_AHEAD_FRAMES + 1, total_frames)):
                                            if lookahead_frame in temporal_player_history:
                                                frame_data_lookahead = temporal_player_history[lookahead_frame]
                                                
                                                # Check if this track has the same player in future frame
                                                if track_id_int in frame_data_lookahead:
                                                    future_data = frame_data_lookahead[track_id_int]
                                                    if future_data['player_name'] == player_name:
                                                        # Same player on same track in future - strong temporal evidence
                                                        temporal_confidence_boost += 0.10
                                                        temporal_evidence_count += 1
                                                        if current_frame_num % 100 == 0:
                                                            print(f"  ðŸ• TEMPORAL (look-ahead): Track #{track_id_int} '{player_name}' found in frame {lookahead_frame} (confidence: {future_data['confidence']:.2f})")
                                                
                                                # Check if this player appears on nearby tracks in future (track change prediction)
                                                for future_track_id, future_data in frame_data_lookahead.items():
                                                    if future_data['player_name'] == player_name and future_track_id != track_id_int:
                                                        # Same player, different track in future - possible track change
                                                        if future_data['bbox'] is not None and detection_idx < len(detections.xyxy):
                                                            current_bbox = detections.xyxy[detection_idx]
                                                            future_bbox = future_data['bbox']
                                                            future_center_x = (future_bbox[0] + future_bbox[2]) / 2
                                                            future_center_y = (future_bbox[1] + future_bbox[3]) / 2
                                                            current_center_x = (current_bbox[0] + current_bbox[2]) / 2
                                                            current_center_y = (current_bbox[1] + current_bbox[3]) / 2
                                                            distance = np.sqrt((current_center_x - future_center_x)**2 + (current_center_y - future_center_y)**2)
                                                            
                                                            if distance < 300:  # 300px threshold
                                                                temporal_confidence_boost += 0.08
                                                                temporal_evidence_count += 1
                                                                if current_frame_num % 100 == 0:
                                                                    print(f"  ðŸ• TEMPORAL (look-ahead): Track #{track_id_int} '{player_name}' likely same as Track #{future_track_id} in frame {lookahead_frame} (distance: {distance:.1f}px)")
                                                        
                                                        break  # Found match, no need to check other tracks
                                        
                                        # Apply temporal confidence boost to similarity score
                                        if temporal_evidence_count > 0:
                                            # Boost similarity based on temporal evidence (capped at 1.0)
                                            boosted_similarity = min(1.0, similarity + temporal_confidence_boost)
                                            if boosted_similarity > similarity:
                                                similarity = boosted_similarity
                                                if current_frame_num % 100 == 0:
                                                    print(f"  ðŸ• TEMPORAL BOOST: Track #{track_id_int} '{player_name}' - similarity {similarity:.3f} â†’ {boosted_similarity:.3f} (temporal evidence: {temporal_evidence_count} frames)")
                                        
                                        # ðŸ›¡ï¸ CRITICAL PROTECTION CHECK #1: Check anchor frame status FIRST (before any other logic)
                                        # This must happen BEFORE similarity checks, consensus checks, or any assignment logic
                                        current_frame = frame_data.get('frame_num', 0)
                                        
                                        # Check 1: Does track have anchor frame confidence (1.00)?
                                        is_anchor_frame = False
                                        if track_id_int in track_name_confidence:
                                            _, confidence, _ = track_name_confidence[track_id_int]
                                            if confidence >= 1.00:
                                                is_anchor_frame = True
                                                anchor_name, _, _ = track_name_confidence[track_id_int]
                                                if frame_data.get('frame_num', 0) % 100 == 0:
                                                    print(f"  ðŸ”’ ANCHOR FRAME BLOCK: Track #{track_id_int} has anchor frame '{anchor_name}' (confidence 1.00) - skipping ALL gallery matching")
                                        
                                        # Check 2: Is track in anchor protection window?
                                        # CRITICAL FIX: Extend protection window dynamically if track continues beyond original window
                                        is_anchor_protected = False
                                        if not is_anchor_frame and track_id_int in track_anchor_protection:
                                            prot_name, prot_start, prot_end = track_anchor_protection[track_id_int]
                                            
                                            # CRITICAL FIX: Extend protection to END of video dynamically
                                            # This ensures anchor-assigned tracks maintain protection for entire video
                                            # If track continues beyond protection window, extend it to end of video
                                            if current_frame > prot_end:
                                                # Extend protection to end of video (not just 150 more frames)
                                                new_protection_end = total_frames if 'total_frames' in globals() and total_frames > 0 else current_frame + 10000
                                                track_anchor_protection[track_id_int] = (prot_name, prot_start, new_protection_end)
                                                prot_end = new_protection_end
                                                if frame_data.get('frame_num', 0) % 500 == 0:
                                                    print(f"  ðŸ”‹ PROTECTION EXTENDED: Track #{track_id_int} '{prot_name}' protection extended to frame {prot_end} (end of video)")
                                            
                                            # Check if track is within (now potentially extended) protection window
                                            if prot_start <= current_frame <= prot_end:
                                                is_anchor_protected = True
                                                if frame_data.get('frame_num', 0) % 100 == 0:
                                                    print(f"  ðŸ›¡ï¸ ANCHOR PROTECTION BLOCK: Track #{track_id_int} is in protection window for '{prot_name}' (frames {prot_start}-{prot_end}) - skipping ALL gallery matching")
                                        
                                        # ðŸ›¡ï¸ CRITICAL: Anchor-protected tracks MUST NEVER receive gallery matches
                                        # Anchor frames are ground truth (1.00 confidence) - they define WHO the player is
                                        # Gallery matching is for UNTAGGED players only - anchor-protected players are already identified
                                        # This prevents ANY player from taking an anchor player's name
                                        
                                        # PERMANENT OWNERSHIP CHECK: If track is in track_anchor_assigned, it's PERMANENTLY owned
                                        # Once a track is assigned via anchor frame, it can NEVER be reassigned to a different player
                                        # Anchor matches carry through from anchor point to anchor point - the player OWNS that track
                                        is_permanently_owned = track_id_int in track_anchor_assigned
                                        if is_permanently_owned:
                                            owner_name, owner_frame = track_anchor_assigned[track_id_int]
                                            # HARD BLOCK: Track is permanently owned - skip ALL gallery matching
                                            if frame_data.get('frame_num', 0) % 100 == 0:
                                                print(f"  ðŸ›¡ï¸ PERMANENT OWNERSHIP: Track #{track_id_int} is PERMANENTLY owned by '{owner_name}' (anchor frame {owner_frame}) - BLOCKING ALL gallery matching")
                                            continue  # Skip this track entirely - it's permanently owned
                                        
                                        # ðŸš« ROSTER CHECK: Skip gallery matching for inactive players
                                        if player_name and not is_player_active_in_roster(player_name, video_specific_roster):
                                            if frame_data.get('frame_num', 0) % 100 == 0:
                                                print(f"  ðŸš« ROSTER BLOCK: Player '{player_name}' is inactive in roster - skipping gallery matching")
                                            continue  # Skip inactive players
                                        
                                        # If track is anchor-protected (has 1.00 confidence OR is in protection window), BLOCK ALL gallery matching
                                        # This is a HARD BLOCK - no exceptions, no reconnection logic, no "if name matches" checks
                                        # Anchor frames are the source of truth - if a track is anchor-protected, it's already identified
                                        if is_anchor_frame or is_anchor_protected:
                                            # Get the protected name for logging
                                            protected_name = None
                                            if is_anchor_frame:
                                                protected_name = anchor_name
                                            elif track_id_int in track_anchor_protection:
                                                protected_name, _, _ = track_anchor_protection[track_id_int]
                                            
                                            # DIAGNOSTIC: Always log when anchor-protected track is detected (first 30 frames or every 100)
                                            if current_frame_num <= 30 or current_frame_num % 100 == 0:
                                                print(f"  ðŸ›¡ï¸ ANCHOR PROTECTED: Track #{track_id_int} is anchor-protected for '{protected_name}' (is_anchor_frame={is_anchor_frame}, is_anchor_protected={is_anchor_protected}) - entering learning section")
                                            
                                            # HARD BLOCK: Skip ALL gallery matching for anchor-protected tracks
                                            # This ensures anchor players can NEVER be overridden by gallery matches
                                            if frame_data.get('frame_num', 0) % 100 == 0:
                                                print(f"  ðŸ›¡ï¸ HARD BLOCK: Track #{track_id_int} is anchor-protected for '{protected_name}' - BLOCKING ALL gallery matching (anchor frames are ground truth)")
                                            
                                            # Continue to gallery learning section (update gallery from anchor-protected tracks)
                                            # But DO NOT apply any gallery matches to this track
                                            # ðŸŽ“ AGGRESSIVE RE-ID LEARNING: Update gallery from anchor-protected tracks (ground truth)
                                            # These are locked anchors - we know they're correct, so use them for gallery learning
                                            # CRITICAL: Learn from EVERY frame where anchor-protected players appear (not just occasionally)
                                            # This builds up gallery features for Rocco, Cameron, and Ellie until reaching 100% accuracy
                                            
                                            # CRITICAL FIX: Find detection_idx for this anchor-protected track
                                            # We need to find which detection in the current frame corresponds to this track_id
                                            detection_idx_for_learning = None
                                            if detections is not None and detections.tracker_id is not None:
                                                for idx, tid in enumerate(detections.tracker_id):
                                                    if tid is not None and int(tid) == track_id_int:
                                                        detection_idx_for_learning = idx
                                                        break
                                            
                                            # DIAGNOSTIC: Check if learning conditions are met
                                            learning_conditions_met = (
                                                player_gallery is not None and 
                                                reid_features is not None and 
                                                detection_idx_for_learning is not None and
                                                detection_idx_for_learning < len(reid_features)
                                            )
                                            
                                            if not learning_conditions_met:
                                                missing = []
                                                if player_gallery is None:
                                                    missing.append("player_gallery is None")
                                                if reid_features is None:
                                                    missing.append("reid_features is None")
                                                elif detection_idx_for_learning is None:
                                                    missing.append(f"detection_idx is None (Track #{track_id_int} not found in detections)")
                                                    # Additional diagnostic: show available track IDs
                                                    if detections is not None and detections.tracker_id is not None:
                                                        available_tracks = [int(tid) for tid in detections.tracker_id if tid is not None]
                                                        missing.append(f"available tracks: {available_tracks[:10]}")
                                                elif detection_idx_for_learning >= len(reid_features):
                                                    missing.append(f"detection_idx ({detection_idx_for_learning}) >= len(reid_features) ({len(reid_features)})")
                                                
                                                # ALWAYS log anchor learning skipped messages (not just every 100 frames)
                                                # This is critical for debugging why anchor learning isn't happening
                                                print(f"  âš  ANCHOR LEARNING SKIPPED (Frame {current_frame_num}, Track #{track_id_int}, '{protected_name}'): {', '.join(missing)}")
                                                
                                                # Track that we attempted anchor learning but conditions weren't met
                                                # This helps diagnose why stats are empty
                                                if protected_name:
                                                    protected_name_clean = extract_player_name(protected_name)
                                                    if protected_name_clean and protected_name_clean not in anchor_learning_stats:
                                                        # Initialize stats entry even if learning didn't happen
                                                        # This shows we tried but conditions weren't met
                                                        anchor_learning_stats[protected_name_clean] = {
                                                            'frames_learned': 0,
                                                            'total_features': 0,
                                                            'first_frame': current_frame_num,
                                                            'last_frame': current_frame_num,
                                                            'skipped_reasons': missing.copy()
                                                        }
                                                    elif protected_name_clean in anchor_learning_stats:
                                                        # Update skipped reasons if already exists
                                                        if 'skipped_reasons' not in anchor_learning_stats[protected_name_clean]:
                                                            anchor_learning_stats[protected_name_clean]['skipped_reasons'] = []
                                                        anchor_learning_stats[protected_name_clean]['skipped_reasons'].extend(missing)
                                            
                                            if player_gallery is not None and reid_features is not None and detection_idx_for_learning is not None and detection_idx_for_learning < len(reid_features):
                                                try:
                                                    protected_feature = reid_features[detection_idx_for_learning]
                                                    
                                                    # DIAGNOSTIC: Log when anchor learning is about to happen
                                                    if current_frame_num <= 10 or current_frame_num % 100 == 0:
                                                        print(f"  ðŸŽ“ ANCHOR LEARNING: Frame {current_frame_num}, Track #{track_id_int}, '{protected_name}' - conditions met, updating gallery...")
                                                    
                                                    # Only update if feature is valid (no NaN)
                                                    if not np.isnan(protected_feature).any():
                                                        # Get the protected player name
                                                        protected_player_name = None
                                                        if track_id_int in track_anchor_protection:
                                                            protected_player_name, _, _ = track_anchor_protection[track_id_int]
                                                        elif track_id_int in track_name_confidence:
                                                            protected_player_name, _, _ = track_name_confidence[track_id_int]
                                                        
                                                        # CRITICAL: Extract player name from list format if needed
                                                        # Sometimes player_name is stored as ['Name', 'Team', ''] instead of 'Name'
                                                        if isinstance(protected_player_name, list) and len(protected_player_name) > 0:
                                                            protected_player_name = str(protected_player_name[0]).strip()
                                                        elif protected_player_name:
                                                            protected_player_name = str(protected_player_name).strip()
                                                        
                                                        if protected_player_name:
                                                            # CRITICAL: Exclude Ellie Hill and Cameron Hill from gallery updates for this video
                                                            # These players are likely incorrectly matched in this video
                                                            protected_player_name_clean = extract_player_name(protected_player_name)
                                                            excluded_players = ['Ellie Hill', 'Cameron Hill']
                                                            if protected_player_name_clean in excluded_players:
                                                                if frame_data.get('frame_num', 0) % 100 == 0:
                                                                    print(f"  ðŸš« GALLERY UPDATE SKIPPED: '{protected_player_name_clean}' excluded from gallery updates for this video (likely incorrect matches)")
                                                                # Skip gallery update for excluded players
                                                            else:
                                                                # Find player ID in gallery (use normalized name for matching)
                                                                player_id = None
                                                                for pid, profile in player_gallery.players.items():
                                                                    # Normalize both names for comparison
                                                                    profile_name_clean = extract_player_name(profile.name)
                                                                    if profile_name_clean == protected_player_name_clean:
                                                                        player_id = pid
                                                                        break
                                                                
                                                                # If not found, create new player with normalized name
                                                                if player_id is None:
                                                                    player_id = protected_player_name_clean.lower().replace(" ", "_")
                                                                    player_gallery.add_player(
                                                                        name=protected_player_name_clean,
                                                                        player_id=player_id
                                                                    )
                                                                
                                                                if player_id is not None:
                                                                    # Get dominant color if available
                                                                    learned_dominant_color = None
                                                                    if 'detection_dominant_colors' in locals() and detection_idx_for_learning is not None and detection_idx_for_learning < len(detection_dominant_colors):
                                                                        learned_dominant_color = detection_dominant_colors[detection_idx_for_learning]
                                                                    
                                                                    # Extract uniform info
                                                                    uniform_info_for_update = None
                                                                    if 'uniform_info_list' in locals() and detection_idx_for_learning is not None and detection_idx_for_learning < len(uniform_info_list):
                                                                        uniform_info_for_update = uniform_info_list[detection_idx_for_learning]
                                                                    
                                                                    # Extract body and jersey features for highest quality images
                                                                    body_features_for_gallery = None
                                                                    jersey_features_for_gallery = None
                                                                    # CRITICAL FIX: Use detection_idx_for_learning instead of detection_idx
                                                                    if reid_tracker is not None and detection_idx_for_learning is not None and detection_idx_for_learning < len(detections) and YOLO_AVAILABLE:
                                                                        try:
                                                                            # Get frame for Re-ID
                                                                            frame_for_reid = batch_frame
                                                                            original_frame = frame_data.get('original_frame_for_learning', None)
                                                                            if original_frame is not None:
                                                                                frame_for_reid = original_frame
                                                                            elif roi_bounds is not None:
                                                                                full_frame = frame_data.get('full_frame', None)
                                                                                if full_frame is not None:
                                                                                    frame_for_reid = full_frame
                                                                            
                                                                            # Create single detection for this player
                                                                            # CRITICAL FIX: Use detection_idx_for_learning (the correct index for this anchor-protected track)
                                                                            single_bbox = detections.xyxy[detection_idx_for_learning:detection_idx_for_learning+1]
                                                                            single_detection = sv.Detections(xyxy=single_bbox)
                                                                            
                                                                            # Extract body features (full bbox) - anchor-protected tracks are high quality
                                                                            body_features_for_gallery = reid_tracker.extract_body_features(
                                                                                frame_for_reid, single_detection)
                                                                            if body_features_for_gallery is not None and len(body_features_for_gallery) > 0:
                                                                                body_features_for_gallery = body_features_for_gallery[0]
                                                                            
                                                                            # Extract jersey features (torso region) - anchor-protected tracks are high quality
                                                                            jersey_features_for_gallery = reid_tracker.extract_jersey_features(
                                                                                frame_for_reid, single_detection)
                                                                            if jersey_features_for_gallery is not None and len(jersey_features_for_gallery) > 0:
                                                                                jersey_features_for_gallery = jersey_features_for_gallery[0]
                                                                        except Exception as e:
                                                                            # Don't fail if feature extraction fails
                                                                            if frame_data.get('frame_num', 0) % 200 == 0:
                                                                                print(f"  âš  Could not extract body/jersey features for anchor-protected track: {e}")
                                                                    
                                                                    # Update gallery with anchor-protected track features (ground truth - highest priority)
                                                                    # Calculate bbox quality for anchor frames (larger bboxes = higher quality)
                                                                    bbox_quality = 0.5  # Default
                                                                    if detection_idx_for_learning is not None and detection_idx_for_learning < len(detections.xyxy):
                                                                        bbox = detections.xyxy[detection_idx_for_learning]
                                                                        bbox_width = bbox[2] - bbox[0]
                                                                        bbox_height = bbox[3] - bbox[1]
                                                                        bbox_area = bbox_width * bbox_height
                                                                        # Normalize quality: larger bboxes = higher quality (closer/clearer view)
                                                                        # Typical player bbox: ~4000-15000 pixels, normalize to 0.5-1.0
                                                                        bbox_quality = min(1.0, max(0.5, bbox_area / 10000.0))
                                                                    
                                                                    # Update gallery with anchor-protected track features
                                                                    player_gallery.update_player(
                                                                        player_id=player_id,
                                                                        features=protected_feature.reshape(1, -1),
                                                                        dominant_color=learned_dominant_color,
                                                                        reference_frame={
                                                                            'frame_num': frame_data.get('frame_num', 0),
                                                                            'video_path': input_path,
                                                                            'bbox': detections.xyxy[detection_idx_for_learning].tolist() if detection_idx_for_learning is not None and detection_idx_for_learning < len(detections.xyxy) else None,
                                                                            'similarity': 1.00,  # Anchor-protected tracks are ground truth
                                                                            'confidence': 1.00,  # Highest confidence
                                                                            'quality': bbox_quality,  # Quality based on bbox size (larger = better)
                                                                            'is_anchor': True  # Mark as anchor (ground truth)
                                                                        },
                                                                        uniform_info=uniform_info_for_update,
                                                                        body_features=body_features_for_gallery,
                                                                        jersey_features=jersey_features_for_gallery
                                                                    )
                                                                    
                                                                    # DIAGNOSTIC: Log successful gallery update
                                                                    if current_frame_num <= 10 or current_frame_num % 100 == 0:
                                                                        print(f"  âœ“ ANCHOR LEARNING: Gallery updated for '{protected_player_name_clean}' from Track #{track_id_int} (Frame {current_frame_num})")
                                                                
                                                                # Track learning statistics (use normalized name) - ALWAYS track, even if player_id was None (will be created)
                                                                try:
                                                                    # Use the already-normalized name
                                                                    stats_player_name = protected_player_name_clean
                                                                    
                                                                    if stats_player_name:
                                                                        if stats_player_name not in anchor_learning_stats:
                                                                            anchor_learning_stats[stats_player_name] = {
                                                                                'frames_learned': 0,
                                                                                'total_features': 0,
                                                                                'first_frame': current_frame_num,
                                                                                'last_frame': current_frame_num
                                                                            }
                                                                        anchor_learning_stats[stats_player_name]['frames_learned'] += 1
                                                                        anchor_learning_stats[stats_player_name]['total_features'] += 1
                                                                        anchor_learning_stats[stats_player_name]['last_frame'] = current_frame_num
                                                                        
                                                                        # Log first few updates to verify stats are being tracked
                                                                        if current_frame_num <= 10 or (current_frame_num % 100 == 0 and anchor_learning_stats[stats_player_name]['frames_learned'] <= 10):
                                                                            print(f"  ðŸŽ“ STATS UPDATE: '{stats_player_name}' - {anchor_learning_stats[stats_player_name]['frames_learned']} frames learned (Track #{track_id_int})")
                                                                    
                                                                    # Get current reference frame count for this player (including uniform variants)
                                                                    current_ref_count = player_gallery.get_total_reference_frames(player_id) if player_id else 0
                                                                    
                                                                    # Log learning progress (every 50 frames to show progress without spam)
                                                                    if frame_data.get('frame_num', 0) % 50 == 0:
                                                                        # Use stats_player_name instead of protected_player_name for consistency
                                                                        if stats_player_name in anchor_learning_stats:
                                                                            stats = anchor_learning_stats[stats_player_name]
                                                                            print(f"  ðŸŽ“ LEARNING: '{stats_player_name}' - {stats['frames_learned']} frames learned, {current_ref_count} total ref frames (Track #{track_id_int}, ground truth)")
                                                                except Exception as stats_error:
                                                                    # Log stats tracking errors separately from gallery update errors
                                                                    if frame_data.get('frame_num', 0) % 200 == 0:
                                                                        print(f"  âš  Could not track anchor learning stats for '{protected_player_name}': {stats_error}")
                                                except Exception as e:
                                                    # Silently fail - learning is optional
                                                    if frame_data.get('frame_num', 0) % 200 == 0:
                                                        print(f"  âš  Could not update gallery from anchor-protected track: {e}")
                                                        import traceback
                                                        traceback.print_exc()
                                            
                                            continue  # Skip this track - anchor protection takes priority
                                        
                                        # Track age protection: tracks that have existed longer are more established
                                        current_frame = frame_data.get('frame_num', 0)
                                        if track_id_int not in track_first_seen:
                                            track_first_seen[track_id_int] = current_frame
                                        track_age_frames = current_frame - track_first_seen[track_id_int]
                                        track_age_seconds = track_age_frames / fps if fps > 0 else 0
                                        
                                        # Check if this track already has a real name (not a generic "Player X" name)
                                        existing_name_raw = player_names.get(pid_str, "")
                                        # CRITICAL FIX: Handle case where existing_name might be a list
                                        if isinstance(existing_name_raw, list) and len(existing_name_raw) > 0:
                                            existing_name = str(existing_name_raw[0])
                                        elif existing_name_raw:
                                            existing_name = str(existing_name_raw)
                                        else:
                                            existing_name = ""
                                        has_real_name = existing_name and isinstance(existing_name, str) and not existing_name.strip().startswith("Player ")
                                        
                                        # ANCHOR FRAME TEMPORAL PROTECTION: Check if track is in anchor protection window
                                        # Three-zone system: Hard lock â†’ Soft lock â†’ Decay â†’ Normal
                                        anchor_protection_multiplier = 1.0  # Default: no protection
                                        anchor_protection_zone = None
                                        
                                        if track_id_int in track_anchor_protection:
                                            protected_name_raw, protection_start, protection_end = track_anchor_protection[track_id_int]
                                            # CRITICAL: Normalize protected_name to prevent list format issues
                                            protected_name = extract_player_name(protected_name_raw)
                                            current_frame = frame_data.get('frame_num', 0)
                                            
                                            if protection_start <= current_frame <= protection_end:
                                                # Find the nearest anchor frame center within protection window
                                                # This allows multiple anchor frames to contribute to protection
                                                # Calculate distance from nearest anchor frame
                                                distance_from_anchor = min(
                                                    abs(current_frame - protection_start),
                                                    abs(current_frame - protection_end)
                                                )
                                                # Approximate center of protection window
                                                protection_center = (protection_start + protection_end) // 2
                                                distance_from_center = abs(current_frame - protection_center)
                                                
                                                # ZONE 1: Hard Protection (Â±0-50 frames from center)
                                                if distance_from_center <= ANCHOR_HARD_PROTECTION_FRAMES:
                                                    anchor_protection_zone = "HARD"
                                                    # Complete block: Re-ID cannot override
                                                    # FORCE the protected name - don't let Re-ID change it
                                                    if existing_name != protected_name:
                                                        # CRITICAL: Check if protected_name is already assigned to a different track
                                                        if protected_name in player_to_track_global:
                                                            old_track_id = player_to_track_global[protected_name]
                                                            if old_track_id != track_id_int:
                                                                # Player is on a different track - clear it first
                                                                old_track_str = str(int(old_track_id))
                                                                if old_track_str in player_names:
                                                                    player_names[old_track_str] = ""  # Clear old track
                                                                if old_track_id in track_name_confidence:
                                                                    del track_name_confidence[old_track_id]
                                                                if frame_data.get('frame_num', 0) % 10 == 0:
                                                                    print(f"  ðŸ—‘ï¸ UNIQUENESS (hard prot): Cleared '{protected_name}' from Track #{old_track_id} (moving to Track #{track_id_int})")
                                                        
                                                        old_name = existing_name
                                                        player_names[pid_str] = protected_name
                                                        player_to_track_global[protected_name] = track_id_int
                                                        existing_name = protected_name
                                                        has_real_name = True
                                                        if frame_data.get('frame_num', 0) % 10 == 0:  # More frequent logging
                                                            print(f"  ðŸ›¡ï¸ HARD PROTECTION: Track #{track_id} FORCED '{protected_name}' (was: '{old_name}', {distance_from_center}f from anchor)")
                                                    else:
                                                        if frame_data.get('frame_num', 0) % 100 == 0:
                                                            print(f"  ðŸ›¡ï¸ HARD PROTECTION: Track #{track_id} = {protected_name} ({distance_from_center}f from anchor, cannot override)")
                                                    
                                                    # ðŸŽ“ RE-ID LEARNING: Extract features from protected track and update gallery
                                                    # Protected tracks are ground truth - Re-ID should learn from them, not override them
                                                    if player_gallery is not None and reid_features is not None and detection_idx < len(reid_features):
                                                        try:
                                                            # Get Re-ID features for this protected track
                                                            protected_feature = reid_features[detection_idx]
                                                            
                                                            # Only update if feature is valid (no NaN)
                                                            if not np.isnan(protected_feature).any():
                                                                # CRITICAL: Exclude Ellie Hill and Cameron Hill from gallery updates for this video
                                                                # These players are likely incorrectly matched in this video
                                                                protected_name_clean = extract_player_name(protected_name)
                                                                excluded_players = ['Ellie Hill', 'Cameron Hill']
                                                                if protected_name_clean in excluded_players:
                                                                    if frame_data.get('frame_num', 0) % 100 == 0:
                                                                        print(f"  ðŸš« GALLERY UPDATE SKIPPED: '{protected_name_clean}' excluded from gallery updates for this video (likely incorrect matches)")
                                                                    # Skip gallery update for excluded players
                                                                # ðŸš« ROSTER CHECK: Skip gallery updates for inactive players
                                                                elif not is_player_active_in_roster(protected_name_clean, video_specific_roster):
                                                                    if frame_data.get('frame_num', 0) % 100 == 0:
                                                                        print(f"  ðŸš« ROSTER BLOCK: Player '{protected_name_clean}' is inactive in roster - skipping gallery update")
                                                                    # Skip gallery update for inactive players
                                                                else:
                                                                    # Find player ID in gallery (use normalized name for matching)
                                                                    player_id = None
                                                                    for pid, profile in player_gallery.players.items():
                                                                        # Normalize both names for comparison
                                                                        profile_name_clean = extract_player_name(profile.name)
                                                                        if profile_name_clean == protected_name_clean:
                                                                            player_id = pid
                                                                            break
                                                                    
                                                                    # If not found, create new player with normalized name
                                                                    if player_id is None:
                                                                        player_id = protected_name_clean.lower().replace(" ", "_")
                                                                        player_gallery.add_player(
                                                                            name=protected_name_clean,
                                                                            player_id=player_id
                                                                        )
                                                                    
                                                                    if player_id is not None:
                                                                        # Get dominant color if available
                                                                        learned_dominant_color = None
                                                                        if 'detection_dominant_colors' in locals() and detection_idx < len(detection_dominant_colors):
                                                                            learned_dominant_color = detection_dominant_colors[detection_idx]
                                                                        
                                                                        # Extract uniform info
                                                                        uniform_info_for_update = None
                                                                        if 'uniform_info_list' in locals() and detection_idx < len(uniform_info_list):
                                                                            uniform_info_for_update = uniform_info_list[detection_idx]
                                                                        
                                                                        # Update gallery with protected track features (high priority - ground truth)
                                                                        player_gallery.update_player(
                                                                            player_id=player_id,
                                                                            features=protected_feature.reshape(1, -1),  # Reshape to (1, feature_dim)
                                                                            dominant_color=learned_dominant_color,
                                                                            reference_frame={
                                                                                'frame_num': frame_data.get('frame_num', 0),
                                                                                'video_path': input_path,
                                                                                'bbox': detections.xyxy[detection_idx_for_learning].tolist() if detection_idx_for_learning is not None and detection_idx_for_learning < len(detections.xyxy) else None,
                                                                                'similarity': 1.00,  # Protected tracks are ground truth - highest confidence
                                                                                'confidence': 1.00,  # Anchor frame confidence
                                                                                'is_anchor': True  # Mark as anchor frame
                                                                            },
                                                                            uniform_info=uniform_info_for_update
                                                                        )
                                                                        
                                                                        if frame_data.get('frame_num', 0) % 50 == 0:
                                                                            print(f"  ðŸŽ“ RE-ID LEARNING: Updated gallery for '{protected_name}' from protected Track #{track_id} (ground truth, similarity: 1.00)")
                                                        except Exception as e:
                                                            # Silently fail - learning is optional
                                                            if frame_data.get('frame_num', 0) % 200 == 0:
                                                                print(f"  âš  Could not learn from protected track: {e}")
                                                    
                                                    continue  # Skip Re-ID assignment (but we've learned from the protected track)
                                                
                                                # ZONE 2: Soft Protection (Â±50-100 frames from center)
                                                elif distance_from_center <= ANCHOR_SOFT_PROTECTION_FRAMES:
                                                    anchor_protection_zone = "SOFT"
                                                    # Require much stronger evidence (3x multiplier)
                                                    anchor_protection_multiplier = 3.0
                                                    if frame_data.get('frame_num', 0) % 100 == 0:
                                                        print(f"  ðŸ›¡ï¸ SOFT PROTECTION: Track #{track_id} = {protected_name} ({distance_from_center}f from anchor, 3x evidence required)")
                                                
                                                # ZONE 3: Decay Protection (Â±100-150 frames from center)
                                                elif distance_from_center <= ANCHOR_DECAY_FRAMES:
                                                    anchor_protection_zone = "DECAY"
                                                    # Gradually decay from 3x to 1x based on distance
                                                    # At 100 frames: 3x, at 150 frames: 1x
                                                    decay_progress = (distance_from_center - ANCHOR_SOFT_PROTECTION_FRAMES) / (ANCHOR_DECAY_FRAMES - ANCHOR_SOFT_PROTECTION_FRAMES)
                                                    anchor_protection_multiplier = 3.0 - (2.0 * decay_progress)  # 3.0 â†’ 1.0
                                                    if frame_data.get('frame_num', 0) % 100 == 0:
                                                        print(f"  ðŸ›¡ï¸ DECAY PROTECTION: Track #{track_id} = {protected_name} ({distance_from_center}f from anchor, {anchor_protection_multiplier:.1f}x evidence required)")
                                                
                                                # Apply name from anchor protection if in decay zones
                                                if anchor_protection_zone in ["SOFT", "DECAY"] and existing_name != protected_name:
                                                    # Force the protected name to match anchor
                                                    player_names[pid_str] = protected_name
                                                    existing_name = protected_name
                                                    has_real_name = True
                                        
                                        # ANCHOR FRAME PROTECTION: Never override anchor frame assignments (1.00 confidence)
                                        # This is a fallback check for anchor frames that don't have temporal protection yet
                                        is_anchor_frame_assignment = False
                                        if track_id_int in track_name_confidence:
                                            _, confidence, _ = track_name_confidence[track_id_int]
                                            if confidence >= 1.00:
                                                is_anchor_frame_assignment = True
                                                
                                                # ðŸŽ“ RE-ID LEARNING: Extract features from anchor-protected track and update gallery
                                                # Anchor frames are ground truth - Re-ID should learn from them
                                                if player_gallery is not None and reid_features is not None and detection_idx < len(reid_features):
                                                    try:
                                                        # Get Re-ID features for this anchor-protected track
                                                        anchor_feature = reid_features[detection_idx]
                                                        
                                                        # Only update if feature is valid (no NaN)
                                                        if not np.isnan(anchor_feature).any() and existing_name:
                                                            # Find player ID in gallery
                                                            player_id = None
                                                            for pid, profile in player_gallery.players.items():
                                                                if profile.name == existing_name:
                                                                    player_id = pid
                                                                    break
                                                            
                                                            if player_id is not None:
                                                                # Get dominant color if available
                                                                learned_dominant_color = None
                                                                if 'detection_dominant_colors' in locals() and detection_idx < len(detection_dominant_colors):
                                                                    learned_dominant_color = detection_dominant_colors[detection_idx]
                                                                
                                                                # Extract uniform info
                                                                uniform_info_for_update = None
                                                                if 'uniform_info_list' in locals() and detection_idx < len(uniform_info_list):
                                                                    uniform_info_for_update = uniform_info_list[detection_idx]
                                                                
                                                                # Update gallery with anchor-protected track features (ground truth)
                                                                player_gallery.update_player(
                                                                    player_id=player_id,
                                                                    features=anchor_feature.reshape(1, -1),
                                                                    dominant_color=learned_dominant_color,
                                                                    reference_frame={
                                                                        'frame_num': frame_data.get('frame_num', 0),
                                                                        'video_path': input_path,
                                                                        'bbox': detections.xyxy[detection_idx_for_learning].tolist() if detection_idx_for_learning is not None and detection_idx_for_learning < len(detections.xyxy) else None,
                                                                        'similarity': 1.00,  # Anchor frames are ground truth
                                                                        'confidence': 1.00,
                                                                        'is_anchor': True
                                                                    },
                                                                    uniform_info=uniform_info_for_update
                                                                )
                                                                
                                                                if frame_data.get('frame_num', 0) % 100 == 0:
                                                                    print(f"  ðŸŽ“ RE-ID LEARNING: Updated gallery for '{existing_name}' from anchor-protected Track #{track_id} (ground truth)")
                                                    except Exception as e:
                                                        # Silently fail - learning is optional
                                                        pass
                                                
                                                # Skip gallery matching for anchor frame tracks - they're ground truth
                                                if frame_data.get('frame_num', 0) % 100 == 0:
                                                    print(f"  ðŸ”’ ANCHOR PROTECTED: Track #{track_id} = {existing_name} (anchor frame 1.00 confidence, skipping gallery match)")
                                                continue
                                        
                                        # ðŸ›¡ï¸ CRITICAL: Anchor frames (confidence 1.00) are PERMANENT and CANNOT be updated
                                        # Check this BEFORE any update logic calculations
                                        is_anchor_locked = False
                                        if track_id_int in track_name_confidence:
                                            _, confidence, _ = track_name_confidence[track_id_int]
                                            if confidence >= 1.00:
                                                is_anchor_locked = True
                                                if frame_data.get('frame_num', 0) % 100 == 0:
                                                    print(f"  ðŸ”’ ANCHOR LOCKED: Track #{track_id_int} has anchor frame (confidence 1.00) - PERMANENT, cannot be updated")
                                        
                                        # Also check if track is in anchor protection window
                                        if not is_anchor_locked and track_id_int in track_anchor_protection:
                                            prot_name, prot_start, prot_end = track_anchor_protection[track_id_int]
                                            current_frame = frame_data.get('frame_num', 0)
                                            if prot_start <= current_frame <= prot_end:
                                                is_anchor_locked = True
                                                if frame_data.get('frame_num', 0) % 100 == 0:
                                                    print(f"  ðŸ”’ ANCHOR PROTECTED: Track #{track_id_int} is in protection window for '{prot_name}' (frames {prot_start}-{prot_end}) - cannot be updated")
                                        
                                        # If anchor-locked, skip ALL update logic
                                        if is_anchor_locked:
                                            allow_update = False
                                            # Skip to assignment check (which will also block due to protection)
                                            # But first, ensure player_names is synced if anchor frame was just applied
                                            if track_id_int in track_name_confidence:
                                                anchor_name, _, _ = track_name_confidence[track_id_int]
                                                pid_str = str(track_id_int)
                                                if pid_str not in player_names or player_names[pid_str] != anchor_name:
                                                    player_names[pid_str] = anchor_name
                                                    if frame_data.get('frame_num', 0) % 200 == 0:
                                                        print(f"  ðŸ”„ SYNCED: Track #{track_id_int} player_names synced to anchor '{anchor_name}'")
                                            continue  # Skip all update logic - anchor frames are permanent
                                        
                                        # ALLOW NAME UPDATE: If new evidence is significantly better
                                        # HIGH CONFIDENCE LOCK: Once a track has high confidence (>=0.75), lock it and only allow updates with much stronger evidence
                                        allow_update = False
                                        if has_real_name and track_id_int in track_name_confidence:
                                            old_name, old_similarity, old_frame = track_name_confidence[track_id_int]
                                            current_frame = frame_data.get('frame_num', 0)
                                            
                                            # CRITICAL: Only allow update if NAME is actually CHANGING
                                            # Don't update "Rocco" â†’ "Rocco" just because similarity changed slightly
                                            if player_name != old_name:
                                                similarity_improvement = similarity - old_similarity
                                                frames_since_update = current_frame - old_frame
                                                
                                                # TRACK AGE PROTECTION: Older tracks require stronger evidence to override
                                                # Tracks that have existed for >5 seconds are considered "established" and harder to change
                                                is_established_track = track_age_seconds > 5.0
                                                age_multiplier = 1.0
                                                if is_established_track:
                                                    # Established tracks (5-10s): require 1.5x stronger evidence
                                                    if track_age_seconds < 10.0:
                                                        age_multiplier = 1.5
                                                    # Very established tracks (>10s): require 2x stronger evidence
                                                    else:
                                                        age_multiplier = 2.0
                                                
                                                # COMBINED MULTIPLIER: Age protection Ã— Anchor protection
                                                # In soft/decay zones near anchors, requirements increase further
                                                combined_multiplier = age_multiplier * anchor_protection_multiplier
                                                
                                                # HIGH CONFIDENCE LOCK: If old match was high confidence (>=0.75), require much stronger evidence
                                                if old_similarity >= 0.80:
                                                    # Very high confidence lock: require 0.25+ improvement AND 90+ frames (almost never update)
                                                    # Apply combined multiplier (age + anchor protection)
                                                    required_improvement = 0.25 * combined_multiplier
                                                    required_frames = int(90 * combined_multiplier)
                                                    if similarity_improvement >= required_improvement and frames_since_update >= required_frames:
                                                        allow_update = True
                                                        if frame_data.get('frame_num', 0) % 100 == 0:
                                                            age_info = f" (age: {track_age_seconds:.1f}s)" if is_established_track else ""
                                                            anchor_info = f" anchor:{anchor_protection_multiplier:.1f}x" if anchor_protection_multiplier > 1.0 else ""
                                                            print(f"  ðŸ”“ UNLOCK: Track #{track_id} '{old_name}' â†’ '{player_name}' (very-high-conf lock: {old_similarity:.2f} â†’ {similarity:.2f}, +{similarity_improvement:.2f}{age_info}{anchor_info})")
                                                elif old_similarity >= 0.75:
                                                    # High confidence lock: require 0.20+ improvement AND 60+ frames
                                                    required_improvement = 0.20 * combined_multiplier
                                                    required_frames = int(60 * combined_multiplier)
                                                    if similarity_improvement >= required_improvement and frames_since_update >= required_frames:
                                                        allow_update = True
                                                        if frame_data.get('frame_num', 0) % 100 == 0:
                                                            age_info = f" (age: {track_age_seconds:.1f}s)" if is_established_track else ""
                                                            anchor_info = f" anchor:{anchor_protection_multiplier:.1f}x" if anchor_protection_multiplier > 1.0 else ""
                                                            print(f"  ðŸ”“ UNLOCK: Track #{track_id} '{old_name}' â†’ '{player_name}' (high-conf lock: {old_similarity:.2f} â†’ {similarity:.2f}, +{similarity_improvement:.2f}{age_info}{anchor_info})")
                                                elif old_similarity >= 0.70:
                                                    # Medium confidence: allow update with 0.15+ improvement AND 30+ frames
                                                    required_improvement = 0.15 * combined_multiplier
                                                    required_frames = int(30 * combined_multiplier)
                                                    if similarity_improvement >= required_improvement and frames_since_update >= required_frames:
                                                        allow_update = True
                                                        if frame_data.get('frame_num', 0) % 100 == 0:
                                                            age_info = f" (age: {track_age_seconds:.1f}s)" if is_established_track else ""
                                                            anchor_info = f" anchor:{anchor_protection_multiplier:.1f}x" if anchor_protection_multiplier > 1.0 else ""
                                                            print(f"  ðŸ”„ UPDATE: Track #{track_id} '{old_name}' â†’ '{player_name}' (similarity improved: {old_similarity:.2f} â†’ {similarity:.2f}, +{similarity_improvement:.2f}{age_info}{anchor_info})")
                                                else:
                                                    # Low confidence (< 0.70): More lenient - allow update with 0.10+ improvement AND 15+ frames
                                                    # This makes it easier to replace low-confidence assignments with better matches
                                                    required_improvement = 0.10 * combined_multiplier
                                                    required_frames = int(15 * combined_multiplier)
                                                    if similarity_improvement >= required_improvement and frames_since_update >= required_frames:
                                                        allow_update = True
                                                        if frame_data.get('frame_num', 0) % 100 == 0:
                                                            age_info = f" (age: {track_age_seconds:.1f}s)" if is_established_track else ""
                                                            anchor_info = f" anchor:{anchor_protection_multiplier:.1f}x" if anchor_protection_multiplier > 1.0 else ""
                                                            print(f"  ðŸ”„ UPDATE: Track #{track_id} '{old_name}' â†’ '{player_name}' (low-conf: {old_similarity:.2f} â†’ {similarity:.2f}, +{similarity_improvement:.2f}{age_info}{anchor_info})")
                                                    # Also allow immediate replacement if new match is significantly better (0.20+ improvement)
                                                    # But still apply combined multiplier (age + anchor)
                                                    elif similarity_improvement >= (0.20 * combined_multiplier):
                                                        allow_update = True
                                                        if frame_data.get('frame_num', 0) % 100 == 0:
                                                            age_info = f" (age: {track_age_seconds:.1f}s)" if is_established_track else ""
                                                            anchor_info = f" anchor:{anchor_protection_multiplier:.1f}x" if anchor_protection_multiplier > 1.0 else ""
                                                            print(f"  ðŸ”„ UPDATE: Track #{track_id} '{old_name}' â†’ '{player_name}' (low-conf immediate: {old_similarity:.2f} â†’ {similarity:.2f}, +{similarity_improvement:.2f}{age_info}{anchor_info})")
                                            # NEVER update if name is the same (prevents constant re-labeling)
                                            
                                            # MULTI-FRAME CONSENSUS: For new assignments (not updates), require same match across multiple frames
                                            # This prevents single-frame false positives from overriding established tracks
                                            if not has_real_name and track_id_int not in track_name_confidence:
                                                # TRACK AGE REQUIREMENT: Require track to exist for min_track_length frames before assigning ID
                                                # This prevents brief false detections from getting player names
                                                if track_age_frames < min_track_length:
                                                    if frame_data.get('frame_num', 0) % 100 == 0:
                                                        print(f"  â³ Track #{track_id} too young ({track_age_frames} < {min_track_length} frames) - waiting before ID assignment")
                                                    continue  # Skip assignment until track is old enough
                                                
                                                # Initialize consensus tracking for this track
                                                if track_id_int not in track_match_consensus:
                                                    track_match_consensus[track_id_int] = deque(maxlen=5)  # Track last 5 match attempts
                                                
                                                # Add current match attempt to consensus
                                                track_match_consensus[track_id_int].append((player_name, similarity, current_frame))
                                                
                                                # Require at least 3 frames of consistent matching before assigning
                                                consensus_required = 3
                                                # Use higher threshold for consensus (same as untagged assignment threshold)
                                                untagged_consensus_threshold = max(0.75, gallery_similarity_threshold + 0.10)
                                                if len(track_match_consensus[track_id_int]) >= consensus_required:
                                                    # Check if last N matches are all for the same player with good similarity
                                                    recent_matches = list(track_match_consensus[track_id_int])[-consensus_required:]
                                                    same_player_count = sum(1 for m in recent_matches if m[0] == player_name and m[1] >= untagged_consensus_threshold)
                                                    
                                                    if same_player_count < consensus_required:
                                                        # Not enough consensus - don't assign yet
                                                        if frame_data.get('frame_num', 0) % 100 == 0:
                                                            print(f"  â³ Waiting for consensus: Track #{track_id} needs {consensus_required} frames of '{player_name}' (have {same_player_count}/{consensus_required})")
                                                        continue  # Skip this assignment, wait for more consensus
                                                else:
                                                    # Not enough frames yet - wait for consensus
                                                    if frame_data.get('frame_num', 0) % 50 == 0:
                                                        print(f"  â³ Building consensus: Track #{track_id} '{player_name}' ({len(track_match_consensus[track_id_int])}/{consensus_required} frames)")
                                                    continue  # Skip assignment until we have consensus
                                        
                                        # SEED FRAME: Skip name assignments if we're only doing gallery learning
                                        # Only update gallery for tracks with 1.0 confidence, don't assign new names
                                        if should_force_gallery_learning_only:
                                            if frame_data.get('frame_num', 0) % 100 == 0:
                                                print(f"  ðŸ”„ SEED FRAME: Skipping name assignment for Track #{track_id_int} (learning-only mode)")
                                            continue  # Skip name assignment, but gallery learning will still happen for 1.0 confidence tracks
                                        
                                        # CRITICAL FIX: Apply match ONLY if similarity meets threshold
                                        # For NEW assignments to untagged tracks, require HIGHER confidence to prevent false matches
                                        # This allows cross-video matching while maintaining high accuracy
                                        # Higher threshold (0.75+) for new assignments prevents false matches like Cameron Melnik when he's not in video
                                        # Lower threshold (gallery_similarity_threshold) for updates to existing tracks
                                        if not has_real_name and track_id_int not in track_name_confidence:
                                            # NEW ASSIGNMENT: Require higher confidence (0.75+) for cross-video matching
                                            # This prevents false matches while still allowing legitimate cross-video identification
                                            untagged_assignment_threshold = max(0.75, gallery_similarity_threshold + 0.10)  # At least 0.75, or 0.10 above base threshold
                                            similarity_meets_threshold = similarity >= untagged_assignment_threshold
                                            if similarity < untagged_assignment_threshold and frame_data.get('frame_num', 0) % 200 == 0:
                                                print(f"  âš  NEW ASSIGNMENT: '{player_name}' similarity {similarity:.3f} < untagged threshold {untagged_assignment_threshold:.3f} (base: {gallery_similarity_threshold:.3f}) - requiring higher confidence for cross-video matching")
                                        else:
                                            # UPDATE TO EXISTING TRACK: Use standard threshold
                                            similarity_meets_threshold = similarity >= gallery_similarity_threshold
                                        
                                        # CRITICAL: Ensure player_name is a real name from gallery, not generic
                                        # If player_name is None or generic, skip assignment (don't create generic names)
                                        if player_name and (player_name.startswith("Player ") or player_name.startswith("#") or player_name.startswith("Anonymous")):
                                            # This is a generic name - don't assign it
                                            player_name = None
                                        
                                        # ENHANCEMENT: Breadcrumb-based re-ID for long occlusions
                                        # If this track doesn't have a name yet, check breadcrumbs from occluded tracks
                                        breadcrumb_match = None
                                        breadcrumb_similarity = 0.0
                                        if not has_real_name and reid_tracker is not None and detection_idx < len(detections.xyxy):
                                            try:
                                                # Extract current detection's Re-ID feature
                                                current_bbox = detections.xyxy[detection_idx:detection_idx+1]
                                                current_detection = sv.Detections(xyxy=current_bbox)
                                                
                                                # Get frame for Re-ID
                                                frame_for_reid = batch_frame
                                                original_frame = frame_data.get('original_frame_for_learning', None)
                                                if original_frame is not None:
                                                    frame_for_reid = original_frame
                                                
                                                current_feature = reid_tracker.extract_features(
                                                    frame_for_reid, current_detection, team_colors, ball_colors)
                                                
                                                if current_feature is not None and len(current_feature) > 0:
                                                    current_feature = current_feature[0]
                                                    
                                                    # Check breadcrumbs from occluded tracks
                                                    current_frame_num = frame_data.get('frame_num', 0)
                                                    for occluded_track_id, breadcrumbs in track_reid_breadcrumbs.items():
                                                        # Check if track is truly occluded (not active)
                                                        if occluded_track_id in active_track_ids:
                                                            continue  # Track is active, skip
                                                        
                                                        # Check if track was occluded long enough (at least OCCLUSION_THRESHOLD_FRAMES)
                                                        if occluded_track_id in track_last_seen:
                                                            occlusion_duration = current_frame_num - track_last_seen[occluded_track_id]
                                                            if occlusion_duration < OCCLUSION_THRESHOLD_FRAMES:
                                                                continue  # Too recent, not a long occlusion
                                                        
                                                        # Get player name from occluded track
                                                        occluded_track_str = str(occluded_track_id)
                                                        if occluded_track_str not in player_names:
                                                            continue  # No name for occluded track
                                                        
                                                        occluded_player_name = player_names[occluded_track_str]
                                                        if not occluded_player_name or occluded_player_name.startswith("Player "):
                                                            continue  # Generic name, skip
                                                        
                                                        # Compare current feature with breadcrumbs
                                                        best_breadcrumb_sim = 0.0
                                                        for breadcrumb_feature, breadcrumb_frame, breadcrumb_bbox in breadcrumbs:
                                                            try:
                                                                # Calculate cosine similarity (using numpy for compatibility)
                                                                # Cosine similarity = dot(a,b) / (norm(a) * norm(b))
                                                                a = current_feature.flatten()
                                                                b = breadcrumb_feature.flatten()
                                                                dot_product = np.dot(a, b)
                                                                norm_a = np.linalg.norm(a)
                                                                norm_b = np.linalg.norm(b)
                                                                if norm_a > 0 and norm_b > 0:
                                                                    sim = dot_product / (norm_a * norm_b)
                                                                    best_breadcrumb_sim = max(best_breadcrumb_sim, sim)
                                                            except:
                                                                continue
                                                        
                                                        # If breadcrumb match is strong enough, use it
                                                        if best_breadcrumb_sim > breadcrumb_similarity and best_breadcrumb_sim >= gallery_similarity_threshold:
                                                            breadcrumb_match = occluded_player_name
                                                            breadcrumb_similarity = best_breadcrumb_sim
                                                            if frame_data.get('frame_num', 0) % 100 == 0:
                                                                print(f"  ðŸž BREADCRUMB MATCH: Track #{track_id} matches occluded '{occluded_player_name}' (Track #{occluded_track_id}) via breadcrumb (similarity: {best_breadcrumb_sim:.3f}, occluded {occlusion_duration}f ago)")
                                            except Exception as e:
                                                # Silently fail - breadcrumbs are optional
                                                pass
                                        
                                        # Use breadcrumb match if available and better than gallery match
                                        if breadcrumb_match and breadcrumb_similarity > similarity:
                                            player_name = breadcrumb_match
                                            similarity = breadcrumb_similarity
                                            similarity_meets_threshold = True
                                        
                                        # CRITICAL: Only assign if similarity meets threshold (no fallback matches)
                                        # This prevents false positives - if similarity is too low (e.g., 0.4792), don't assign
                                        # Better to leave track unnamed than assign wrong player
                                        if player_name and similarity < gallery_similarity_threshold:
                                            # Log rejected matches occasionally to help diagnose false positives
                                            if frame_data.get('frame_num', 0) % 200 == 0:
                                                print(f"  âœ— Rejected match: {player_name} (similarity: {similarity:.4f} < threshold: {gallery_similarity_threshold:.2f}) - too low, would cause false positive")
                                        
                                        # ðŸ›¡ï¸ DRIFT PREVENTION CHECKS: Apply all Grok's fixes before assignment
                                        drift_prevention_passed = True
                                        drift_rejection_reason = None
                                        
                                        if player_name and (not has_real_name or allow_update) and similarity_meets_threshold:
                                            # ðŸ›¡ï¸ CHECK #1: Identity lock after N matches (Grok's Fix #1)
                                            if track_id_int in track_confirmed_identity:
                                                lock_name, lock_frame, lock_conf, match_count = track_confirmed_identity[track_id_int]
                                                if lock_name != player_name:
                                                    # Track has confirmed identity - require very strong evidence to override
                                                    if similarity < 0.90:  # Require 0.90 to override lock
                                                        drift_prevention_passed = False
                                                        drift_rejection_reason = f"Identity locked: '{lock_name}' (confirmed {match_count}x, similarity {similarity:.3f} < 0.90 required)"
                                                        if frame_data.get('frame_num', 0) % 100 == 0:
                                                            print(f"  ðŸ”’ DRIFT BLOCKED: {drift_rejection_reason}")
                                            
                                            # ðŸ›¡ï¸ CHECK #2: Motion-consistency gating (Grok's Fix #2)
                                            if drift_prevention_passed and detection_idx < len(detections.xyxy):
                                                det_bbox = detections.xyxy[detection_idx]
                                                det_center_x = (det_bbox[0] + det_bbox[2]) / 2
                                                det_center_y = (det_bbox[1] + det_bbox[3]) / 2
                                                if violates_motion_model(track_id_int, (det_center_x, det_center_y), track_positions_history, MAX_PIXELS_PER_FRAME):
                                                    drift_prevention_passed = False
                                                    drift_rejection_reason = "Motion model violation: position change too large or direction inconsistent"
                                                    if frame_data.get('frame_num', 0) % 100 == 0:
                                                        print(f"  ðŸš« DRIFT BLOCKED: {drift_rejection_reason}")
                                            
                                            # ðŸ›¡ï¸ CHECK #3: Bounding box size consistency (Grok's Fix #3)
                                            if drift_prevention_passed and track_id_int in track_previous_bbox and detection_idx < len(detections.xyxy):
                                                old_bbox = track_previous_bbox[track_id_int]
                                                old_area = (old_bbox[2] - old_bbox[0]) * (old_bbox[3] - old_bbox[1])
                                                new_bbox = detections.xyxy[detection_idx]
                                                new_area = (new_bbox[2] - new_bbox[0]) * (new_bbox[3] - new_bbox[1])
                                                
                                                # Check if area changed dramatically (>50%)
                                                area_ratio = new_area / max(old_area, 1.0)
                                                if area_ratio < 0.5 or area_ratio > 2.0:
                                                    drift_prevention_passed = False
                                                    drift_rejection_reason = f"Bbox size inconsistency: area changed {area_ratio:.2f}x (old: {old_area:.0f}, new: {new_area:.0f})"
                                                    if frame_data.get('frame_num', 0) % 100 == 0:
                                                        print(f"  ðŸ“¦ DRIFT BLOCKED: {drift_rejection_reason}")
                                            
                                            # ðŸ›¡ï¸ CHECK #4: Jersey color signature validation (Grok's Fix #4)
                                            if drift_prevention_passed and track_id_int in track_jersey_signatures and detection_idx < len(detection_dominant_colors):
                                                old_signature = track_jersey_signatures[track_id_int]
                                                new_signature = detection_dominant_colors[detection_idx]
                                                
                                                if old_signature is not None and new_signature is not None:
                                                    # Calculate color distance in HSV
                                                    color_distance = np.sqrt(
                                                        (old_signature[0] - new_signature[0])**2 * 2.0 +  # H is more important
                                                        (old_signature[1] - new_signature[1])**2 +
                                                        (old_signature[2] - new_signature[2])**2 * 0.5
                                                    )
                                                    
                                                    if color_distance > 30:  # Threshold for color difference
                                                        drift_prevention_passed = False
                                                        drift_rejection_reason = f"Jersey color mismatch: distance {color_distance:.1f} > 30"
                                                        if frame_data.get('frame_num', 0) % 100 == 0:
                                                            print(f"  ðŸŽ¨ DRIFT BLOCKED: {drift_rejection_reason}")
                                            
                                            # ðŸ›¡ï¸ CHECK #5: Proximity-based swap prevention (Grok's Fix #7)
                                            if drift_prevention_passed and track_id_int in track_positions_history:
                                                # Check if any other track with different name is very close
                                                if detection_idx < len(detections.xyxy):
                                                    det_bbox = detections.xyxy[detection_idx]
                                                    det_center_x = (det_bbox[0] + det_bbox[2]) / 2
                                                    det_center_y = (det_bbox[1] + det_bbox[3]) / 2
                                                    
                                                    for other_track_id, other_positions in track_positions_history.items():
                                                        if other_track_id != track_id_int and len(other_positions) > 0:
                                                            other_frame, other_x, other_y, *_ = other_positions[-1]
                                                            distance = np.sqrt((det_center_x - other_x)**2 + (det_center_y - other_y)**2)
                                                            
                                                            if distance < 100:  # Players are very close (<100px)
                                                                # Check if other track has a different name
                                                                other_track_str = str(int(other_track_id))
                                                                other_name = player_names.get(other_track_str, "")
                                                                if other_name and other_name != player_name:
                                                                    # Temporarily require higher similarity to prevent swaps
                                                                    required_similarity = gallery_similarity_threshold * 1.5
                                                                    if similarity < required_similarity:
                                                                        drift_prevention_passed = False
                                                                        drift_rejection_reason = f"Proximity swap prevention: '{player_name}' too close to '{other_name}' ({distance:.1f}px), similarity {similarity:.3f} < {required_similarity:.3f}"
                                                                        if frame_data.get('frame_num', 0) % 100 == 0:
                                                                            print(f"  ðŸ‘¥ DRIFT BLOCKED: {drift_rejection_reason}")
                                                                    break
                                            
                                            # ðŸ›¡ï¸ CHECK #6: Merge cooldown window (Grok's Fix #8)
                                            if drift_prevention_passed and track_id_int in track_merge_cooldown:
                                                merge_frame, cooldown_end = track_merge_cooldown[track_id_int]
                                                current_frame = frame_data.get('frame_num', 0)
                                                if current_frame < cooldown_end:
                                                    drift_prevention_passed = False
                                                    drift_rejection_reason = f"Merge cooldown: track merged at frame {merge_frame}, cooldown until frame {cooldown_end} (current: {current_frame})"
                                                    if frame_data.get('frame_num', 0) % 100 == 0:
                                                        print(f"  â³ DRIFT BLOCKED: {drift_rejection_reason}")
                                            
                                            # If any drift prevention check failed, skip assignment
                                            if not drift_prevention_passed:
                                                if frame_data.get('frame_num', 0) % 50 == 0:
                                                    print(f"  ðŸ›¡ï¸ DRIFT PREVENTION: Blocked '{player_name}' assignment to Track #{track_id_int} - {drift_rejection_reason}")
                                                continue  # Skip this assignment
                                            
                                            # GLOBAL JERSEY CONSTRAINT: Check if this jersey is already assigned to ANY track in the video
                                            jersey_conflict = False
                                            if jersey_number and jersey_number in jersey_to_track_global:
                                                assigned_track = jersey_to_track_global[jersey_number]
                                                if assigned_track != track_id:
                                                    # This jersey is already owned by a different track!
                                                    # BUT: If the owning track is inactive, allow reassignment
                                                    if jersey_number in jerseys_available_for_reassignment:
                                                        # Jersey available for reassignment - no conflict!
                                                        if frame_data.get('frame_num', 0) % 100 == 0:
                                                            print(f"  ðŸ”„ Reassigning jersey #{jersey_number} from Track #{assigned_track} (inactive) to Track #{track_id}")
                                                        jersey_conflict = False
                                                    else:
                                                        # Jersey still locked to active track
                                                        jersey_conflict = True
                                                        if frame_data.get('frame_num', 0) % 100 == 0:
                                                            print(f"  âš  Jersey conflict: {player_name} (#{jersey_number}) already assigned to Track #{assigned_track}")
                                            
                                            # ðŸ›¡ï¸ CRITICAL CHECK #1: Is the TARGET TRACK already protected for a DIFFERENT player?
                                            # This prevents Anay from being assigned to Track #8 if Track #8 is protected for Carson
                                            current_frame = frame_data.get('frame_num', 0)
                                            
                                            # DIAGNOSTIC: Log protection check (first few frames only)
                                            if current_frame < 10 and track_id_int in track_anchor_protection:
                                                prot_name, prot_start, prot_end = track_anchor_protection[track_id_int]
                                                print(f"  ðŸ” PROTECTION CHECK: Frame {current_frame}, Track #{track_id_int} is protected for '{prot_name}' (window: {prot_start}-{prot_end}), trying to assign '{player_name}'")
                                            
                                            if track_id_int in track_anchor_protection:
                                                protected_name, prot_start, prot_end = track_anchor_protection[track_id_int]
                                                if prot_start <= current_frame <= prot_end:
                                                    # Track is protected for a different player - BLOCK assignment
                                                    if protected_name != player_name:
                                                        prot_center = (prot_start + prot_end) // 2
                                                        dist_from_center = abs(current_frame - prot_center)
                                                        
                                                        # HARD PROTECTION: Never allow different player on protected track
                                                        if dist_from_center <= ANCHOR_HARD_PROTECTION_FRAMES:
                                                            # Log every block (not just every 10 frames) for debugging
                                                            print(f"  ðŸ›¡ï¸ TRACK PROTECTION BLOCK: Track #{track_id_int} is HARD protected for '{protected_name}' (f{current_frame}, {dist_from_center}f from anchor) - BLOCKING '{player_name}' assignment")
                                                            continue  # Skip this assignment completely
                                                        
                                                        # SOFT/DECAY: Still block in protection zones (ALWAYS block, not just log)
                                                        if dist_from_center <= ANCHOR_DECAY_FRAMES:
                                                            # Log every block - user needs to see protection is working
                                                            print(f"  ðŸ›¡ï¸ TRACK PROTECTION BLOCK: Track #{track_id_int} is protected for '{protected_name}' (f{current_frame}, {dist_from_center}f from anchor) - BLOCKING '{player_name}' assignment")
                                                            continue  # Skip this assignment - ALWAYS block, don't allow override
            
                                            # ðŸ›¡ï¸ CRITICAL CHECK #2: Check if this player name has anchor protection on a DIFFERENT track
                                            # If so, prevent assignment to this track (player already has protected identity)
                                            player_has_protected_track = False
                                            protected_track_id = None
                                            
                                            # Check if this player name has anchor protection on any track
                                            for prot_track_id, (prot_name, prot_start, prot_end) in track_anchor_protection.items():
                                                if prot_name == player_name and prot_start <= current_frame <= prot_end:
                                                    # Check if protected track is active in current frame
                                                    if prot_track_id in active_track_ids:
                                                        player_has_protected_track = True
                                                        protected_track_id = prot_track_id
                                                        break
                                            
                                            # If player has protected track and we're trying to assign to a different track, BLOCK IT
                                            if player_has_protected_track and protected_track_id != track_id_int:
                                                # Calculate distance from anchor center for protected track
                                                prot_name, prot_start, prot_end = track_anchor_protection[protected_track_id]
                                                prot_center = (prot_start + prot_end) // 2
                                                dist_from_center = abs(current_frame - prot_center)
                                                
                                                # HARD PROTECTION: Never allow assignment to different track
                                                if dist_from_center <= ANCHOR_HARD_PROTECTION_FRAMES:
                                                    # Log every block - user needs to see protection is working
                                                    print(f"  ðŸ›¡ï¸ PROTECTED PLAYER BLOCK: '{player_name}' has HARD protection on Track #{protected_track_id} (f{current_frame}, {dist_from_center}f from anchor) - BLOCKING assignment to Track #{track_id_int}")
                                                    continue  # Skip this assignment completely
                                                
                                                # SOFT/DECAY: Still block unless new track is much closer to anchor
                                                # (This would require anchor bbox info, so for now just block)
                                                if dist_from_center <= ANCHOR_DECAY_FRAMES:
                                                    # Log every block - user needs to see protection is working
                                                    print(f"  ðŸ›¡ï¸ PROTECTED PLAYER BLOCK: '{player_name}' has protection on Track #{protected_track_id} (f{current_frame}, {dist_from_center}f from anchor) - BLOCKING assignment to Track #{track_id_int}")
                                                    continue  # Skip this assignment - ALWAYS block in protection zones
                                            
                                            # CRITICAL FIX #3: GLOBAL PLAYER CONSTRAINT - Prevent same player on multiple tracks
                                            # This prevents Rocco from appearing on tracks #1, #6, and #7 simultaneously
                                            player_conflict = False
                                            if player_name and player_name in player_to_track_global:
                                                assigned_track = player_to_track_global[player_name]
                                                if assigned_track != track_id:
                                                    # This player is already assigned to a different track!
                                                    # Check if the assigned track is still active
                                                    if assigned_track in active_track_ids:
                                                        # Player is on an active track - this is a conflict!
                                                        current_frame = frame_data.get('frame_num', 0)
                                                        
                                                        # ðŸ›¡ï¸ ANCHOR PROTECTION CHECK: If existing track has anchor protection, NEVER override
                                                        has_anchor_protection = assigned_track in track_anchor_protection
                                                        if has_anchor_protection:
                                                            protected_name, prot_start, prot_end = track_anchor_protection[assigned_track]
                                                            if prot_start <= current_frame <= prot_end:
                                                                # Calculate distance from anchor center
                                                                prot_center = (prot_start + prot_end) // 2
                                                                dist_from_center = abs(current_frame - prot_center)
                                                                if dist_from_center <= ANCHOR_HARD_PROTECTION_FRAMES:
                                                                    # HARD PROTECTION: Never override
                                                                    player_conflict = True
                                                                    # Log every block - user needs to see protection is working
                                                                    print(f"  ðŸ›¡ï¸ ANCHOR BLOCK: {player_name} on Track #{assigned_track} has HARD anchor protection (f{current_frame}, {dist_from_center}f from anchor) - rejecting Track #{track_id}")
                                                                    # Skip confidence check - anchor protection wins
                                                                    continue  # Skip to next detection
                                                        
                                                        # ðŸŽ¯ CONFIDENCE-BASED CONFLICT RESOLUTION: Compare confidence levels
                                                        # Get confidence of existing assignment
                                                        existing_confidence = 0.0
                                                        if assigned_track in track_name_confidence:
                                                            _, existing_confidence, _ = track_name_confidence[assigned_track]
                                                        
                                                        # Get confidence of new assignment
                                                        new_confidence = similarity
                                                        
                                                        # Calculate confidence difference
                                                        confidence_diff = new_confidence - existing_confidence
                                                        
                                                        # Only allow new assignment if:
                                                        # 1. New confidence is significantly higher (0.20+ improvement), OR
                                                        # 2. Existing confidence is very low (<0.50) and new is decent (>=0.60)
                                                        min_improvement_threshold = 0.20  # Must be 0.20+ better
                                                        low_confidence_threshold = 0.50  # Existing is unreliable
                                                        decent_confidence_threshold = 0.60  # New is reliable
                                                        
                                                        allow_override = False
                                                        if confidence_diff >= min_improvement_threshold:
                                                            # New match is significantly better - allow override
                                                            allow_override = True
                                                            if current_frame % 30 == 0:
                                                                print(f"  âœ… OVERRIDE: {player_name} Track #{assigned_track} ({existing_confidence:.2f}) â†’ Track #{track_id} ({new_confidence:.2f}) [+{confidence_diff:.2f}]")
                                                        elif existing_confidence < low_confidence_threshold and new_confidence >= decent_confidence_threshold:
                                                            # Existing is unreliable, new is decent - allow override
                                                            allow_override = True
                                                            if current_frame % 30 == 0:
                                                                print(f"  âœ… OVERRIDE: {player_name} Track #{assigned_track} (low: {existing_confidence:.2f}) â†’ Track #{track_id} (decent: {new_confidence:.2f})")
                                                        else:
                                                            # Block assignment - existing is better or too close
                                                            player_conflict = True
                                                            if current_frame % 30 == 0:
                                                                print(f"  âš  BLOCKED: {player_name} already on Track #{assigned_track} ({existing_confidence:.2f}), rejecting Track #{track_id} ({new_confidence:.2f}) [diff: {confidence_diff:+.2f}]")
                                                        
                                                        if allow_override:
                                                            # ðŸ›¡ï¸ CRITICAL: Check if old track has anchor protection BEFORE clearing
                                                            # NEVER clear a protected track's name, even if new match is better
                                                            old_track_has_protection = False
                                                            if assigned_track in track_anchor_protection:
                                                                old_prot_name, old_prot_start, old_prot_end = track_anchor_protection[assigned_track]
                                                                if old_prot_start <= current_frame <= old_prot_end:
                                                                    old_track_has_protection = True
                                                                    # Calculate distance from anchor center
                                                                    old_prot_center = (old_prot_start + old_prot_end) // 2
                                                                    old_dist_from_center = abs(current_frame - old_prot_center)
                                                                    
                                                                    # HARD PROTECTION: Never override protected track, even with better match
                                                                    if old_dist_from_center <= ANCHOR_HARD_PROTECTION_FRAMES:
                                                                        player_conflict = True
                                                                        # Log every block - user needs to see protection is working
                                                                        print(f"  ðŸ›¡ï¸ PROTECTED TRACK BLOCK: Track #{assigned_track} '{old_name}' has HARD anchor protection (f{current_frame}, {old_dist_from_center}f from anchor) - REJECTING override by '{player_name}' on Track #{track_id} (even with better confidence: {new_confidence:.2f} vs {existing_confidence:.2f})")
                                                                        continue  # Skip to next detection - protected track cannot be overridden
                                                                    
                                                                    # SOFT/DECAY: Still block unless new match is MUCH better (0.30+ improvement)
                                                                    if old_dist_from_center <= ANCHOR_DECAY_FRAMES:
                                                                        if confidence_diff < 0.30:  # Require 0.30+ improvement to override in decay zone
                                                                            player_conflict = True
                                                                            # Log every block - user needs to see protection is working
                                                                            print(f"  ðŸ›¡ï¸ PROTECTED TRACK BLOCK: Track #{assigned_track} '{old_name}' has protection (f{current_frame}, {old_dist_from_center}f from anchor) - REJECTING override by '{player_name}' (improvement: {confidence_diff:.2f} < 0.30 required)")
                                                                            continue  # Skip - not enough improvement to override protected track
                                                            
                                                            # Only clear if track is NOT protected
                                                            if not old_track_has_protection:
                                                                # Remove old assignment to make room for new one
                                                                old_pid_str = str(int(assigned_track))
                                                                if old_pid_str in player_names:
                                                                    old_name = player_names[old_pid_str]
                                                                    # Clear old track's name (will be reassigned later)
                                                                    player_names[old_pid_str] = ""
                                                                    # Remove from global mapping
                                                                    if old_name in player_to_track_global:
                                                                        del player_to_track_global[old_name]
                                                                    # Clear confidence
                                                                    if assigned_track in track_name_confidence:
                                                                        del track_name_confidence[assigned_track]
                                                                    if current_frame % 30 == 0:
                                                                        print(f"  ðŸ—‘ï¸ Cleared old assignment: Track #{assigned_track} â†’ '' (making room for better match)")
                                                            else:
                                                                # Track is protected - block override
                                                                player_conflict = True
                                                                # Log every block - user needs to see protection is working
                                                                print(f"  ðŸ›¡ï¸ PROTECTED TRACK BLOCK: Track #{assigned_track} is protected - cannot override even with better match")
                                                        else:
                                                            # Block this assignment
                                                            player_conflict = True
                                                        
                                                        # Report conflict to GUI for user resolution (skip during preview mode)
                                                        if not preview_mode:
                                                            try:
                                                                import shared_state
                                                                shared_state.report_player_conflict(player_name, assigned_track, track_id, current_frame)
                                                                # Also notify live viewer controls if available
                                                                live_viewer_controls = shared_state.get_live_viewer_controls()
                                                                if live_viewer_controls and hasattr(live_viewer_controls, 'refresh_conflicts'):
                                                                    # Use after_idle for thread-safe GUI update
                                                                    try:
                                                                        live_viewer_controls.window.after_idle(live_viewer_controls.refresh_conflicts)
                                                                    except:
                                                                        pass
                                                            except:
                                                                pass  # Silently fail if GUI not available
                                                    else:
                                                        # Assigned track is inactive - check spatial distance before reassigning
                                                        # CRITICAL: Only allow reassignment if new track is reasonably close to where old track was
                                                        # This prevents switching to tracks that are far away (likely different players)
                                                        allow_reassignment = True
                                                        spatial_check_distance = 120  # pixels - max distance for reassignment (reduced from 200 for stricter control)
                                                        
                                                        # For protected players, use even stricter distance
                                                        if player_name in player_anchor_protection:
                                                            # Check if player has active protection
                                                            has_active_protection = False
                                                            for prot_start, prot_end, _, _ in player_anchor_protection[player_name]:
                                                                if prot_start <= current_frame <= prot_end:
                                                                    has_active_protection = True
                                                                    break
                                                            if has_active_protection:
                                                                spatial_check_distance = 80  # Very strict for protected players
                                                        
                                                        # CRITICAL: Check how recently the track was lost
                                                        # If track was lost very recently (< 5 frames), it might just be a detection glitch
                                                        # Don't reassign immediately - wait for track to come back
                                                        frames_since_lost = current_frame - track_last_seen.get(assigned_track, current_frame)
                                                        if frames_since_lost < 5:  # Very recent loss - might be detection glitch
                                                            allow_reassignment = False
                                                            if current_frame % 10 == 0:
                                                                print(f"  â¸ HOLDING: '{player_name}' Track #{assigned_track} lost only {frames_since_lost} frame(s) ago - waiting for recovery (possible detection glitch)")
                                                        
                                                        # Get last known position of old track
                                                        old_track_last_pos = None
                                                        if assigned_track in player_position_history:
                                                            if len(player_position_history[assigned_track]) > 0:
                                                                old_track_last_pos = player_position_history[assigned_track][-1]
                                                        
                                                        # Get current position of new track
                                                        new_track_center = None
                                                        if detection_idx < len(detections.xyxy):
                                                            new_track_bbox = detections.xyxy[detection_idx]
                                                            new_track_center = ((new_track_bbox[0] + new_track_bbox[2]) / 2, 
                                                                               (new_track_bbox[1] + new_track_bbox[3]) / 2)
                                                        
                                                        # Check spatial distance if we have both positions
                                                        if old_track_last_pos is not None and new_track_center is not None:
                                                            distance = np.sqrt((new_track_center[0] - old_track_last_pos[0])**2 + 
                                                                              (new_track_center[1] - old_track_last_pos[1])**2)
                                                            
                                                            # Also check if old track had high confidence - if so, require closer distance
                                                            old_confidence = 0.0
                                                            if assigned_track in track_name_confidence:
                                                                _, old_confidence, _ = track_name_confidence[assigned_track]
                                                            
                                                            # VELOCITY/MOVEMENT DIRECTION CHECK: If old track was moving, new track should be moving similarly
                                                            velocity_match = True
                                                            if assigned_track in track_state and 'velocity' in track_state[assigned_track]:
                                                                old_velocity = track_state[assigned_track]['velocity']
                                                                if old_velocity is not None and len(old_velocity) >= 2:
                                                                    # Calculate time since old track was last seen
                                                                    frames_since_last_seen = current_frame - track_last_seen.get(assigned_track, current_frame)
                                                                    if frames_since_last_seen > 0 and frames_since_last_seen < 30:  # Only check if recently lost
                                                                        # Predict where old track should be based on velocity
                                                                        predicted_x = old_track_last_pos[0] + old_velocity[0] * frames_since_last_seen
                                                                        predicted_y = old_track_last_pos[1] + old_velocity[1] * frames_since_last_seen
                                                                        predicted_distance = np.sqrt((new_track_center[0] - predicted_x)**2 + 
                                                                                                     (new_track_center[1] - predicted_y)**2)
                                                                        
                                                                        # New track should be closer to predicted position than to last position
                                                                        # This catches cases where a player is moving and a new track appears in their path
                                                                        if predicted_distance < distance * 0.8:  # 20% closer to predicted = good match
                                                                            velocity_match = True
                                                                        else:
                                                                            # New track is not following the movement pattern
                                                                            velocity_match = False
                                                                            if frame_data.get('frame_num', 0) % 50 == 0:
                                                                                print(f"  âš  Movement mismatch: Track #{track_id} not following Track #{assigned_track} movement pattern (predicted: {predicted_distance:.1f}px, actual: {distance:.1f}px)")
                                                            
                                                            # High confidence tracks (>=0.75) require much closer distance (100px)
                                                            if old_confidence >= 0.75:
                                                                spatial_check_distance = 100
                                                            
                                                            # Established tracks also require closer distance
                                                            old_track_age = 0
                                                            if assigned_track in track_first_seen:
                                                                old_track_age = current_frame - track_first_seen[assigned_track]
                                                            if old_track_age > 150:  # >5 seconds at 30fps
                                                                spatial_check_distance = max(100, spatial_check_distance * 0.8)  # 20% stricter
                                                            
                                                            if distance > spatial_check_distance or not velocity_match:
                                                                allow_reassignment = False
                                                                player_conflict = True  # Block assignment
                                                                reason = "too far apart" if distance > spatial_check_distance else "movement mismatch"
                                                                if frame_data.get('frame_num', 0) % 50 == 0:
                                                                    print(f"  âš  BLOCKED reassignment: {player_name} from Track #{assigned_track} to Track #{track_id} - {reason} ({distance:.1f}px > {spatial_check_distance}px)")
                                                        
                                                        if allow_reassignment:
                                                            if frame_data.get('frame_num', 0) % 100 == 0:
                                                                distance_info = ""
                                                                if old_track_last_pos is not None and new_track_center is not None:
                                                                    distance_info = f" (distance: {distance:.1f}px)"
                                                                print(f"  ðŸ”„ Reassigning {player_name} from Track #{assigned_track} (inactive) to Track #{track_id}{distance_info}")
                                                            # Remove old assignment
                                                        del player_to_track_global[player_name]
                                                        # Also clean up jersey and team mappings for the old track
                                                        if assigned_track in track_to_jersey:
                                                            old_jersey = track_to_jersey[assigned_track]
                                                            if old_jersey in jersey_to_track_global:
                                                                del jersey_to_track_global[old_jersey]
                                                        # CRITICAL: Only delete team assignment if track is truly inactive
                                                        # Don't delete if team classification just temporarily failed
                                                        # The persistent team fix above will handle None teams gracefully
                                                        if assigned_track in track_to_team_global:
                                                            # Only delete if we're sure the track is gone (not just team classification failed)
                                                            del track_to_team_global[assigned_track]
                                                        if player_name in player_to_team_global:
                                                            del player_to_team_global[player_name]
                                            
                                            # COACH CHECK: Coaches should not be assigned to any team
                                            # CRITICAL FIX #2: Check coach status BEFORE getting player_team from gallery
                                            is_coach = player_name and any(coach.lower() in player_name.lower() for coach in coach_names)
                                            if is_coach:
                                                # Force player_team to None for coaches (even if they have a team in gallery)
                                                player_team = None
                                                if frame_data.get('frame_num', 0) % 100 == 0:
                                                    print(f"  ðŸƒ Coach detected: {player_name} - will not be assigned to any team")
                                            
                                            # TEAM PERSISTENCE: If this track was previously assigned to a team, enforce it
                                            # A Gray track cannot switch to Blue (and vice versa)
                                            # CRITICAL: persistent_team will only be non-None if track_to_team_global had a valid team
                                            # This prevents None teams from breaking persistence logic
                                            persistent_team = track_to_team_persistent.get(track_id_int, None)
                                            if persistent_team and not is_coach:
                                                # ENHANCEMENT: Player-to-team auto-locking based on video_type
                                                # In game mode: STRICT - no team switches allowed (players stay on same team)
                                                # In practice mode: FLEXIBLE - allow team switches (players may change jerseys)
                                                if video_type == "game":
                                                    # GAME MODE: Strict team locking - never allow switches
                                                    if player_team and player_team != persistent_team:
                                                        if frame_data.get('frame_num', 0) % 100 == 0:
                                                            print(f"  ðŸ”’ðŸ”’ GAME MODE LOCK: Track #{track_id} locked to '{persistent_team}' team - blocking switch to '{player_team}'")
                                                        player_team = persistent_team  # Force persistent team
                                                    elif player_team is None:
                                                        player_team = persistent_team
                                                        if frame_data.get('frame_num', 0) % 100 == 0:
                                                            print(f"  ðŸ”’ðŸ”’ GAME MODE LOCK: Track #{track_id} team classification failed, using locked team '{persistent_team}'")
                                                else:
                                                    # PRACTICE MODE: Flexible - allow switches but prefer persistence
                                                    # Players may switch from blue penny to gray team jersey during practice
                                                    if player_team and player_team != persistent_team:
                                                        if frame_data.get('frame_num', 0) % 100 == 0:
                                                            print(f"  ðŸ”’ Practice mode: Track #{track_id} was on '{persistent_team}' team, but allowing switch to '{player_team}' (practice flexibility)")
                                                        # In practice, we allow the switch but update persistent_team for future frames
                                                        # This allows jersey changes during practice
                                                    elif player_team is None:
                                                        player_team = persistent_team
                                                        if frame_data.get('frame_num', 0) % 100 == 0:
                                                            print(f"  ðŸ”’ Team persistence (recovery): Track #{track_id} team classification failed, using persistent team '{persistent_team}'")
                                            
                                            # GLOBAL TEAM CONSTRAINT: Check if this player is already assigned to a different team
                                            # Also check if detection team matches player team
                                            team_conflict = False
                                            detection_team = detection_teams[detection_idx] if detection_idx < len(detection_teams) else None
                                            
                                            # Team classification: Trust detection classification for current video
                                            # Gallery team is used as a hint/fallback, but detection takes precedence
                                            # This allows players to be on different teams in different videos
                                            # Only use gallery team if detection team is uncertain/None
                                            if player_team and not is_coach:
                                                # If detection team is None or uncertain, use gallery team as fallback
                                                if detection_team is None:
                                                    detection_team = player_team
                                                    if frame_data.get('frame_num', 0) % 100 == 0:
                                                        print(f"  â„¹ Using gallery team '{player_team}' for {player_name} (detection team was None)")
                                                # If detection team differs from gallery, trust detection (player may be on different team in this video)
                                                elif detection_team != player_team:
                                                    if frame_data.get('frame_num', 0) % 100 == 0:
                                                        print(f"  â„¹ Team mismatch: {player_name} is '{player_team}' in gallery, but detection classified as '{detection_team}' - trusting detection for current video")
                                            
                                            # Apply team persistence to detection_team if track has persistent team
                                            if persistent_team and not is_coach:
                                                detection_team = persistent_team
                                            
                                            # Check 1: Is this player already assigned to a different team globally?
                                            if player_name in player_to_team_global:
                                                assigned_team, assigned_track_id = player_to_team_global[player_name]
                                                if assigned_track_id != track_id and assigned_team is not None:
                                                    # Player is already on a different team!
                                                    if detection_team and detection_team != assigned_team:
                                                        team_conflict = True
                                                        if frame_data.get('frame_num', 0) % 100 == 0:
                                                            print(f"  âš  Team conflict: {player_name} already on '{assigned_team}' team (Track #{assigned_track_id}), detection is '{detection_team}'")
                                            
                                            # Check 2: Does detection team match player team?
                                            # CRITICAL: This should ALWAYS prevent cross-team matches
                                            if not team_conflict and detection_team and player_team:
                                                if detection_team != player_team:
                                                    team_conflict = True
                                                    # Always print team mismatches (not just every 100 frames) for debugging
                                                    print(f"  âš  Team mismatch: {player_name} is on '{player_team}' team, but detection is '{detection_team}' team (Frame {frame_data.get('frame_num', 'unknown')})")
                                            elif frame_data.get('frame_num', 0) % 100 == 0 and detection_idx == 0:
                                                # Diagnostic: Show when team info is missing (only for first detection to avoid spam)
                                                if detection_team is None:
                                                    print(f"  â„¹ Detection #{detection_idx} has no team classification (team classification may be failing)")
                                                if player_team is None and not is_coach:
                                                    print(f"  â„¹ Player '{player_name}' has no team assigned in gallery (set team in Player Gallery)")
                                            
                                            # Check 3: Coaches should never be assigned to teams
                                            if is_coach and (player_team or detection_team):
                                                team_conflict = True
                                                if frame_data.get('frame_num', 0) % 100 == 0:
                                                    print(f"  ðŸƒ Coach exclusion: {player_name} is a coach - cannot be assigned to team")
                                            
                                            # Check 4: If detection has a team but player doesn't have a team assigned, allow it
                                            # But if player has a team and detection team doesn't match, it's a conflict
                                            # This handles cases where team info might be missing
                                            
                                            # If jersey conflict OR team conflict OR player conflict, try next best match
                                            if (jersey_conflict or team_conflict or player_conflict) and detection_idx in all_matches_per_detection:
                                                # Find next best match without jersey or team conflict
                                                # Use GUI Re-ID similarity threshold for alternative matches
                                                for alt_match in all_matches_per_detection[detection_idx]:
                                                    # Unpack alternative match (pname, pjersey, pteam, sim)
                                                    if len(alt_match) == 4:
                                                        alt_name, alt_jersey, alt_player_team, alt_sim = alt_match
                                                    else:
                                                        # Backward compatibility: old format (pname, pjersey, sim)
                                                        alt_name, alt_jersey, alt_sim = alt_match[:3]
                                                        alt_player_team = None
                                                    
                                                    # CRITICAL: Use gallery threshold for alternative matches too (prevents false positives)
                                                    # Alternative matches must meet gallery threshold, not just Re-ID threshold
                                                    if alt_sim >= gallery_similarity_threshold:
                                                        
                                                        # Check if this alternative has jersey conflict
                                                        alt_jersey_conflict = False
                                                        if alt_jersey and alt_jersey in jersey_to_track_global:
                                                            alt_assigned_track = jersey_to_track_global[alt_jersey]
                                                            if alt_assigned_track != track_id:
                                                                # Check if jersey is available for reassignment
                                                                if alt_jersey not in jerseys_available_for_reassignment:
                                                                    alt_jersey_conflict = True
                                                        
                                                        # Check if this alternative has player conflict (same player on different track)
                                                        alt_player_conflict = False
                                                        if alt_name and alt_name in player_to_track_global:
                                                            alt_assigned_track = player_to_track_global[alt_name]
                                                            if alt_assigned_track != track_id:
                                                                # Check if assigned track is still active
                                                                if alt_assigned_track in active_track_ids:
                                                                    # ðŸŽ¯ CONFIDENCE-BASED CONFLICT RESOLUTION for alternative matches too
                                                                    alt_existing_confidence = 0.0
                                                                    if alt_assigned_track in track_name_confidence:
                                                                        _, alt_existing_confidence, _ = track_name_confidence[alt_assigned_track]
                                                                    
                                                                    alt_new_confidence = alt_sim
                                                                    alt_confidence_diff = alt_new_confidence - alt_existing_confidence
                                                                    
                                                                    # Same rules: only allow if significantly better (0.20+) or existing is low (<0.50) and new is decent (>=0.60)
                                                                    if alt_confidence_diff < 0.20 and not (alt_existing_confidence < 0.50 and alt_new_confidence >= 0.60):
                                                                        alt_player_conflict = True
                                                                        if frame_data.get('frame_num', 0) % 50 == 0:
                                                                            print(f"  âš  ALT BLOCKED: {alt_name} already on Track #{alt_assigned_track} ({alt_existing_confidence:.2f}), rejecting Track #{track_id} ({alt_new_confidence:.2f})")
                                                        
                                                        # Check if this alternative has team conflict
                                                        alt_team_conflict = False
                                                        
                                                        # COACH CHECK: Coaches should not be assigned to teams
                                                        is_coach_alt = alt_name and any(coach.lower() in alt_name.lower() for coach in coach_names)
                                                        if is_coach_alt and (alt_player_team or detection_team):
                                                            alt_team_conflict = True
                                                        
                                                        # TEAM PERSISTENCE: If track has persistent team, alternative must match
                                                        persistent_team = track_to_team_persistent.get(track_id_int, None)
                                                        if persistent_team and not is_coach_alt:
                                                            if alt_player_team and alt_player_team != persistent_team:
                                                                alt_team_conflict = True
                                                            if detection_team and detection_team != persistent_team:
                                                                # Override detection_team with persistent team
                                                                detection_team = persistent_team
                                                        
                                                        # Check 1: Is alternative player already on a different team?
                                                        if not alt_team_conflict and alt_name in player_to_team_global:
                                                            alt_assigned_team, alt_assigned_track_id = player_to_team_global[alt_name]
                                                            if alt_assigned_track_id != track_id and alt_assigned_team is not None:
                                                                if detection_team and detection_team != alt_assigned_team:
                                                                    alt_team_conflict = True
                                                        # Check 2: Does detection team match alternative player team?
                                                        if not alt_team_conflict and detection_team and alt_player_team:
                                                            if detection_team != alt_player_team:
                                                                alt_team_conflict = True
                                                        
                                                        # CRITICAL: Check for pending corrections on alternative match
                                                        alt_has_correction = False
                                                        try:
                                                            import shared_state
                                                            pending_corrections_alt = shared_state.get_pending_corrections()
                                                            if track_id_int in pending_corrections_alt:
                                                                alt_correct_player = pending_corrections_alt[track_id_int]
                                                                if alt_correct_player is None:
                                                                    # This track should be unassigned - skip this alternative
                                                                    alt_has_correction = True
                                                                elif alt_correct_player != alt_name:
                                                                    # User wants different player - use correction
                                                                    alt_name = alt_correct_player
                                                                    # Get jersey and team from gallery
                                                                    if player_gallery and alt_name in [p.name for p in player_gallery.players.values()]:
                                                                        for pid, profile in player_gallery.players.items():
                                                                            if profile.name == alt_name:
                                                                                alt_jersey = profile.jersey_number
                                                                                alt_player_team = profile.team
                                                                                break
                                                        except:
                                                            pass
                                                        
                                                        if not alt_jersey_conflict and not alt_team_conflict and not alt_player_conflict and not alt_has_correction:
                                                            # ðŸ›¡ï¸ FINAL PROTECTION CHECK: Before assigning alternative match, verify track is not protected
                                                            alt_current_frame = frame_data.get('frame_num', 0)
                                                            if track_id_int in track_anchor_protection:
                                                                alt_prot_name, alt_prot_start, alt_prot_end = track_anchor_protection[track_id_int]
                                                                if alt_prot_start <= alt_current_frame <= alt_prot_end:
                                                                    if alt_prot_name != alt_name:
                                                                        # Track is protected for different player - BLOCK alternative match
                                                                        alt_prot_center = (alt_prot_start + alt_prot_end) // 2
                                                                        alt_dist_from_center = abs(alt_current_frame - alt_prot_center)
                                                                        
                                                                        # HARD PROTECTION: Never allow different player
                                                                        if alt_dist_from_center <= ANCHOR_HARD_PROTECTION_FRAMES:
                                                                            # Log every block - user needs to see protection is working
                                                                            print(f"  ðŸ›¡ï¸ FINAL PROTECTION BLOCK (ALT): Track #{track_id_int} is HARD protected for '{alt_prot_name}' - BLOCKING alternative '{alt_name}'")
                                                                            continue  # Skip this alternative match
                                                                        
                                                                        # SOFT/DECAY: Still block unless similarity is extremely high (0.85+)
                                                                        if alt_dist_from_center <= ANCHOR_DECAY_FRAMES:
                                                                            if alt_sim < 0.85:
                                                                                # Log every block - user needs to see protection is working
                                                                                print(f"  ðŸ›¡ï¸ FINAL PROTECTION BLOCK (ALT): Track #{track_id_int} is protected for '{alt_prot_name}' - BLOCKING alternative '{alt_name}' (similarity {alt_sim:.2f} < 0.85)")
                                                                                continue  # Skip - not high enough confidence
                                                            
                                                            # Use this alternative match!
                                                            
                                                            # If updating an existing name, release the old jersey first
                                                            if allow_update and track_id_int in track_name_confidence:
                                                                old_name, old_similarity, old_frame = track_name_confidence[track_id_int]
                                                                # Find and release old jersey
                                                                for jersey, tid in list(jersey_to_track_global.items()):
                                                                    if tid == track_id_int:
                                                                        # Only delete if the new jersey is different
                                                                        if jersey != alt_jersey:
                                                                            del jersey_to_track_global[jersey]
                                                                        break
                                                            
                                                            # CRITICAL: Normalize player name before storing and tracking
                                                            alt_name_clean = extract_player_name(alt_name)
                                                            player_names[pid_str] = alt_name_clean
                                                            
                                                            # ðŸŽ¯ UNTAGGED PLAYER IDENTIFICATION TRACKING: Track when untagged players are identified (alternative path)
                                                            # Check if this player is NOT in anchor frames (untagged player)
                                                            is_untagged_player_alt = alt_name_clean not in players_in_anchor_frames_set
                                                            current_frame_num_alt = frame_data.get('frame_num', 0)
                                                            
                                                            if is_untagged_player_alt:
                                                                # Track untagged player identification statistics (use normalized name)
                                                                if alt_name_clean not in untagged_player_stats:
                                                                    untagged_player_stats[alt_name_clean] = {
                                                                        'first_identified_frame': current_frame_num_alt,
                                                                        'last_identified_frame': current_frame_num_alt,
                                                                        'total_frames': 1,
                                                                        'avg_confidence': alt_sim,
                                                                        'max_confidence': alt_sim,
                                                                        'min_confidence': alt_sim,
                                                                        'track_ids': {track_id_int},
                                                                        'frames_learned': 0
                                                                    }
                                                                else:
                                                                    # Update existing stats
                                                                    stats = untagged_player_stats[alt_name_clean]
                                                                    stats['last_identified_frame'] = current_frame_num_alt
                                                                    stats['total_frames'] += 1
                                                                    # Update average confidence (running average)
                                                                    stats['avg_confidence'] = ((stats['avg_confidence'] * (stats['total_frames'] - 1)) + alt_sim) / stats['total_frames']
                                                                    stats['max_confidence'] = max(stats['max_confidence'], alt_sim)
                                                                    stats['min_confidence'] = min(stats['min_confidence'], alt_sim)
                                                                    stats['track_ids'].add(track_id_int)
                                                                
                                                                # Log untagged player identification (first time only)
                                                                if untagged_player_stats[alt_name_clean]['total_frames'] == 1:
                                                                    print(f"  ðŸŽ¯ UNTAGGED PLAYER IDENTIFIED: '{alt_name_clean}' on Track #{track_id} (similarity: {alt_sim:.3f}, threshold: {gallery_similarity_threshold:.3f})")
                                                            
                                                            # ðŸŽ“ LEARN FROM HIGH-CONFIDENCE UNTAGGED MATCHES: Add features to gallery (alternative path)
                                                            # Only learn if similarity >0.75 (high confidence) and player is untagged
                                                            if is_untagged_player_alt and alt_sim >= 0.75 and player_gallery is not None and reid_features is not None and detection_idx < len(reid_features):
                                                                try:
                                                                    untagged_feature_alt = reid_features[detection_idx]
                                                                    
                                                                    # Only update if feature is valid (no NaN)
                                                                    if not np.isnan(untagged_feature_alt).any():
                                                                        # Find player ID in gallery (use normalized name for matching)
                                                                        untagged_player_id_alt = None
                                                                        for pid, profile in player_gallery.players.items():
                                                                            # Normalize both names for comparison
                                                                            profile_name_clean = extract_player_name(profile.name)
                                                                            if profile_name_clean == alt_name_clean:
                                                                                untagged_player_id_alt = pid
                                                                                break
                                                                        
                                                                        # If not found, create new player with normalized name
                                                                        if untagged_player_id_alt is None:
                                                                            untagged_player_id_alt = alt_name_clean.lower().replace(" ", "_")
                                                                            player_gallery.add_player(
                                                                                name=alt_name_clean,
                                                                                player_id=untagged_player_id_alt
                                                                            )
                                                                        
                                                                        if untagged_player_id_alt is not None:
                                                                            # Get dominant color if available
                                                                            learned_dominant_color_alt = None
                                                                            if 'detection_dominant_colors' in locals() and detection_idx < len(detection_dominant_colors):
                                                                                learned_dominant_color_alt = detection_dominant_colors[detection_idx]
                                                                            
                                                                            # Extract uniform info
                                                                            uniform_info_for_update_alt = None
                                                                            if 'uniform_info_list' in locals() and detection_idx < len(uniform_info_list):
                                                                                uniform_info_for_update_alt = uniform_info_list[detection_idx]
                                                                            
                                                                            # Extract body and jersey features
                                                                            body_features_for_gallery_alt = None
                                                                            jersey_features_for_gallery_alt = None
                                                                            if reid_tracker is not None and detection_idx < len(detections) and YOLO_AVAILABLE:
                                                                                try:
                                                                                    frame_for_reid_alt = batch_frame
                                                                                    original_frame_alt = frame_data.get('original_frame_for_learning', None)
                                                                                    if original_frame_alt is not None:
                                                                                        frame_for_reid_alt = original_frame_alt
                                                                                    elif roi_bounds is not None:
                                                                                        full_frame_alt = frame_data.get('full_frame', None)
                                                                                        if full_frame_alt is not None:
                                                                                            frame_for_reid_alt = full_frame_alt
                                                                                    
                                                                                    single_bbox_alt = detections.xyxy[detection_idx:detection_idx+1]
                                                                                    single_detection_alt = sv.Detections(xyxy=single_bbox_alt)
                                                                                    
                                                                                    body_features_for_gallery_alt = reid_tracker.extract_body_features(
                                                                                        frame_for_reid_alt, single_detection_alt)
                                                                                    if body_features_for_gallery_alt is not None and len(body_features_for_gallery_alt) > 0:
                                                                                        body_features_for_gallery_alt = body_features_for_gallery_alt[0]
                                                                                    
                                                                                    jersey_features_for_gallery_alt = reid_tracker.extract_jersey_features(
                                                                                        frame_for_reid_alt, single_detection_alt)
                                                                                    if jersey_features_for_gallery_alt is not None and len(jersey_features_for_gallery_alt) > 0:
                                                                                        jersey_features_for_gallery_alt = jersey_features_for_gallery_alt[0]
                                                                                except Exception as e:
                                                                                    if current_frame_num_alt % 200 == 0:
                                                                                        print(f"  âš  Could not extract body/jersey features for untagged player (alt): {e}")
                                                                            
                                                                            # Update gallery with untagged player features (5x weight)
                                                                            player_gallery.update_player(
                                                                                player_id=untagged_player_id_alt,
                                                                                features=untagged_feature_alt.reshape(1, -1),
                                                                                dominant_color=learned_dominant_color_alt,
                                                                                reference_frame={
                                                                                    'frame_num': current_frame_num_alt,
                                                                                    'video_path': input_path,
                                                                                    'bbox': detections.xyxy[detection_idx_for_learning].tolist() if detection_idx_for_learning is not None and detection_idx_for_learning < len(detections.xyxy) else None,
                                                                                    'similarity': alt_sim,
                                                                                    'confidence': alt_sim,
                                                                                    'is_anchor': False
                                                                                },
                                                                                uniform_info=uniform_info_for_update_alt,
                                                                                body_features=body_features_for_gallery_alt,
                                                                                jersey_features=jersey_features_for_gallery_alt,
                                                                                feature_weight=5.0  # 5x weight
                                                                            )
                                                                            
                                                                            # Track learning statistics (use normalized name)
                                                                            if alt_name_clean not in untagged_learning_stats:
                                                                                untagged_learning_stats[alt_name_clean] = {
                                                                                    'frames_learned': 0,
                                                                                    'total_features': 0,
                                                                                    'first_frame': current_frame_num_alt,
                                                                                    'last_frame': current_frame_num_alt,
                                                                                    'avg_confidence': alt_sim
                                                                                }
                                                                            
                                                                            stats_alt = untagged_learning_stats[alt_name_clean]
                                                                            stats_alt['frames_learned'] += 1
                                                                            stats_alt['total_features'] += 1
                                                                            stats_alt['last_frame'] = current_frame_num_alt
                                                                            stats_alt['avg_confidence'] = ((stats_alt['avg_confidence'] * (stats_alt['frames_learned'] - 1)) + alt_sim) / stats_alt['frames_learned']
                                                                            
                                                                            # Update untagged_player_stats (use normalized name)
                                                                            if alt_name_clean in untagged_player_stats:
                                                                                untagged_player_stats[alt_name_clean]['frames_learned'] += 1
                                                                            
                                                                            if current_frame_num_alt % 100 == 0:
                                                                                print(f"  ðŸŽ“ UNTAGGED LEARNING: Updated gallery for '{alt_name_clean}' from Track #{track_id} (similarity: {alt_sim:.3f}, 5x weight)")
                                                                        else:
                                                                            if current_frame_num_alt % 200 == 0:
                                                                                print(f"  âš  Could not find '{alt_name}' in gallery for untagged learning (alt)")
                                                                except Exception as e:
                                                                    if current_frame_num_alt % 200 == 0:
                                                                        print(f"  âš  Could not learn from untagged player '{alt_name}' (alt): {e}")
                                                            
                                                            # LOCKED ROUTE: Only lock routes for players who are in anchor frames
                                                            # This prevents route locking for players not in the video (e.g., Jax Derryberry)
                                                            current_frame = frame_data.get('frame_num', 0)
                                                            
                                                            # Check if player appears in any anchor frame
                                                            player_in_anchor_frames = False
                                                            if anchor_frames:
                                                                for frame_num, anchors in anchor_frames.items():
                                                                    if anchors and isinstance(anchors, list):
                                                                        for anchor in anchors:
                                                                            if anchor.get('player_name') == alt_name:
                                                                                player_in_anchor_frames = True
                                                                                break
                                                                    if player_in_anchor_frames:
                                                                        break
                                                            
                                                            # Only lock route if player is in anchor frames
                                                            # If anchor_frames is empty/None, don't lock (prevents locking players not in video)
                                                            if player_in_anchor_frames:
                                                                # LOCKED ROUTE: If this is an early-frame assignment, lock the route
                                                                # NOTE: Coaches can be tracked (for movement analysis), but won't be assigned to teams
                                                                try:
                                                                    import shared_state
                                                                    # ADAPTIVE THRESHOLD: 30 seconds worth of frames (adjusts for fps)
                                                                    # At 30fps: 30s * 30fps = 900 frames
                                                                    # At 120fps: 30s * 120fps = 3600 frames
                                                                    early_frame_threshold_adaptive = int(30 * fps) if fps > 0 else 1000
                                                                    if shared_state.lock_early_route(alt_name, track_id_int, current_frame, early_frame_threshold=early_frame_threshold_adaptive):
                                                                        # Only print if this is a NEW lock (not already locked)
                                                                        if is_processing and current_frame <= early_frame_threshold_adaptive:
                                                                            print(f"  ðŸ”’ ROUTE LOCKED: {alt_name} â†’ Track #{track_id_int} (Frame {current_frame}) - this track will be preferred for future matches")
                                                                except:
                                                                    pass  # Silently fail if shared_state not available
                                                            else:
                                                                # Player is NOT in anchor frames - don't lock route
                                                                if is_processing and current_frame <= 10:
                                                                    print(f"  âš  SKIPPING ROUTE LOCK: '{alt_name}' is not in anchor frames - route will not be locked")
                                                            if alt_jersey:
                                                                jersey_to_track_global[alt_jersey] = track_id
                                                            # CRITICAL: Store player-to-track mapping to prevent duplicates
                                                            # If updating, release old player assignment first
                                                            if allow_update and track_id_int in track_name_confidence:
                                                                old_name, old_similarity, old_frame = track_name_confidence[track_id_int]
                                                                if old_name in player_to_track_global and player_to_track_global[old_name] == track_id_int:
                                                                    del player_to_track_global[old_name]
                                                            player_to_track_global[alt_name] = track_id
                                                            
                                                            # Store team assignment globally (player can only be on one team)
                                                            # COACH CHECK: Coaches should not be assigned to teams
                                                            is_coach_alt = alt_name and any(coach.lower() in alt_name.lower() for coach in coach_names)
                                                            if alt_player_team and not is_coach_alt:
                                                                # If updating, release old team assignment first
                                                                if allow_update and track_id_int in track_name_confidence:
                                                                    old_name, old_similarity, old_frame = track_name_confidence[track_id_int]
                                                                    if old_name in player_to_team_global:
                                                                        old_team, old_track_id = player_to_team_global[old_name]
                                                                        if old_track_id == track_id_int:
                                                                            del player_to_team_global[old_name]
                                                                    # Release old track-to-team mapping
                                                                    if track_id_int in track_to_team_global:
                                                                        del track_to_team_global[track_id_int]
                                                                player_to_team_global[alt_name] = (alt_player_team, track_id)
                                                                # TEAM PERSISTENCE: Store track-to-team mapping
                                                                track_to_team_global[track_id_int] = alt_player_team
                                                            elif is_coach_alt:
                                                                # Coach - ensure no team assignment
                                                                if track_id_int in track_to_team_global:
                                                                    del track_to_team_global[track_id_int]
                                                                if alt_name in player_to_team_global:
                                                                    del player_to_team_global[alt_name]
                                                            # If alt_player_team is None and not a coach, don't store team assignment
                                                            
                                                            # ðŸ”‹ SUPERPOWER REPLENISHMENT: Check EVERY Re-ID match (alternative path)
                                                            current_frame_alt = frame_data.get('frame_num', 0)
                                                            
                                                            # Check BOTH track_name_confidence AND player_names for existing assignment
                                                            old_assigned_name_alt = None
                                                            if track_id_int in track_name_confidence:
                                                                old_assigned_name_alt = track_name_confidence[track_id_int][0]
                                                            elif pid_str in player_names:
                                                                # Name exists in player_names but not in confidence yet (e.g., forced by anchor protection)
                                                                old_assigned_name_alt = player_names[pid_str]
                                                            
                                                            is_confirmation_alt = (old_assigned_name_alt is not None and old_assigned_name_alt == alt_name)
                                                            is_high_confidence_new_alt = (old_assigned_name_alt is None and alt_sim >= 0.75)
                                                            
                                                            # Refresh protection on EVERY confirmation (runs every frame)
                                                            if is_confirmation_alt or is_high_confidence_new_alt:
                                                                new_protection_start_alt = max(0, current_frame_alt - ANCHOR_DECAY_FRAMES)
                                                                new_protection_end_alt = current_frame_alt + ANCHOR_DECAY_FRAMES
                                                                
                                                                if track_id_int in track_anchor_protection:
                                                                    old_name_alt, old_start_alt, old_end_alt = track_anchor_protection[track_id_int]
                                                                    if old_name_alt == alt_name:
                                                                        merged_start_alt = min(old_start_alt, new_protection_start_alt)
                                                                        merged_end_alt = max(old_end_alt, new_protection_end_alt)
                                                                        track_anchor_protection[track_id_int] = (alt_name, merged_start_alt, merged_end_alt)
                                                                        if current_frame_alt % 200 == 0:
                                                                            total_frames = merged_end_alt - merged_start_alt
                                                                            print(f"  ðŸ”‹ SUPERPOWER REPLENISHED: {alt_name} Track #{track_id} protection extended to {total_frames} total frames!")
                                                                elif is_high_confidence_new_alt:
                                                                    track_anchor_protection[track_id_int] = (alt_name, new_protection_start_alt, new_protection_end_alt)
                                                                    if current_frame_alt % 200 == 0:
                                                                        print(f"  ðŸ›¡ï¸ NEW PROTECTION ACTIVATED: {alt_name} Track #{track_id} protected for Â±{ANCHOR_DECAY_FRAMES}f (confidence: {alt_sim:.2f})")
                                                            
                                                            # CRITICAL: Only store/update confidence when assigning NEW names or UPDATING names
                                                            if not has_real_name or allow_update:
                                                                current_frame_num_alt = frame_data.get('frame_num', 0)
                                                                track_name_confidence[track_id_int] = (alt_name, alt_sim, current_frame_num_alt)
                                                                
                                                                # ðŸŽ¯ HIGH-CONFIDENCE HISTORY TRACKING: Track long successful Re-ID history
                                                                # If Re-ID has been working correctly for many frames, extend protection automatically
                                                                if alt_sim >= HIGH_CONFIDENCE_THRESHOLD:
                                                                    if track_id_int in track_high_confidence_history:
                                                                        hist_name, first_frame, last_frame, consecutive_frames = track_high_confidence_history[track_id_int]
                                                                        if hist_name == alt_name:
                                                                            # Same player, still high confidence - extend history
                                                                            track_high_confidence_history[track_id_int] = (alt_name, first_frame, current_frame_num_alt, consecutive_frames + 1)
                                                                        else:
                                                                            # Different player - reset history
                                                                            track_high_confidence_history[track_id_int] = (alt_name, current_frame_num_alt, current_frame_num_alt, 1)
                                                                    else:
                                                                        # First high-confidence assignment - start tracking
                                                                        track_high_confidence_history[track_id_int] = (alt_name, current_frame_num_alt, current_frame_num_alt, 1)
                                                                else:
                                                                    # Confidence dropped below threshold - clear history
                                                                    if track_id_int in track_high_confidence_history:
                                                                        del track_high_confidence_history[track_id_int]
                                                                
                                                                # CRITICAL FIX: Ensure player_names is synced with track_name_confidence
                                                                # This ensures the display pipeline shows the correct name
                                                                player_names[pid_str] = alt_name
                                                                
                                                                # ENHANCEMENT: Mirror jersey + team assignments when player_names syncs
                                                                if alt_jersey:
                                                                    jersey_to_track_global[alt_jersey] = track_id_int
                                                                # Check if coach (coaches shouldn't be assigned to teams)
                                                                coach_names_check = {"Kevin Hill", "Coach", "coach"}  # Standard coach names
                                                                is_coach_check_alt = alt_name and any(coach.lower() in alt_name.lower() for coach in coach_names_check)
                                                                if alt_team and not is_coach_check_alt:
                                                                    track_to_team_global[track_id_int] = alt_team
                                                                    player_to_team_global[alt_name] = (alt_team, track_id_int)
                                                                
                                                                # Optional debug logging (dev mode - can be disabled)
                                                                if frame_data.get('frame_num', 0) % 500 == 0:
                                                                    print(f"  ðŸ”„ Synced name (alt): Track #{track_id_int} â†’ {alt_name} (jersey: {alt_jersey or 'N/A'}, team: {alt_team or 'N/A'})")
                                                                
                                                                update_str = " [UPDATED]" if allow_update else ""
                                                                conflict_str = " [resolved jersey conflict]" if jersey_conflict else ""
                                                                conflict_str += " [resolved team conflict]" if team_conflict else ""
                                                                # Only print if we're still in the main processing loop
                                                                if is_processing:
                                                                    print(f"  âœ“ Gallery match applied: Track #{track_id} = {alt_name} #{alt_jersey or '?'} (similarity: {alt_sim:.2f}){conflict_str}{update_str}")
                                                                
                                                                # AUTOMATIC LEARNING: Learn team colors from high-confidence alternative matches
                                                                # OPTIMIZATION: Stop learning once we have enough samples (50 per team)
                                                                # FOCUS MODE: Only learn team colors from focused players
                                                                should_learn_team_alt = (focused_players_set is None or alt_name in focused_players_set)
                                                                
                                                                # Check if we already have enough samples for this team
                                                                team_has_enough_alt = False
                                                                if alt_player_team and alt_player_team in learned_colors_by_team:
                                                                    team_has_enough_alt = len(learned_colors_by_team[alt_player_team]) >= 50
                                                                
                                                                if should_learn_team_alt and not team_has_enough_alt and alt_sim >= 0.7 and detection_idx < len(detection_dominant_colors):
                                                                    detection_color = detection_dominant_colors[detection_idx]
                                                                    
                                                                    if detection_color is not None:
                                                                        # Determine which team to learn for:
                                                                        # 1. If detection has a team, use that
                                                                        # 2. Otherwise, if player has a team in gallery, use that
                                                                        # 3. This allows learning even when team classification is failing
                                                                        team_to_learn = None
                                                                        if detection_idx < len(detection_teams):
                                                                            detection_team = detection_teams[detection_idx]
                                                                            if detection_team:
                                                                                team_to_learn = detection_team
                                                                            elif alt_player_team:
                                                                                team_to_learn = alt_player_team
                                                                        elif alt_player_team:
                                                                            team_to_learn = alt_player_team
                                                                        
                                                                        if team_to_learn:
                                                                            if team_to_learn not in learned_colors_by_team:
                                                                                learned_colors_by_team[team_to_learn] = []
                                                                            # Only append if we haven't reached the limit (50 samples)
                                                                            if len(learned_colors_by_team[team_to_learn]) < 50:
                                                                                learned_colors_by_team[team_to_learn].append(detection_color.tolist() if isinstance(detection_color, np.ndarray) else detection_color)
                                                            break
                                                else:
                                                    # No conflict-free alternative found
                                                    if frame_data.get('frame_num', 0) % 100 == 0:
                                                        conflict_types = []
                                                        if jersey_conflict:
                                                            conflict_types.append("jersey")
                                                        if team_conflict:
                                                            conflict_types.append("team")
                                                        if player_conflict:
                                                            conflict_types.append("player")
                                                        conflict_str = " and ".join(conflict_types) if conflict_types else "conflicts"
                                                        print(f"  âœ— Track #{track_id}: No valid alternative match (all top matches have {conflict_str} conflicts)")
                                            # CRITICAL: Check for pending corrections BEFORE applying match
                                            has_pending_correction = False
                                            try:
                                                import shared_state
                                                pending_corrections_check = shared_state.get_pending_corrections()
                                                if track_id_int in pending_corrections_check:
                                                    correct_player_check = pending_corrections_check[track_id_int]
                                                    if correct_player_check is None:
                                                        # This track should be unassigned - skip this match entirely
                                                        has_pending_correction = True
                                                        if frame_data.get('frame_num', 0) % 100 == 0:
                                                            print(f"  ðŸ”§ USER CORRECTION: Track #{track_id} skipped (unassign from conflict resolution)")
                                                    elif correct_player_check != player_name:
                                                        # User wants different player - update before applying
                                                        player_name = correct_player_check
                                                        # Get jersey and team from gallery
                                                        if player_gallery and player_name in [p.name for p in player_gallery.players.values()]:
                                                            for pid, profile in player_gallery.players.items():
                                                                if profile.name == player_name:
                                                                    jersey_number = profile.jersey_number
                                                                    player_team = profile.team
                                                                    break
                                            except:
                                                pass
                                            
                                            if not jersey_conflict and not team_conflict and not player_conflict and not has_pending_correction:
                                                # ðŸ›¡ï¸ CRITICAL PROTECTION CHECK #1: Check anchor frame status FIRST (before any assignment)
                                                # This must happen BEFORE any assignment logic - anchor frames are permanent
                                                alt_current_frame = frame_data.get('frame_num', 0)
                                                
                                                # Check 1: Does track have anchor frame confidence (1.00)?
                                                alt_is_anchor_frame = False
                                                if track_id_int in track_name_confidence:
                                                    _, alt_confidence, _ = track_name_confidence[track_id_int]
                                                    if alt_confidence >= 1.00:
                                                        alt_is_anchor_frame = True
                                                        alt_anchor_name, _, _ = track_name_confidence[track_id_int]
                                                        print(f"  ðŸ”’ ANCHOR FRAME BLOCK (ALT): Track #{track_id_int} has anchor frame '{alt_anchor_name}' (confidence 1.00) - BLOCKING alternative match '{player_name}'")
                                                        continue  # Skip this alternative - anchor frames are permanent
                                                
                                                # Check 2: Is track in anchor protection window?
                                                alt_is_anchor_protected = False
                                                if not alt_is_anchor_frame and track_id_int in track_anchor_protection:
                                                    alt_prot_name, alt_prot_start, alt_prot_end = track_anchor_protection[track_id_int]
                                                    if alt_prot_start <= alt_current_frame <= alt_prot_end:
                                                        alt_is_anchor_protected = True
                                                        if alt_prot_name != player_name:
                                                            alt_prot_center = (alt_prot_start + alt_prot_end) // 2
                                                            alt_dist_from_center = abs(alt_current_frame - alt_prot_center)
                                                            
                                                            # HARD PROTECTION: Never allow different player
                                                            if alt_dist_from_center <= ANCHOR_HARD_PROTECTION_FRAMES:
                                                                print(f"  ðŸ›¡ï¸ ANCHOR PROTECTION BLOCK (ALT): Track #{track_id_int} is HARD protected for '{alt_prot_name}' (frames {alt_prot_start}-{alt_prot_end}) - BLOCKING alternative '{player_name}'")
                                                                continue  # Skip this alternative - hard protection
                                                            
                                                            # SOFT/DECAY: Still block unless similarity is extremely high (0.90+)
                                                            if alt_dist_from_center <= ANCHOR_DECAY_FRAMES:
                                                                if similarity < 0.90:
                                                                    print(f"  ðŸ›¡ï¸ ANCHOR PROTECTION BLOCK (ALT): Track #{track_id_int} is protected for '{alt_prot_name}' - BLOCKING alternative '{player_name}' (similarity {similarity:.2f} < 0.90)")
                                                                    continue  # Skip - not high enough confidence
                                                
                                                # ðŸ›¡ï¸ FINAL PROTECTION CHECK: Before assigning, verify track is not protected for different player
                                                # This is the LAST line of defense - prevents any bypass of earlier protection checks
                                                current_frame_final = frame_data.get('frame_num', 0)
                                                if track_id_int in track_anchor_protection:
                                                    final_prot_name, final_prot_start, final_prot_end = track_anchor_protection[track_id_int]
                                                    if final_prot_start <= current_frame_final <= final_prot_end:
                                                        if final_prot_name != player_name:
                                                            # Track is protected for different player - BLOCK assignment
                                                            final_prot_center = (final_prot_start + final_prot_end) // 2
                                                            final_dist_from_center = abs(current_frame_final - final_prot_center)
                                                            
                                                            # HARD PROTECTION: Never allow different player
                                                            if final_dist_from_center <= ANCHOR_HARD_PROTECTION_FRAMES:
                                                                print(f"  ðŸ›¡ï¸ FINAL PROTECTION BLOCK: Track #{track_id_int} is HARD protected for '{final_prot_name}' (f{current_frame_final}) - BLOCKING '{player_name}' assignment (bypass attempt blocked)")
                                                                continue  # Skip this assignment completely
                                                            
                                                            # SOFT/DECAY: Still block unless similarity is extremely high (0.85+)
                                                            if final_dist_from_center <= ANCHOR_DECAY_FRAMES:
                                                                if similarity < 0.85:  # Require very high confidence to override in decay zone
                                                                    # Log every block - user needs to see protection is working
                                                                    print(f"  ðŸ›¡ï¸ FINAL PROTECTION BLOCK: Track #{track_id_int} is protected for '{final_prot_name}' (f{current_frame_final}) - BLOCKING '{player_name}' (similarity {similarity:.2f} < 0.85 required)")
                                                                    continue  # Skip - not high enough confidence
                                                
                                                # No conflict - apply the match and register jersey and team GLOBALLY!
                                                
                                                # If updating an existing name, release the old jersey and team first
                                                if allow_update and track_id_int in track_name_confidence:
                                                    old_name, old_similarity, old_frame = track_name_confidence[track_id_int]
                                                    # Find and release old jersey
                                                    old_jersey = None
                                                    for jersey, tid in list(jersey_to_track_global.items()):
                                                        if tid == track_id_int:
                                                            old_jersey = jersey
                                                            # Only delete if the new jersey is different
                                                            if jersey != jersey_number:
                                                                del jersey_to_track_global[jersey]
                                                            break
                                                    # Release old team assignment
                                                    if old_name in player_to_team_global:
                                                        old_team, old_track_id = player_to_team_global[old_name]
                                                        if old_track_id == track_id_int:
                                                            del player_to_team_global[old_name]
                                                    # Release old player assignment
                                                    if old_name in player_to_track_global:
                                                        if player_to_track_global[old_name] == track_id_int:
                                                            del player_to_track_global[old_name]
                                                
                                                player_names[pid_str] = player_name
                                                
                                                # ðŸ›¡ï¸ DRIFT PREVENTION: Update identity lock tracking (Grok's Fix #1)
                                                if similarity >= 0.75:  # High confidence match
                                                    if track_id_int not in track_confirmed_identity:
                                                        track_confirmed_identity[track_id_int] = (player_name, current_frame_num, similarity, 1)
                                                    elif track_confirmed_identity[track_id_int][0] == player_name:
                                                        # Same player, strengthen lock
                                                        lock_name, lock_frame, lock_conf, match_count = track_confirmed_identity[track_id_int]
                                                        track_confirmed_identity[track_id_int] = (player_name, lock_frame, max(similarity, lock_conf), match_count + 1)
                                                        # If we've hit the lock threshold, log it
                                                        if match_count + 1 >= LOCK_AFTER_N_CONFIDENT_MATCHES:
                                                            if frame_data.get('frame_num', 0) % 100 == 0:
                                                                print(f"  ðŸ”’ IDENTITY LOCKED: Track #{track_id_int} '{player_name}' confirmed {match_count + 1}x (similarity: {max(similarity, lock_conf):.3f}) - identity locked")
                                                
                                                # ðŸ›¡ï¸ DRIFT PREVENTION: Update jersey color signature (Grok's Fix #4)
                                                if detection_idx < len(detection_dominant_colors) and detection_dominant_colors[detection_idx] is not None:
                                                    track_jersey_signatures[track_id_int] = detection_dominant_colors[detection_idx]
                                                
                                                # ðŸ›¡ï¸ DRIFT PREVENTION: Update track positions history for motion consistency (Grok's Fix #2)
                                                if detection_idx < len(detections.xyxy):
                                                    det_bbox = detections.xyxy[detection_idx]
                                                    det_center_x = (det_bbox[0] + det_bbox[2]) / 2
                                                    det_center_y = (det_bbox[1] + det_bbox[3]) / 2
                                                    if track_id_int not in track_positions_history:
                                                        track_positions_history[track_id_int] = deque(maxlen=10)
                                                    track_positions_history[track_id_int].append((current_frame_num, det_center_x, det_center_y, det_bbox.tolist()))
                                                
                                                # ðŸ›¡ï¸ DRIFT PREVENTION: Update previous bbox for size consistency (Grok's Fix #3)
                                                if detection_idx < len(detections.xyxy):
                                                    track_previous_bbox[track_id_int] = detections.xyxy[detection_idx].copy()
                                                
                                                # ðŸ›¡ï¸ DRIFT PREVENTION: Update embedding smoothing (Grok's Fix #6)
                                                if reid_features is not None and detection_idx < len(reid_features):
                                                    if track_id_int not in track_reid_embeddings:
                                                        track_reid_embeddings[track_id_int] = deque(maxlen=10)
                                                    track_reid_embeddings[track_id_int].append(reid_features[detection_idx].copy())
                                                
                                                # ðŸŽ¯ UNTAGGED PLAYER IDENTIFICATION TRACKING: Track when untagged players are identified
                                                # Check if this player is NOT in anchor frames (untagged player)
                                                is_untagged_player = player_name not in players_in_anchor_frames_set
                                                current_frame_num = frame_data.get('frame_num', 0)
                                                
                                                if is_untagged_player:
                                                    # Track untagged player identification statistics
                                                    if player_name not in untagged_player_stats:
                                                        untagged_player_stats[player_name] = {
                                                            'first_identified_frame': current_frame_num,
                                                            'last_identified_frame': current_frame_num,
                                                            'total_frames': 1,
                                                            'avg_confidence': similarity,
                                                            'max_confidence': similarity,
                                                            'min_confidence': similarity,
                                                            'track_ids': {track_id_int},
                                                            'frames_learned': 0
                                                        }
                                                    else:
                                                        # Update existing stats
                                                        stats = untagged_player_stats[player_name]
                                                        stats['last_identified_frame'] = current_frame_num
                                                        stats['total_frames'] += 1
                                                        # Update average confidence (running average)
                                                        stats['avg_confidence'] = ((stats['avg_confidence'] * (stats['total_frames'] - 1)) + similarity) / stats['total_frames']
                                                        stats['max_confidence'] = max(stats['max_confidence'], similarity)
                                                        stats['min_confidence'] = min(stats['min_confidence'], similarity)
                                                        stats['track_ids'].add(track_id_int)
                                                    
                                                    # Log untagged player identification (first time only)
                                                    if untagged_player_stats[player_name]['total_frames'] == 1:
                                                        print(f"  ðŸŽ¯ UNTAGGED PLAYER IDENTIFIED: '{player_name}' on Track #{track_id} (similarity: {similarity:.3f}, threshold: {gallery_similarity_threshold:.3f})")
                                                
                                                # ðŸŽ“ LEARN FROM HIGH-CONFIDENCE UNTAGGED MATCHES: Add features to gallery
                                                # Only learn if similarity >0.75 (high confidence) and player is untagged
                                                if is_untagged_player and similarity >= 0.75 and player_gallery is not None and reid_features is not None and detection_idx < len(reid_features):
                                                    try:
                                                        untagged_feature = reid_features[detection_idx]
                                                        
                                                        # Only update if feature is valid (no NaN)
                                                        if not np.isnan(untagged_feature).any():
                                                            # Find player ID in gallery
                                                            untagged_player_id = None
                                                            for pid, profile in player_gallery.players.items():
                                                                if profile.name == player_name:
                                                                    untagged_player_id = pid
                                                                    break
                                                            
                                                            if untagged_player_id is not None:
                                                                # Get dominant color if available
                                                                learned_dominant_color = None
                                                                if 'detection_dominant_colors' in locals() and detection_idx < len(detection_dominant_colors):
                                                                    learned_dominant_color = detection_dominant_colors[detection_idx]
                                                                
                                                                # Extract uniform info
                                                                uniform_info_for_update = None
                                                                if 'uniform_info_list' in locals() and detection_idx < len(uniform_info_list):
                                                                    uniform_info_for_update = uniform_info_list[detection_idx]
                                                                
                                                                # Extract body and jersey features for high-quality images
                                                                body_features_for_gallery = None
                                                                jersey_features_for_gallery = None
                                                                if reid_tracker is not None and detection_idx < len(detections) and YOLO_AVAILABLE:
                                                                    try:
                                                                        # Get frame for Re-ID
                                                                        frame_for_reid = batch_frame
                                                                        original_frame = frame_data.get('original_frame_for_learning', None)
                                                                        if original_frame is not None:
                                                                            frame_for_reid = original_frame
                                                                        elif roi_bounds is not None:
                                                                            full_frame = frame_data.get('full_frame', None)
                                                                            if full_frame is not None:
                                                                                frame_for_reid = full_frame
                                                                        
                                                                        # Create single detection for this player
                                                                        single_bbox = detections.xyxy[detection_idx:detection_idx+1]
                                                                        single_detection = sv.Detections(xyxy=single_bbox)
                                                                        
                                                                        # Extract body features (full bbox)
                                                                        body_features_for_gallery = reid_tracker.extract_body_features(
                                                                            frame_for_reid, single_detection)
                                                                        if body_features_for_gallery is not None and len(body_features_for_gallery) > 0:
                                                                            body_features_for_gallery = body_features_for_gallery[0]
                                                                        
                                                                        # Extract jersey features (torso region)
                                                                        jersey_features_for_gallery = reid_tracker.extract_jersey_features(
                                                                            frame_for_reid, single_detection)
                                                                        if jersey_features_for_gallery is not None and len(jersey_features_for_gallery) > 0:
                                                                            jersey_features_for_gallery = jersey_features_for_gallery[0]
                                                                    except Exception as e:
                                                                        # Don't fail if feature extraction fails
                                                                        if current_frame_num % 200 == 0:
                                                                            print(f"  âš  Could not extract body/jersey features for untagged player: {e}")
                                                                
                                                                # Update gallery with untagged player features (5x weight - less than anchor frames but still significant)
                                                                player_gallery.update_player(
                                                                    player_id=untagged_player_id,
                                                                    features=untagged_feature.reshape(1, -1),
                                                                    dominant_color=learned_dominant_color,
                                                                    reference_frame={
                                                                        'frame_num': current_frame_num,
                                                                        'video_path': input_path,
                                                                        'bbox': detections.xyxy[detection_idx_for_learning].tolist() if detection_idx_for_learning is not None and detection_idx_for_learning < len(detections.xyxy) else None,
                                                                        'similarity': similarity,
                                                                        'confidence': similarity,  # Use similarity as confidence for untagged matches
                                                                        'is_anchor': False  # Not an anchor frame
                                                                    },
                                                                    uniform_info=uniform_info_for_update,
                                                                    body_features=body_features_for_gallery,
                                                                    jersey_features=jersey_features_for_gallery,
                                                                    feature_weight=5.0  # 5x weight (less than anchor's 10x but still significant)
                                                                )
                                                                
                                                                # Track learning statistics
                                                                if player_name not in untagged_learning_stats:
                                                                    untagged_learning_stats[player_name] = {
                                                                        'frames_learned': 0,
                                                                        'total_features': 0,
                                                                        'first_frame': current_frame_num,
                                                                        'last_frame': current_frame_num,
                                                                        'avg_confidence': similarity
                                                                    }
                                                                
                                                                stats = untagged_learning_stats[player_name]
                                                                stats['frames_learned'] += 1
                                                                stats['total_features'] += 1
                                                                stats['last_frame'] = current_frame_num
                                                                # Update average confidence (running average)
                                                                stats['avg_confidence'] = ((stats['avg_confidence'] * (stats['frames_learned'] - 1)) + similarity) / stats['frames_learned']
                                                                
                                                                # Also update untagged_player_stats if it exists
                                                                if player_name in untagged_player_stats:
                                                                    untagged_player_stats[player_name]['frames_learned'] = stats['frames_learned']
                                                                
                                                                # Update untagged_player_stats
                                                                if player_name in untagged_player_stats:
                                                                    untagged_player_stats[player_name]['frames_learned'] += 1
                                                                
                                                                if current_frame_num % 100 == 0:
                                                                    print(f"  ðŸŽ“ UNTAGGED LEARNING: Updated gallery for '{player_name}' from Track #{track_id} (similarity: {similarity:.3f}, 5x weight)")
                                                            else:
                                                                if current_frame_num % 200 == 0:
                                                                    print(f"  âš  Could not find '{player_name}' in gallery for untagged learning")
                                                    except Exception as e:
                                                        # Silently fail - learning is optional
                                                        if current_frame_num % 200 == 0:
                                                            print(f"  âš  Could not learn from untagged player '{player_name}': {e}")
                                                
                                                # DEBUG: Log player assignments to diagnose "everyone tagged as same player" issue
                                                if frame_data.get('frame_num', 0) % 100 == 0:
                                                    untagged_tag = " [UNTAGGED]" if is_untagged_player else ""
                                                    print(f"  âœ“ ASSIGNED: Track #{track_id} = {player_name} (similarity: {similarity:.3f}, threshold: {gallery_similarity_threshold:.3f}){untagged_tag}")
                                                
                                                # LOCKED ROUTE: If this is an early-frame assignment, lock the route
                                                # NOTE: Coaches can be tracked (for movement analysis), but won't be assigned to teams
                                                current_frame = frame_data.get('frame_num', 0)
                                                try:
                                                    import shared_state
                                                    # ADAPTIVE THRESHOLD: 30 seconds worth of frames (adjusts for fps)
                                                    # At 30fps: 30s * 30fps = 900 frames
                                                    # At 120fps: 30s * 120fps = 3600 frames
                                                    early_frame_threshold_adaptive = int(30 * fps) if fps > 0 else 1000
                                                    if shared_state.lock_early_route(player_name, track_id_int, current_frame, early_frame_threshold=early_frame_threshold_adaptive):
                                                        # Only print if this is a NEW lock (not already locked)
                                                        if is_processing and current_frame <= early_frame_threshold_adaptive:
                                                            print(f"  ðŸ”’ ROUTE LOCKED: {player_name} â†’ Track #{track_id_int} (Frame {current_frame}) - this track will be preferred for future matches")
                                                except:
                                                    pass  # Silently fail if shared_state not available
                                                
                                                if jersey_number:
                                                    jersey_to_track_global[jersey_number] = track_id
                                                # CRITICAL: Store player-to-track mapping to prevent duplicates
                                                player_to_track_global[player_name] = track_id
                                                
                                                # Store team assignment globally (player can only be on one team)
                                                # COACH CHECK: Coaches should not be assigned to teams
                                                is_coach = player_name and any(coach.lower() in player_name.lower() for coach in coach_names)
                                                if player_team and not is_coach:
                                                    player_to_team_global[player_name] = (player_team, track_id)
                                                    # TEAM PERSISTENCE: Store track-to-team mapping
                                                    track_to_team_global[track_id_int] = player_team
                                                    
                                                    # ENHANCEMENT: Auto-lock team in game mode (no mid-game switches)
                                                    if video_type == "game":
                                                        # In game mode, lock the team immediately - player stays on this team
                                                        if frame_data.get('frame_num', 0) % 200 == 0:
                                                            print(f"  ðŸ”’ðŸ”’ GAME MODE: Team locked for {player_name} â†’ {player_team} (Track #{track_id})")
                                                elif is_coach:
                                                    # Coach - ensure no team assignment
                                                    if track_id_int in track_to_team_global:
                                                        del track_to_team_global[track_id_int]
                                                    if player_name in player_to_team_global:
                                                        del player_to_team_global[player_name]
                                                
                                                # CHECK FOR PENDING CORRECTIONS: Override with user correction if available
                                                # CRITICAL: Check BEFORE assigning to prevent conflicts
                                                try:
                                                    import shared_state
                                                    pending_corrections = shared_state.get_pending_corrections()
                                                    if track_id_int in pending_corrections:
                                                        correct_player = pending_corrections[track_id_int]
                                                        if correct_player is None:
                                                            # None means "unassign this player" (from conflict resolution)
                                                            if frame_data.get('frame_num', 0) % 100 == 0:
                                                                print(f"  ðŸ”§ USER CORRECTION: Track #{track_id} '{player_name}' â†’ UNASSIGNED (from conflict resolution)")
                                                            # Clear this track's assignment from all global mappings
                                                            # Remove from player_to_track_global
                                                            if track_id_int in player_to_track_global.values():
                                                                for pname, tid in list(player_to_track_global.items()):
                                                                    if tid == track_id_int:
                                                                        del player_to_track_global[pname]
                                                                        break
                                                            # Remove from jersey_to_track_global
                                                            if track_id_int in jersey_to_track_global.values():
                                                                for jersey, tid in list(jersey_to_track_global.items()):
                                                                    if tid == track_id_int:
                                                                        del jersey_to_track_global[jersey]
                                                                        break
                                                            # Remove from player_to_team_global
                                                            if track_id_int in track_to_team_global:
                                                                old_team = track_to_team_global[track_id_int]
                                                                del track_to_team_global[track_id_int]
                                                                # Also remove from player_to_team_global if this track was the one assigned
                                                                for pname, (team, tid) in list(player_to_team_global.items()):
                                                                    if tid == track_id_int:
                                                                        del player_to_team_global[pname]
                                                                        break
                                                            # Remove from player_names
                                                            if pid_str in player_names:
                                                                del player_names[pid_str]
                                                            # Remove from track_name_confidence
                                                            if track_id_int in track_name_confidence:
                                                                del track_name_confidence[track_id_int]
                                                            # Don't assign this player - skip this match
                                                            continue
                                                        elif correct_player != player_name:
                                                            print(f"  ðŸ”§ USER CORRECTION: Track #{track_id} '{player_name}' â†’ '{correct_player}' (applied from live viewer)")
                                                            player_name = correct_player
                                                            # BREADCRUMB: Store this correction as a track preference
                                                            try:
                                                                import shared_state
                                                                shared_state.set_player_track_breadcrumb(correct_player, track_id_int, confidence=0.8)
                                                            except:
                                                                pass
                                                            # Update gallery match to use corrected player
                                                            # Find player in gallery
                                                            if player_gallery and correct_player in [p.name for p in player_gallery.players.values()]:
                                                                # Get correct player's jersey number and team
                                                                for pid, profile in player_gallery.players.items():
                                                                    if profile.name == correct_player:
                                                                        jersey_number = profile.jersey_number
                                                                        player_team = profile.team
                                                                        break
                                                            # Also update player_names dictionary for labeling
                                                            # CRITICAL: Check if correct_player is already assigned to a different track
                                                            if correct_player in player_to_track_global:
                                                                old_track_id = player_to_track_global[correct_player]
                                                                if old_track_id != track_id_int:
                                                                    # Player is on a different track - clear it first
                                                                    old_track_str = str(int(old_track_id))
                                                                    if old_track_str in player_names:
                                                                        player_names[old_track_str] = ""  # Clear old track
                                                                    if old_track_id in track_name_confidence:
                                                                        del track_name_confidence[old_track_id]
                                                                    print(f"  ðŸ—‘ï¸ UNIQUENESS (correction): Cleared '{correct_player}' from Track #{old_track_id} (correction assigns to Track #{track_id_int})")
                                                            
                                                            track_id_str = str(track_id_int)
                                                            if track_id_str not in player_names or player_names[track_id_str] != correct_player:
                                                                player_names[track_id_str] = correct_player
                                                                player_to_track_global[correct_player] = track_id_int
                                                                print(f"  âœ“ Updated player_names[{track_id_str}] = '{correct_player}'")
                                                except Exception as e:
                                                    pass  # Silently fail if corrections not available
                                                
                                                # ðŸ”‹ SUPERPOWER REPLENISHMENT: Check EVERY Re-ID match (runs on EVERY frame)
                                                # This happens BEFORE confidence update so it works even when not updating
                                                current_frame_check = frame_data.get('frame_num', 0)
                                                
                                                # Check BOTH track_name_confidence AND player_names for existing assignment
                                                # This catches cases where anchor protection forced the name but confidence wasn't updated yet
                                                old_assigned_name = None
                                                if track_id_int in track_name_confidence:
                                                    old_assigned_name = track_name_confidence[track_id_int][0]
                                                elif pid_str in player_names:
                                                    # Name exists in player_names but not in confidence yet (e.g., forced by anchor protection)
                                                    old_assigned_name = player_names[pid_str]
                                                
                                                is_confirmation = (old_assigned_name is not None and old_assigned_name == player_name)
                                                is_high_confidence_new = (old_assigned_name is None and similarity >= 0.75)
                                                
                                                # Refresh protection on EVERY confirmation or high-confidence match
                                                # This ensures protection never expires as long as Re-ID keeps confirming
                                                if is_confirmation or is_high_confidence_new:
                                                    # Calculate new protection window from THIS frame
                                                    new_protection_start = max(0, current_frame_check - ANCHOR_DECAY_FRAMES)
                                                    new_protection_end = current_frame_check + ANCHOR_DECAY_FRAMES
                                                    
                                                    if track_id_int in track_anchor_protection:
                                                        # EXTEND existing protection - merge with new window
                                                        old_name, old_start, old_end = track_anchor_protection[track_id_int]
                                                        if old_name == player_name:  # Only extend if same player
                                                            # Expand protection window to cover both old and new
                                                            merged_start = min(old_start, new_protection_start)
                                                            merged_end = max(old_end, new_protection_end)
                                                            track_anchor_protection[track_id_int] = (player_name, merged_start, merged_end)
                                                            
                                                            if current_frame_check % 200 == 0:  # Occasional logging
                                                                total_protected_frames = merged_end - merged_start
                                                                print(f"  ðŸ”‹ SUPERPOWER REPLENISHED: {player_name} Track #{track_id} protection extended to {total_protected_frames} total frames!")
                                                    elif is_high_confidence_new:
                                                        # CREATE new protection for high-confidence assignments (â‰¥0.75)
                                                        track_anchor_protection[track_id_int] = (player_name, new_protection_start, new_protection_end)
                                                        if current_frame_check % 200 == 0:
                                                            print(f"  ðŸ›¡ï¸ NEW PROTECTION ACTIVATED: {player_name} Track #{track_id} protected for Â±{ANCHOR_DECAY_FRAMES}f (confidence: {similarity:.2f})")
                                                
                                                # CRITICAL: Only store/update confidence when assigning NEW names or UPDATING names
                                                # Don't update confidence every frame for the same name (prevents flickering)
                                                if not has_real_name or allow_update:
                                                    current_frame_num_main = frame_data.get('frame_num', 0)
                                                    # ðŸ• TEMPORAL CONFIDENCE: Apply temporal boost to final confidence
                                                    # The similarity has already been boosted by temporal evidence in the gallery matching section
                                                    # Here we ensure the final confidence reflects temporal consistency
                                                    final_confidence = min(1.0, similarity)  # Cap at 1.0
                                                    track_name_confidence[track_id_int] = (player_name, final_confidence, current_frame_num_main)
                                                    
                                                    # Log temporal confidence if it was boosted
                                                    if final_confidence > similarity - 0.01:  # If confidence was boosted (allowing for rounding)
                                                        if current_frame_num_main % 100 == 0:
                                                            print(f"  ðŸ• TEMPORAL FINAL: Track #{track_id_int} '{player_name}' - final confidence: {final_confidence:.3f} (includes temporal evidence)")
                                                    
                                                    # ðŸŽ¯ HIGH-CONFIDENCE HISTORY TRACKING: Track long successful Re-ID history
                                                    # If Re-ID has been working correctly for many frames, extend protection automatically
                                                    if similarity >= HIGH_CONFIDENCE_THRESHOLD:
                                                        if track_id_int in track_high_confidence_history:
                                                            hist_name, first_frame, last_frame, consecutive_frames = track_high_confidence_history[track_id_int]
                                                            if hist_name == player_name:
                                                                # Same player, still high confidence - extend history
                                                                track_high_confidence_history[track_id_int] = (player_name, first_frame, current_frame_num_main, consecutive_frames + 1)
                                                            else:
                                                                # Different player - reset history
                                                                track_high_confidence_history[track_id_int] = (player_name, current_frame_num_main, current_frame_num_main, 1)
                                                        else:
                                                            # First high-confidence assignment - start tracking
                                                            track_high_confidence_history[track_id_int] = (player_name, current_frame_num_main, current_frame_num_main, 1)
                                                    else:
                                                        # Confidence dropped below threshold - clear history
                                                        if track_id_int in track_high_confidence_history:
                                                            del track_high_confidence_history[track_id_int]
                                                    
                                                    # CRITICAL FIX: Ensure player_names is synced with track_name_confidence
                                                    # This ensures the display pipeline shows the correct name
                                                    player_names[pid_str] = player_name
                                                    
                                                    # ENHANCEMENT: Mirror jersey + team assignments when player_names syncs
                                                    # This ensures consistent global state across all mappings
                                                    if jersey_number:
                                                        jersey_to_track_global[jersey_number] = track_id_int
                                                    # Check if coach (coaches shouldn't be assigned to teams)
                                                    coach_names_check = {"Kevin Hill", "Coach", "coach"}  # Standard coach names
                                                    is_coach_check = player_name and any(coach.lower() in player_name.lower() for coach in coach_names_check)
                                                    if player_team and not is_coach_check:
                                                        track_to_team_global[track_id_int] = player_team
                                                        player_to_team_global[player_name] = (player_team, track_id_int)
                                                    
                                                    # Optional debug logging (dev mode - can be disabled)
                                                    if frame_data.get('frame_num', 0) % 500 == 0:  # Log every 500 frames to reduce spam
                                                        print(f"  ðŸ”„ Synced name: Track #{track_id_int} â†’ {player_name} (jersey: {jersey_number or 'N/A'}, team: {player_team or 'N/A'})")
                                                    
                                                    # GALLERY BREADCRUMB: Update track history in gallery
                                                    if player_gallery and player_id:
                                                        try:
                                                            player_gallery.update_track_history(player_id, track_id_int)
                                                        except:
                                                            pass  # Silently fail if method not available
                                                    
                                                    update_str = " [UPDATED]" if allow_update else ""
                                                    # Only log gallery matches periodically to reduce console spam (every 100 frames or on updates)
                                                    # IMPORTANT: Only print if we're still in the main processing loop
                                                    current_frame = frame_data.get('frame_num', 0)
                                                    if is_processing and (allow_update or current_frame % 100 == 0):
                                                        print(f"  âœ“ Gallery match applied: Track #{track_id} = {player_name} #{jersey_number or '?'} (similarity: {similarity:.2f}){update_str}")
                                                    
                                                    # UPDATE GUI: Send track assignment to live viewer controls with frame/bbox info for anchor frames
                                                    try:
                                                        import shared_state
                                                        # Get bbox from detection if available
                                                        bbox = None
                                                        if detection_idx < len(detections.xyxy):
                                                            xyxy = detections.xyxy[detection_idx]
                                                            bbox = [float(xyxy[0]), float(xyxy[1]), float(xyxy[2]), float(xyxy[3])]
                                                        
                                                        # Update shared state with frame and bbox info for anchor frame creation
                                                        shared_state.update_track_assignment(
                                                            track_id_int, 
                                                            player_name,
                                                            frame_num=current_frame,
                                                            bbox=bbox,
                                                            team=player_team,
                                                            jersey=jersey_number
                                                        )
                                                        
                                                        live_viewer_controls = shared_state.get_live_viewer_controls()
                                                        if live_viewer_controls:
                                                            live_viewer_controls.update_track_assignment(track_id_int, player_name)
                                                    except:
                                                        pass
                                                    
                                                    # AUTOMATIC LEARNING: Learn team colors from high-confidence matches
                                                    # Only learn from high-confidence matches (>= 0.7) to ensure quality
                                                    # OPTIMIZATION: Stop learning once we have enough samples (50 per team)
                                                    # FOCUS MODE: Only learn team colors from focused players
                                                    should_learn_team = (focused_players_set is None or player_name in focused_players_set)
                                                    
                                                    # Check if we already have enough samples for this team
                                                    team_has_enough = False
                                                    if player_team and player_team in learned_colors_by_team:
                                                        team_has_enough = len(learned_colors_by_team[player_team]) >= 50
                                                    
                                                    if should_learn_team and not team_has_enough and similarity >= 0.7 and detection_idx < len(detection_dominant_colors):
                                                        detection_color = detection_dominant_colors[detection_idx]
                                                        
                                                        if detection_color is not None:
                                                            # Determine which team to learn for:
                                                            # 1. If detection has a team, use that
                                                            # 2. Otherwise, if player has a team in gallery, use that
                                                            # 3. This allows learning even when team classification is failing
                                                            team_to_learn = None
                                                            if detection_idx < len(detection_teams):
                                                                detection_team = detection_teams[detection_idx]
                                                                if detection_team:
                                                                    team_to_learn = detection_team
                                                                elif player_team:
                                                                    team_to_learn = player_team
                                                            elif player_team:
                                                                team_to_learn = player_team
                                                            
                                                            if team_to_learn:
                                                                if team_to_learn not in learned_colors_by_team:
                                                                    learned_colors_by_team[team_to_learn] = []
                                                                # Only append if we haven't reached the limit (50 samples)
                                                                if len(learned_colors_by_team[team_to_learn]) < 50:
                                                                    learned_colors_by_team[team_to_learn].append(detection_color.tolist() if isinstance(detection_color, np.ndarray) else detection_color)
                                                    
                                                    # AUTOMATIC PLAYER FEATURE LEARNING: Update player gallery with new high-confidence features
                                                    # Only learn from very high-confidence matches (>= 0.8) to ensure quality
                                                    # FOCUS MODE: Only learn from focused players
                                                    # PERFORMANCE: Only learn every 5 frames to reduce overhead
                                                    should_learn = (focused_players_set is None or player_name in focused_players_set)
                                                    current_frame = frame_data.get('frame_num', 0)
                                                    learn_this_frame = (current_frame % 5 == 0)  # Only learn every 5 frames
                                                    
                                                    # CRITICAL: Track player names matched per frame to prevent impossible duplicates
                                                    # Initialize frame-level tracking if not exists (frame_data is a dict, use dict key)
                                                    if '_player_matches_this_frame' not in frame_data:
                                                        frame_data['_player_matches_this_frame'] = {}  # player_name -> [detection_indices]
                                                    
                                                    # Check if this player name is already matched in this frame (impossible scenario)
                                                    if player_name in frame_data['_player_matches_this_frame']:
                                                        # Same player matched multiple times in same frame - reject all matches
                                                        existing_indices = frame_data['_player_matches_this_frame'][player_name]
                                                        if current_frame % 100 == 0:  # Log occasionally
                                                            print(f"   âš  REJECTED: '{player_name}' matched {len(existing_indices) + 1} times in frame {current_frame} (impossible - rejecting all)")
                                                        # Don't learn from this match - it's likely a false positive
                                                        should_learn = False
                                                    else:
                                                        # First match for this player in this frame - track it
                                                        frame_data['_player_matches_this_frame'][player_name] = [detection_idx]
                                                    
                                                    # CRITICAL FIX: Check that reid_features is not None before accessing len()
                                                    # Also require higher similarity (0.85) to prevent low-confidence pollution
                                                    if should_learn and similarity >= 0.85 and reid_features is not None and detection_idx < len(reid_features) and player_gallery and learn_this_frame:
                                                        try:
                                                            # Get player ID from name
                                                            player_id = player_name.lower().replace(' ', '_')
                                                            detection_feature = reid_features[detection_idx]
                                                            
                                                            # Only update if feature is valid (no NaN)
                                                            if not np.isnan(detection_feature).any():
                                                                # Get dominant color if available
                                                                learned_dominant_color = None
                                                                if detection_idx < len(detection_dominant_colors):
                                                                    learned_dominant_color = detection_dominant_colors[detection_idx]
                                                                
                                                                # Extract uniform info for this detection
                                                                uniform_info_for_update = None
                                                                if detection_idx < len(uniform_info_list) and uniform_info_list[detection_idx]:
                                                                    uniform_info_for_update = uniform_info_list[detection_idx]
                                                                
                                                                # Extract body and jersey features for highest quality images
                                                                body_features_for_gallery = None
                                                                jersey_features_for_gallery = None
                                                                if reid_tracker is not None and detection_idx < len(detections) and YOLO_AVAILABLE:
                                                                    try:
                                                                        # Create single detection for this player
                                                                        single_bbox = detections.xyxy[detection_idx:detection_idx+1]
                                                                        single_detection = sv.Detections(xyxy=single_bbox)
                                                                        
                                                                        # Extract body features (full bbox) - only for high quality matches
                                                                        if similarity >= 0.75:  # Only extract for high confidence matches
                                                                            body_features_for_gallery = reid_tracker.extract_body_features(
                                                                                frame_for_reid, single_detection)
                                                                            if body_features_for_gallery is not None and len(body_features_for_gallery) > 0:
                                                                                body_features_for_gallery = body_features_for_gallery[0]
                                                                        
                                                                        # Extract jersey features (torso region) - only for high quality matches
                                                                        if similarity >= 0.75:  # Only extract for high confidence matches
                                                                            jersey_features_for_gallery = reid_tracker.extract_jersey_features(
                                                                                frame_for_reid, single_detection)
                                                                            if jersey_features_for_gallery is not None and len(jersey_features_for_gallery) > 0:
                                                                                jersey_features_for_gallery = jersey_features_for_gallery[0]
                                                                    except Exception as e:
                                                                        # Don't fail if feature extraction fails
                                                                        pass
                                                                
                                                                # Update player gallery with new feature (averages with existing)
                                                                player_gallery.update_player(
                                                                    player_id=player_id,
                                                                    features=detection_feature.reshape(1, -1),  # Reshape to (1, feature_dim)
                                                                    dominant_color=learned_dominant_color,
                                                                    reference_frame={
                                                                        'frame_num': frame_data.get('frame_num', 0),
                                                                        'video_path': input_path,
                                                                        'bbox': detections.xyxy[detection_idx_for_learning].tolist() if detection_idx_for_learning is not None and detection_idx_for_learning < len(detections.xyxy) else None,
                                                                        'similarity': float(similarity),  # Store similarity score for quality filtering
                                                                        'confidence': float(detections.confidence[detection_idx]) if detection_idx < len(detections.confidence) else 0.0  # Store detection confidence
                                                                    },
                                                                    uniform_info=uniform_info_for_update,  # UNIFORM VARIANTS: Store uniform info for organizing reference frames by uniform type
                                                                    body_features=body_features_for_gallery,
                                                                    jersey_features=jersey_features_for_gallery,
                                                                    foot_features=foot_features_for_gallery  # Add foot features for shoe recognition
                                                                )
                                                                # Log learning more frequently but with less detail
                                                                if current_frame % 300 == 0:  # Log every 300 frames
                                                                    # Count reference frames for this player
                                                                    profile = player_gallery.get_player(player_id)
                                                                    ref_count = len(profile.reference_frames) if profile and profile.reference_frames else 0
                                                                    print(f"   ðŸ“š Auto-learned features for {player_name} (similarity: {similarity:.2f}, {ref_count} ref frames)")
                                                                
                                                                # LEARN SHAPE FEATURES: From bounding box
                                                                if detection_idx < len(detections.xyxy):
                                                                    bbox = detections.xyxy[detection_idx].tolist()
                                                                    player_gallery.learn_shape_features(player_id, bbox)
                                                                
                                                                # LEARN POSITION PREFERENCES: From field position
                                                                if detection_idx < len(detections.xyxy):
                                                                    x1, y1, x2, y2 = detections.xyxy[detection_idx]
                                                                    center_x = (x1 + x2) / 2
                                                                    center_y = (y1 + y2) / 2
                                                                    # Use frame dimensions as field dimensions (approximation)
                                                                    player_gallery.learn_position_preferences(
                                                                        player_id, center_x, center_y, 
                                                                        width, height
                                                                    )
                                                                    
                                                                    # LEARN MOVEMENT FEATURES: Calculate velocity from previous position
                                                                    current_frame = frame_data.get('frame_num', 0)
                                                                    if track_id_int in track_previous_positions:
                                                                        prev_x, prev_y, prev_frame, *_ = track_previous_positions[track_id_int]
                                                                        frame_diff = max(1, current_frame - prev_frame)  # Avoid division by zero
                                                                        
                                                                        # Calculate velocity (pixels per frame)
                                                                        dx = center_x - prev_x
                                                                        dy = center_y - prev_y
                                                                        velocity = np.sqrt(dx**2 + dy**2) / frame_diff
                                                                        
                                                                        # Calculate acceleration (if we have previous velocity)
                                                                        acceleration = None
                                                                        if track_id_int in track_previous_positions and len(track_previous_positions[track_id_int]) > 3:
                                                                            prev_velocity = track_previous_positions[track_id_int][3] if len(track_previous_positions[track_id_int]) > 3 else None
                                                                            if prev_velocity is not None:
                                                                                acceleration = (velocity - prev_velocity) / frame_diff
                                                                        
                                                                        # Learn movement features
                                                                        player_gallery.learn_movement_features(player_id, velocity, acceleration)
                                                                        
                                                                        # Update previous position with velocity
                                                                        track_previous_positions[track_id_int] = (center_x, center_y, current_frame, velocity)
                                                                    else:
                                                                        # First time seeing this track - initialize
                                                                        track_previous_positions[track_id_int] = (center_x, center_y, current_frame, 0.0)
                                                                    
                                                                    # LEARN BALL INTERACTION: Calculate distance to ball if available
                                                                    # Note: ball_center will be calculated later in the frame processing
                                                                    # We'll add ball interaction learning in a separate pass after ball tracking
                                                        except Exception as e:
                                                            # Silently fail - learning is optional
                                                            pass
                                        # Diagnostic: show when we're skipping because name exists
                                        elif frame_data.get('frame_num', 0) % 100 == 0:
                                            if existing_name != player_name:
                                                print(f"  â­ Skipping Track #{track_id}: already named '{existing_name}' (would be '{player_name}' @ {similarity:.2f} similarity)")

                        # AUTO-CREATE ANONYMOUS PLAYERS FROM TRACKS (watch-only mode)
                        # Create anonymous players from stable unmatched tracks to enable learning
                        if watch_only and player_gallery is not None and detections.tracker_id is not None:
                            current_frame = frame_data.get('frame_num', 0)
                            
                            # Track which tracks have matches in this frame
                            matched_tracks = set()
                            if 'gallery_match_cache' in locals():
                                for detection_idx in gallery_match_cache.keys():
                                    if detection_idx < len(detections.tracker_id):
                                        track_id = detections.tracker_id[detection_idx]
                                        if track_id is not None:
                                            matched_tracks.add(int(track_id))
                            
                            # Also check tracks that have real names (not generic "Player X")
                            for i, track_id in enumerate(detections.tracker_id):
                                if track_id is not None:
                                    track_id_int = int(track_id)
                                    pid_str = str(track_id_int)
                                    existing_name_raw = player_names.get(pid_str, "")
                                    # CRITICAL FIX: Handle case where existing_name might be a list
                                    if isinstance(existing_name_raw, list) and len(existing_name_raw) > 0:
                                        existing_name = str(existing_name_raw[0])
                                    elif existing_name_raw:
                                        existing_name = str(existing_name_raw)
                                    else:
                                        existing_name = ""
                                    # If track has a real name (from gallery or previous auto-creation), it's matched
                                    if existing_name and isinstance(existing_name, str) and not existing_name.strip().startswith("Player ") and not existing_name.strip().startswith("Anonymous Track "):
                                        matched_tracks.add(track_id_int)
                            
                            # Process all tracks in this frame
                            for i, track_id in enumerate(detections.tracker_id):
                                if track_id is not None:
                                    track_id_int = int(track_id)
                                    
                                    # Skip if already matched or already auto-created
                                    if track_id_int in matched_tracks or track_id_int in auto_created_players:
                                        # Reset frame count if matched
                                        if track_id_int in unmatched_track_frames:
                                            unmatched_track_frames[track_id_int] = 0
                                        continue
                                    
                                    # Increment unmatched frame count
                                    if track_id_int not in unmatched_track_frames:
                                        unmatched_track_frames[track_id_int] = 0
                                    unmatched_track_frames[track_id_int] += 1
                                    
                                    # Auto-create anonymous player if track has been stable long enough
                                    if unmatched_track_frames[track_id_int] >= min_frames_for_auto_create:
                                        if track_id_int not in auto_created_players:
                                            # Create anonymous player name
                                            anonymous_name = f"Anonymous Track #{track_id_int}"
                                            anonymous_player_id = f"anonymous_track_{track_id_int}"
                                            
                                            # Get detection info for this track
                                            if i < len(reid_features) and not np.isnan(reid_features[i]).any():
                                                detection_feature = reid_features[i]
                                                
                                                # Get team and color if available
                                                detection_team = detection_teams[i] if i < len(detection_teams) else None
                                                detection_color = detection_dominant_colors[i] if i < len(detection_dominant_colors) else None
                                                
                                                # Create player in gallery
                                                try:
                                                    if not player_gallery.has_player(anonymous_player_id):
                                                        player_gallery.add_player(
                                                            player_id=anonymous_player_id,
                                                            name=anonymous_name,
                                                            features=detection_feature.reshape(1, -1),
                                                            jersey_number=None,  # Will be assigned later
                                                            team=detection_team,
                                                            dominant_color=detection_color,
                                                            reference_frame={
                                                                'frame_num': current_frame,
                                                                'video_path': input_path,
                                                                'bbox': detections.xyxy[i].tolist() if i < len(detections.xyxy) else None,
                                                                'confidence': float(detections.confidence[i]) if i < len(detections.confidence) else 0.0,  # Store detection confidence for quality
                                                                'similarity': 0.5  # Default for auto-created (no match yet)
                                                            }
                                                        )
                                                        auto_created_players[track_id_int] = anonymous_player_id
                                                        print(f"  ðŸ¤– Auto-created anonymous player: {anonymous_name} (Track #{track_id_int}, {unmatched_track_frames[track_id_int]} frames)")
                                                    else:
                                                        # Player already exists, just mark as auto-created
                                                        auto_created_players[track_id_int] = anonymous_player_id
                                                except Exception as e:
                                                    # Silently fail - auto-creation is optional
                                                    if current_frame % 200 == 0:
                                                        print(f"  âš  Failed to auto-create player for Track #{track_id_int}: {e}")
                            
                            # Clean up old unmatched tracks (tracks that haven't been seen recently)
                            frames_to_keep = 300  # Keep track counts for 300 frames after last sighting
                            tracks_to_remove = []
                            for track_id_int, frame_count in unmatched_track_frames.items():
                                # If track hasn't been seen in current detections, it might be lost
                                if track_id_int not in [int(tid) for tid in detections.tracker_id if tid is not None]:
                                    # Check if we should remove it (only if it's been a while)
                                    if frame_count > frames_to_keep:
                                        tracks_to_remove.append(track_id_int)
                            
                            for track_id_int in tracks_to_remove:
                                del unmatched_track_frames[track_id_int]
                                # Don't remove from auto_created_players - keep the mapping for later assignment
                            
                            # LEARN FEATURES FOR AUTO-CREATED PLAYERS: Continuously learn from their tracks
                            for i, track_id in enumerate(detections.tracker_id):
                                if track_id is not None:
                                    track_id_int = int(track_id)
                                    
                                    # Check if this track has an auto-created player
                                    if track_id_int in auto_created_players:
                                        anonymous_player_id = auto_created_players[track_id_int]
                                        
                                        # Learn features for this auto-created player
                                        if i < len(reid_features) and not np.isnan(reid_features[i]).any() and player_gallery:
                                            try:
                                                detection_feature = reid_features[i]
                                                
                                                # Get team and color if available
                                                detection_team = detection_teams[i] if i < len(detection_teams) else None
                                                detection_color = detection_dominant_colors[i] if i < len(detection_dominant_colors) else None
                                                
                                                # Update player features (averages with existing)
                                                player_gallery.update_player(
                                                    player_id=anonymous_player_id,
                                                    features=detection_feature.reshape(1, -1),
                                                    dominant_color=detection_color,
                                                    reference_frame={
                                                        'frame_num': current_frame,
                                                        'video_path': input_path,
                                                        'bbox': detections.xyxy[i].tolist() if i < len(detections.xyxy) else None,
                                                        'confidence': float(detections.confidence[i]) if i < len(detections.confidence) else 0.0,  # Store detection confidence for quality
                                                        'similarity': 0.5  # Default for auto-created (no match yet)
                                                    }
                                                )
                                                
                                                # LEARN SHAPE, POSITION, MOVEMENT FEATURES for auto-created players
                                                if i < len(detections.xyxy):
                                                    bbox = detections.xyxy[i].tolist()
                                                    player_gallery.learn_shape_features(anonymous_player_id, bbox)
                                                    
                                                    x1, y1, x2, y2 = detections.xyxy[i]
                                                    center_x = (x1 + x2) / 2
                                                    center_y = (y1 + y2) / 2
                                                    player_gallery.learn_position_preferences(
                                                        anonymous_player_id, center_x, center_y, width, height
                                                    )
                                                    
                                                    # Learn movement features
                                                    if track_id_int in track_previous_positions:
                                                        prev_x, prev_y, prev_frame = track_previous_positions[track_id_int][:3]
                                                        frame_diff = max(1, current_frame - prev_frame)
                                                        dx = center_x - prev_x
                                                        dy = center_y - prev_y
                                                        velocity = np.sqrt(dx**2 + dy**2) / frame_diff
                                                        
                                                        acceleration = None
                                                        if len(track_previous_positions[track_id_int]) > 3:
                                                            prev_velocity = track_previous_positions[track_id_int][3]
                                                            if prev_velocity is not None:
                                                                acceleration = (velocity - prev_velocity) / frame_diff
                                                        
                                                        player_gallery.learn_movement_features(anonymous_player_id, velocity, acceleration)
                                                        track_previous_positions[track_id_int] = (center_x, center_y, current_frame, velocity)
                                                    else:
                                                        track_previous_positions[track_id_int] = (center_x, center_y, current_frame, 0.0)
                                                
                                                # Log learning progress periodically
                                                if current_frame % 500 == 0:
                                                    print(f"   ðŸ“š Learning features for {player_gallery.get_player(anonymous_player_id).name if player_gallery.has_player(anonymous_player_id) else 'Anonymous'} (Track #{track_id_int})")
                                            except Exception as e:
                                                # Silently fail - learning is optional
                                                pass

                        # Apply Re-ID matching for better ID persistence (with
                        # color features)
                        if reid_tracker is not None and reid_features is not None and len(
                                detections) > 0:
                            try:
                                # Update Re-ID tracker with features for
                                # existing tracks first (including color
                                # features)
                                reid_tracker.update_tracks(
                                    detections.tracker_id, reid_features, reid_color_features)

                                # For detections without IDs (new or lost), try
                                # to match using Re-ID
                                unmatched_indices = [
                                    i for i, tid in enumerate(
                                        detections.tracker_id) if tid is None]

                                # ENHANCED: Get all existing track IDs from Re-ID tracker (not just current frame)
                                # Include both tracks with Re-ID features AND
                                # tracks from track_state (recently lost)
                                all_existing_track_ids = list(
                                    reid_tracker.track_features.keys())

                                # CRITICAL FIX: Also include tracks from track_state that might not have Re-ID features yet
                                # This expands the matching pool for better
                                # reconnection
                                if len(track_state) > 0:
                                    track_state_ids = set(track_state.keys())
                                    reid_track_ids_set = set(
                                        all_existing_track_ids)
                                    # Add tracks from track_state that are
                                    # recently lost (within buffer time)
                                    for tid in track_state_ids:
                                        if tid not in reid_track_ids_set:
                                            # Track exists in track_state but not in Re-ID - might be recently lost
                                            # Add it to matching pool (Re-ID
                                            # will use position similarity if
                                            # no features)
                                            all_existing_track_ids.append(tid)

                                # ENHANCED: Increased limit from 50 to 100 tracks for better matching (was limiting too aggressively)
                                # If there are too many tracks, limit to most
                                # recent ones (this helps performance)
                                if len(all_existing_track_ids) > 100:
                                    # This is a workaround - ideally cleanup
                                    # should prevent this, but limit search
                                    # space
                                    # Last 100 tracks (increased from 50)
                                    all_existing_track_ids = all_existing_track_ids[-100:]

                                if len(unmatched_indices) > 0 and len(
                                        all_existing_track_ids) > 0:
                                    # Safety check: ensure reid_features has
                                    # enough elements
                                    if len(reid_features) == 0:
                                        print(
                                            f"âš  Re-ID features is empty, skipping matching")
                                        continue

                                    max_idx = max(
                                        unmatched_indices) if unmatched_indices else -1
                                    if max_idx >= len(reid_features):
                                        print(
                                            f"âš  Re-ID features length mismatch: {
                                                len(reid_features)} features but indices up to {max_idx}")
                                        continue

                                    # Get features for unmatched detections
                                    unmatched_features = reid_features[unmatched_indices]

                                    # Get color features for unmatched
                                    # detections
                                    unmatched_color_features = None
                                    if reid_color_features is not None:
                                        # Safety check for color features
                                        if (len(reid_color_features.get('team_color', [])) > 0 and
                                                max_idx < len(reid_color_features['team_color'])):
                                            ball_color_array = reid_color_features['ball_color'][unmatched_indices] if max_idx < len(
                                                reid_color_features.get('ball_color', [])) else np.zeros((len(unmatched_indices), 3))
                                            unmatched_color_features = {
                                                'team_color': reid_color_features['team_color'][unmatched_indices],
                                                'ball_color': ball_color_array
                                            }

                                    # Filter out NaN features
                                    valid_unmatched = []
                                    valid_unmatched_indices = []
                                    valid_unmatched_color = {
                                        'team_color': [], 'ball_color': []} if unmatched_color_features else None

                                    for idx, feat in zip(
                                            unmatched_indices, unmatched_features):
                                        if not np.isnan(feat).any():
                                            valid_unmatched.append(feat)
                                            valid_unmatched_indices.append(idx)
                                            if valid_unmatched_color and unmatched_color_features:
                                                valid_unmatched_color['team_color'].append(
                                                    unmatched_color_features['team_color'][len(valid_unmatched) - 1])
                                                valid_unmatched_color['ball_color'].append(
                                                    unmatched_color_features['ball_color'][len(valid_unmatched) - 1])

                                    if len(valid_unmatched) > 0:
                                        # Convert color lists to arrays if
                                        # available
                                        if valid_unmatched_color:
                                            valid_unmatched_color['team_color'] = np.array(
                                                valid_unmatched_color['team_color'])
                                            valid_unmatched_color['ball_color'] = np.array(
                                                valid_unmatched_color['ball_color'])

                                        # CRITICAL FIX: Add position similarity for better matching
                                        # Position is crucial for soccer -
                                        # players can't teleport across field
                                        position_similarity = None
                                        if len(valid_unmatched) > 0 and len(
                                                all_existing_track_ids) > 0:
                                            # Get positions of unmatched
                                            # detections
                                            unmatched_positions = []
                                            for original_idx in valid_unmatched_indices:
                                                # CRITICAL FIX: valid_unmatched_indices already contains original indices
                                                # No need to index into
                                                # unmatched_indices again
                                                if original_idx < len(
                                                        detections.xyxy):
                                                    x1, y1, x2, y2 = detections.xyxy[original_idx]
                                                    center_x = (x1 + x2) / 2
                                                    center_y = (y1 + y2) / 2
                                                    unmatched_positions.append(
                                                        [center_x, center_y])

                                            # Get last known positions of existing tracks from track_state
                                            # CRITICAL FIX: Only use tracks that have BOTH Re-ID features AND position data
                                            # This ensures position_similarity
                                            # has the same shape as
                                            # color_similarity
                                            existing_positions = []
                                            valid_existing_ids = []
                                            for track_id in all_existing_track_ids:
                                                # Only include tracks that have
                                                # both Re-ID features and
                                                # position data
                                                if track_id in track_state and track_id in reid_tracker.track_features:
                                                    last_xyxy = track_state[track_id]['xyxy']
                                                    center_x = (
                                                        last_xyxy[0] + last_xyxy[2]) / 2
                                                    center_y = (
                                                        last_xyxy[1] + last_xyxy[3]) / 2
                                                    existing_positions.append(
                                                        [center_x, center_y])
                                                    valid_existing_ids.append(
                                                        track_id)

                                            # Calculate position similarity
                                            # (inverse distance, normalized)
                                            if len(unmatched_positions) > 0 and len(
                                                    existing_positions) > 0:
                                                unmatched_positions = np.array(
                                                    unmatched_positions)
                                                existing_positions = np.array(
                                                    existing_positions)

                                                # Calculate pairwise distances
                                                # unmatched_positions: (N, 2), existing_positions: (M, 2)
                                                # Result: (N, M) distance
                                                # matrix
                                                distances = np.sqrt(
                                                    np.sum(
                                                        (unmatched_positions[:, np.newaxis, :] - existing_positions[np.newaxis, :, :]) ** 2, axis=2)
                                                )

                                                # Convert distances to similarity (closer = higher similarity)
                                                # ENHANCED: Increased max_distance for better matching (was 200, now 300 pixels)
                                                # Players can move further
                                                # during occlusions or when
                                                # processing every Nth frame
                                                max_distance = 300.0  # Increased from 200 for more lenient position matching
                                                position_similarity = 1.0 - \
                                                    np.clip(
                                                        distances / max_distance, 0, 1.0)

                                                # CRITICAL FIX: Update all_existing_track_ids to match valid_existing_ids
                                                # This ensures all similarity
                                                # matrices use the same set of
                                                # tracks
                                                all_existing_track_ids = valid_existing_ids
                                            else:
                                                # No position data available -
                                                # set position_similarity to
                                                # None
                                                position_similarity = None

                                        # Try to match to existing tracks (with
                                        # color features AND position
                                        # similarity)
                                        matches = reid_tracker.match_detections_to_tracks(
                                            np.array(valid_unmatched),
                                            all_existing_track_ids,
                                            # CRITICAL FIX: Use position
                                            # similarity
                                            position_similarity=position_similarity,
                                            new_color_features=valid_unmatched_color
                                        )

                                        # Apply matches (update track IDs)
                                        # matches maps from valid_unmatched index to track_id
                                        # We need to map back to original detection index
                                        # Safety check: ensure matches is a
                                        # dict (not None)
                                        if matches is None:
                                            matches = {}

                                        # Log Re-ID matching results for
                                        # diagnostics
                                        if len(matches) > 0 and frame_data.get(
                                                'frame_num', 0) % 100 == 0:  # Log every 100 frames
                                            print(
                                                f"ðŸ”— Re-ID matched {
                                                    len(matches)} lost tracks at frame {
                                                    frame_data.get(
                                                        'frame_num',
                                                        'unknown')}")

                                        for valid_idx, matched_track_id in matches.items():
                                            # CRITICAL FIX: Validate index before accessing to prevent IndexError
                                            # valid_idx is an index into
                                            # valid_unmatched_indices (which
                                            # contains original indices)
                                            if valid_idx < len(
                                                    valid_unmatched_indices):
                                                original_idx = valid_unmatched_indices[valid_idx]
                                                if original_idx < len(
                                                        detections.tracker_id):
                                                    detections.tracker_id[original_idx] = matched_track_id

                                                # Update Re-ID tracker with the
                                                # matched feature and color
                                                matched_color = None
                                                if reid_color_features is not None and original_idx < len(
                                                        reid_color_features.get('team_color', [])):
                                                    matched_color = {
                                                        'team_color': reid_color_features['team_color'][original_idx:original_idx + 1],
                                                        'ball_color': reid_color_features['ball_color'][original_idx:original_idx + 1]
                                                    }
                                                if original_idx < len(
                                                        reid_features):
                                                    reid_tracker.update_tracks(
                                                        [matched_track_id], [reid_features[original_idx]], matched_color)

                                        # Log when Re-ID has unmatched
                                        # detections but no matches found
                                        if len(valid_unmatched) > 0 and len(
                                                matches) == 0 and frame_data.get('frame_num', 0) % 200 == 0:
                                            print(
                                                f"âš  Re-ID: {len(valid_unmatched)} unmatched detections, but no matches found (similarity threshold may be too strict)")
                                            # Diagnostic: Show how many
                                            # existing tracks we're matching
                                            # against
                                            if hasattr(reid_tracker,
                                                       'track_features'):
                                                num_existing_tracks = len(
                                                    [
                                                        tid for tid in all_existing_track_ids if tid in reid_tracker.track_features and len(
                                                            reid_tracker.track_features[tid]) > 0])
                                                print(
                                                    f"   â†’ Matching against {num_existing_tracks} existing tracks with Re-ID features")
                            except Exception as e:
                                print(f"âš  Re-ID matching failed: {e}")
                                import traceback
                                traceback.print_exc()

                        # Note: Seed data (player mappings) is already used via rejected_ids filtering
                        # ByteTrack handles ID continuity automatically based on position matching
                        # Re-ID provides additional feature-based matching for better persistence
                        # The setup wizard's player mappings help during
                        # consolidation, not during tracking

                        # Apply track post-processing (interpolation, NMS,
                        # lifecycle management)
                        if post_processor is not None and detections is not None and len(
                                detections) > 0:
                            try:
                                # Track interpolation (fills gaps when
                                # detection temporarily lost)
                                detections = post_processor.interpolate_tracks(
                                    detections, frame_count)

                                # Safety check after post-processing
                                if detections is None:
                                    detections = sv.Detections.empty()

                                # Track NMS (removes duplicate tracks tracking
                                # same player)
                                if detections is not None and len(
                                        detections) > 0:
                                    detections = post_processor.apply_track_nms(
                                        detections, current_frame=frame_count)

                                    # Safety check after NMS
                                    if detections is None:
                                        detections = sv.Detections.empty()

                                    # Cleanup inactive tracks from history
                                    if detections is not None:
                                        active_track_ids = [
                                            tid for tid in detections.tracker_id if tid is not None]
                                        post_processor.cleanup_inactive_tracks(
                                            active_track_ids)
                            except Exception as e:
                                print(f"âš  Post-processing failed: {e}")
                                # Ensure detections is not None after error
                                if detections is None:
                                    detections = sv.Detections.empty()

                        # Apply enhanced temporal smoothing (Option A & B)
                        if temporal_smoothing and detections is not None and len(
                                detections) > 0:
                            smoothed_xyxy = detections.xyxy.copy()

                            # Handle case where tracker_id is None
                            tracker_ids = detections.tracker_id if detections.tracker_id is not None else [
                                None] * len(detections.xyxy)
                            for i, (track_id, conf) in enumerate(
                                    zip(tracker_ids, detections.confidence)):
                                if track_id is not None:
                                    track_id_int = int(track_id)
                                    
                                    # CRITICAL: Bypass temporal smoothing for tracks that drift repeatedly
                                    # These tracks should use detection positions directly (no smoothing)
                                    if track_id_int in track_drift_bypass and track_drift_bypass[track_id_int]:
                                        # Skip all temporal smoothing for this track - use detection position directly
                                        # The bbox will remain at the detection position (already in smoothed_xyxy)
                                        if current_frame_num % 100 == 0:
                                            print(f"  ðŸ”§ DRIFT BYPASS: Track #{track_id_int} skipping temporal smoothing (using detection position directly)")
                                        continue  # Skip to next detection
                                    
                                    # Get center of bounding box
                                    # CRITICAL: Use positions AFTER scaling (detections.xyxy is already scaled)
                                    x1, y1, x2, y2 = detections.xyxy[i]
                                    center = ((x1 + x2) / 2, (y1 + y2) / 2)
                                    
                                    # CRITICAL: Store position in history in correct coordinate system (after scaling)
                                    # This ensures temporal smoothing uses positions in the same coordinate system
                                    if track_id not in player_position_history:
                                        player_position_history[track_id] = deque(maxlen=smoothing_window)
                                    # Only store if we haven't already stored it (to avoid duplicates)
                                    if len(player_position_history[track_id]) == 0 or \
                                       player_position_history[track_id][-1] != center:
                                        player_position_history[track_id].append(center)

                                    # REAL-WORLD SPACE VALIDATION AND SMOOTHING (if homography available)
                                    # This helps reduce jitters and dropped IDs by:
                                    # 1. Filtering impossible movements (too fast in real-world space)
                                    # 2. Smoothing in physically accurate space
                                    # 3. Better position prediction
                                    if homography_matrix is not None:
                                        # Transform to real-world coordinates
                                        center_m = transform_point_to_field(
                                            center, homography_matrix)

                                    if center_m is not None:
                                        # Get player_name for this track_id (for analytics aggregation)
                                        player_name_smooth = player_names.get(str(track_id), f"Player_{track_id}")
                                        
                                        # Validate movement speed (reject if
                                        # player moved impossibly fast)
                                        # Use player_name for player_last_pos_m to preserve position across track_id changes
                                        if player_name_smooth in player_last_pos_m and player_last_pos_m[
                                                player_name_smooth] is not None:
                                            prev_pos_m = player_last_pos_m[player_name_smooth]
                                            current_frame = frame_data.get(
                                                'frame_num', frame_count)
                                            prev_frame = current_frame - 1  # Assume 1 frame difference

                                            # Calculate time difference
                                            time_diff = 1.0 / fps if fps > 0 else 1.0

                                            # Calculate speed in m/s
                                            dx = center_m[0] - prev_pos_m[0]
                                            dy = center_m[1] - prev_pos_m[1]
                                            distance_m = np.sqrt(
                                                dx * dx + dy * dy)
                                            speed_mps = distance_m / time_diff

                                            # If speed is unrealistic (likely
                                            # tracking error), use smoothed
                                            # position
                                            if speed_mps > max_player_speed_mps:
                                                # Movement too fast - likely tracking error or ID switch
                                                # Use smoothed position from
                                                # history instead (still use track_id for position history)
                                                if track_id in player_position_history_m and len(
                                                        player_position_history_m[track_id]) > 0:
                                                    # Average recent positions
                                                    # in real-world space
                                                    recent_positions = list(
                                                        player_position_history_m[track_id])
                                                    avg_x_m = sum(
                                                        p[0] for p in recent_positions) / len(recent_positions)
                                                    avg_y_m = sum(
                                                        p[1] for p in recent_positions) / len(recent_positions)
                                                    center_m = (
                                                        avg_x_m, avg_y_m)
                                                else:
                                                    # No history - use previous
                                                    # position (don't update)
                                                    center_m = prev_pos_m

                                        # Smooth in real-world space (more
                                        # physically accurate)
                                        # NOTE: player_position_history_m still keyed by track_id for per-track smoothing
                                        if track_id not in player_position_history_m:
                                            player_position_history_m[track_id] = deque(
                                                maxlen=smoothing_window)
                                        player_position_history_m[track_id].append(
                                            center_m)

                                        # If we have enough history, smooth the
                                        # position in real-world space
                                        if len(
                                                player_position_history_m[track_id]) >= min(
                                                3, smoothing_window):
                                            recent_positions = list(
                                                player_position_history_m[track_id])
                                            # Weighted average (more weight to
                                            # recent positions)
                                            weights = np.linspace(
                                                0.5, 1.0, len(recent_positions))
                                            weights = weights / weights.sum()
                                            avg_x_m = sum(
                                                p[0] * w for p,
                                                w in zip(
                                                    recent_positions,
                                                    weights))
                                            avg_y_m = sum(
                                                p[1] * w for p,
                                                w in zip(
                                                    recent_positions,
                                                    weights))
                                            center_m = (avg_x_m, avg_y_m)

                                        # Store real-world position for next frame (keyed by player_name for analytics)
                                        player_last_pos_m[player_name_smooth] = center_m

                                        # CRITICAL FIX: Transform smoothed real-world position back to pixels
                                        # This prevents coordinate system mismatch that causes drift
                                        # The previous code was using pixel positions from history which may be
                                        # in a different coordinate system (pre-scaling), causing drift
                                        if homography_inv is not None:
                                            # Transform smoothed real-world position back to pixel coordinates
                                            center_px = transform_field_to_point(center_m, homography_inv)
                                            if center_px is not None:
                                                center = center_px
                                                # CRITICAL: Store this position in history (in correct coordinate system)
                                                if track_id not in player_position_history:
                                                    player_position_history[track_id] = deque(maxlen=smoothing_window)
                                                player_position_history[track_id].append(center)
                                            else:
                                                # Transformation failed - fall back to pixel smoothing
                                                if track_id in player_position_history:
                                                    pixel_positions = list(player_position_history[track_id])
                                                    if len(pixel_positions) >= 2:
                                                        weights = np.linspace(0.5, 1.0, len(pixel_positions))
                                                        weights = weights / weights.sum()
                                                        avg_x = sum(p[0] * w for p, w in zip(pixel_positions, weights))
                                                        avg_y = sum(p[1] * w for p, w in zip(pixel_positions, weights))
                                                        center = (avg_x, avg_y)
                                        else:
                                            # No homography - use pixel smoothing directly
                                            # CRITICAL: Ensure positions are in correct coordinate system (after scaling)
                                            if track_id in player_position_history:
                                                pixel_positions = list(player_position_history[track_id])
                                                if len(pixel_positions) >= 2:
                                                    weights = np.linspace(0.5, 1.0, len(pixel_positions))
                                                    weights = weights / weights.sum()
                                                    avg_x = sum(p[0] * w for p, w in zip(pixel_positions, weights))
                                                    avg_y = sum(p[1] * w for p, w in zip(pixel_positions, weights))
                                                    center = (avg_x, avg_y)
                                            # Store current position in history (in correct coordinate system - after scaling)
                                            if track_id not in player_position_history:
                                                player_position_history[track_id] = deque(maxlen=smoothing_window)
                                            player_position_history[track_id].append(center)

                                    # CRITICAL FIX: Temporal smoothing and EMA should be ALTERNATIVES, not both applied
                                    # Using both causes double-smoothing which leads to lag and drift
                                    # Priority: Kalman > EMA > Weighted Average (temporal smoothing)

                                    # Initialize enhanced tracking if enabled
                                    # IMPROVED: More aggressive smoothing for
                                    # stable, locked tracking
                                    if use_enhanced_kalman and ENHANCED_TRACKING_AVAILABLE:
                                        if track_id not in enhanced_kalman_filters:
                                            # Lower process_noise = more trust in predictions (smoother)
                                            # Lower measurement_noise = more
                                            # trust in measurements (less
                                            # jitter)
                                            enhanced_kalman_filters[track_id] = EnhancedKalmanFilter(
                                                process_noise=0.005,
                                                # IMPROVED: Even lower (was 0.01) for smoother tracking
                                                # IMPROVED: Even lower (was
                                                # 0.1) for less jitter/blinking
                                                measurement_noise=0.05 *
                                                (2.0 - conf)
                                            )

                                    if use_ema_smoothing and ENHANCED_TRACKING_AVAILABLE:
                                        if track_id not in ema_smoothers:
                                            # Lower alpha = more smoothing (was
                                            # 0.2, now 0.15 for even smoother
                                            # tracking)
                                            ema_smoothers[track_id] = EMASmoother(
                                                alpha=0.15, min_history=5)  # IMPROVED: More smoothing, less blinking

                                    # Store confidence history
                                    if track_id not in confidence_history:
                                        confidence_history[track_id] = deque(
                                            maxlen=10)
                                    confidence_history[track_id].append(conf)

                                    # Apply enhanced smoothing (Priority: Kalman > EMA > Weighted Average)
                                    # Only ONE smoothing method should be applied to avoid double-smoothing
                                    smoothing_applied = False
                                    
                                    # Option 1: Kalman filter (highest priority - most sophisticated)
                                    if use_enhanced_kalman and ENHANCED_TRACKING_AVAILABLE and track_id in enhanced_kalman_filters:
                                        try:
                                            # Predict next position
                                            kf = enhanced_kalman_filters[track_id]
                                            predicted_pos = kf.predict(dt=1.0)

                                            # Update with measurement
                                            kf.update(center, confidence=conf)

                                            # Get smoothed position from Kalman
                                            # filter
                                            smoothed_center = kf.get_position()

                                            # Get velocity for better box
                                            # positioning
                                            vx, vy = kf.get_velocity()

                                            # Use smoothed center
                                            center = smoothed_center
                                            smoothing_applied = True
                                        except Exception as e:
                                            # Fallback to simple smoothing if
                                            # Kalman fails
                                            pass

                                    # Option 2: EMA smoothing (if Kalman not used)
                                    if not smoothing_applied and use_ema_smoothing and ENHANCED_TRACKING_AVAILABLE and track_id in ema_smoothers:
                                        try:
                                            smoothed_center = ema_smoothers[track_id].update(
                                                center, confidence=conf)
                                            center = smoothed_center
                                            smoothing_applied = True
                                        except Exception as e:
                                            # Fallback to simple smoothing if
                                            # EMA fails
                                            pass

                                    # Option 3: Weighted average temporal smoothing (fallback if neither Kalman nor EMA used)
                                    # This is the "temporal_smoothing" weighted average method
                                    if not smoothing_applied:
                                        # Add to history
                                        if track_id not in player_position_history:
                                            player_position_history[track_id] = deque(
                                                maxlen=smoothing_window)
                                        player_position_history[track_id].append(
                                            center)

                                        # IMPROVED: Use weighted average (more
                                        # weight to recent positions) for
                                        # smoother tracking
                                        if len(
                                                player_position_history[track_id]) >= min(
                                                3, smoothing_window):
                                            # Weighted average: recent
                                            # positions have more weight
                                            positions = list(
                                                player_position_history[track_id])
                                            # IMPROVED: More aggressive
                                            # weighting (was 0.3)
                                            weights = np.linspace(
                                                0.2, 1.0, len(positions))
                                            weights = weights / weights.sum()
                                            avg_x = sum(
                                                p[0] * w for p,
                                                w in zip(
                                                    positions,
                                                    weights))
                                            avg_y = sum(
                                                p[1] * w for p,
                                                w in zip(
                                                    positions,
                                                    weights))
                                            center = (avg_x, avg_y)

                                    # Get current box size
                                    box_width = x2 - x1
                                    box_height = y2 - y1

                                    # Update bounding box to smoothed center
                                    smoothed_xyxy[i] = np.array([
                                        center[0] - box_width / 2,
                                        center[1] - box_height / 2,
                                        center[0] + box_width / 2,
                                        center[1] + box_height / 2
                                    ], dtype=np.float32)

                            # Update detections with smoothed positions
                            detections.xyxy = smoothed_xyxy

                            # CRITICAL FIX: Keep smoothing filters alive longer to prevent blinking
                            # Don't delete filters immediately - keep them alive for track_buffer_scaled frames
                            # This prevents blinking when tracks are
                            # temporarily lost for 1-2 frames
                            current_frame = frame_data.get(
                                'frame_num', frame_count)

                            # Update last seen frame for active tracks
                            active_track_ids = set(
                                tid for tid in detections.tracker_id if tid is not None)
                            for tid in active_track_ids:
                                track_last_seen[tid] = current_frame
                            
                            # ðŸ” OCCLUSION RECOVERY: Track tagged/anchor-protected players that disappeared
                            # When they reappear nearby, recover their identity
                            
                            # Step 1: Update disappeared_tagged_players - track who disappeared
                            for tid, last_frame in list(track_last_seen.items()):
                                frames_since_seen = current_frame - last_frame
                                
                                # Check if this track is tagged/anchor-protected and just disappeared
                                if frames_since_seen > 0 and frames_since_seen <= OCCLUSION_RECOVERY_FRAMES:
                                    pid_str = str(int(tid))
                                    player_name_raw = player_names.get(pid_str, "")
                                    
                                    # CRITICAL FIX: Handle case where player_name might be a list
                                    if isinstance(player_name_raw, list) and len(player_name_raw) > 0:
                                        player_name = str(player_name_raw[0])
                                    elif player_name_raw:
                                        player_name = str(player_name_raw)
                                    else:
                                        player_name = ""
                                    
                                    # Only track real players (not "Player X") with anchor protection or real names
                                    has_anchor = tid in track_anchor_protection
                                    has_real_name = player_name and isinstance(player_name, str) and not player_name.strip().startswith("Player ")
                                    
                                    if (has_anchor or has_real_name) and player_name:
                                        # Get last known position from track_state
                                        if tid in track_state and 'xyxy' in track_state[tid]:
                                            last_bbox = track_state[tid]['xyxy']
                                            center_x = (last_bbox[0] + last_bbox[2]) / 2
                                            center_y = (last_bbox[1] + last_bbox[3]) / 2
                                            
                                            # ENHANCED: Use Kalman filter prediction if available
                                            predicted_pos = None
                                            predicted_velocity = None
                                            if use_enhanced_kalman and ENHANCED_TRACKING_AVAILABLE and tid in enhanced_kalman_filters:
                                                try:
                                                    kf = enhanced_kalman_filters[tid]
                                                    # Predict position after frames_since_seen frames
                                                    predicted_pos = kf.predict(dt=float(frames_since_seen))
                                                    predicted_velocity = kf.get_velocity()
                                                    # Use predicted position for better recovery
                                                    center_x, center_y = predicted_pos[0], predicted_pos[1]
                                                    if current_frame % 100 == 0:
                                                        print(f"  ðŸŽ¯ KALMAN PREDICTION: {player_name} (Track #{tid}) predicted at ({center_x:.1f}, {center_y:.1f}) after {frames_since_seen} frames")
                                                except Exception as e:
                                                    # Fallback to last known position if Kalman fails
                                                    if current_frame % 200 == 0:
                                                        print(f"  âš  Kalman prediction failed for {player_name}: {e}")
                                            
                                            team = track_to_team_global.get(tid, "")
                                            
                                            # Add to disappeared tracking (update if already exists)
                                            disappeared_tagged_players[player_name] = {
                                                'track_id': tid,
                                                'last_pos': (center_x, center_y),
                                                'predicted_pos': predicted_pos,  # Store predicted position
                                                'predicted_velocity': predicted_velocity,  # Store predicted velocity
                                                'last_frame': last_frame,
                                                'team': team,
                                                'had_anchor': has_anchor,
                                                'frames_since_seen': frames_since_seen  # Track how long missing
                                            }
                            
                            # Step 2: Clean up old disappeared players (beyond recovery window)
                            players_to_remove = []
                            for player_name, info in disappeared_tagged_players.items():
                                frames_missing = current_frame - info['last_frame']
                                if frames_missing > OCCLUSION_RECOVERY_FRAMES:
                                    players_to_remove.append(player_name)
                            for player_name in players_to_remove:
                                del disappeared_tagged_players[player_name]
                            
                            # Step 3: Check if new tracks appeared near disappeared players
                            for tid in active_track_ids:
                                # Only check newly appeared tracks
                                if tid not in track_first_seen or (current_frame - track_first_seen[tid]) <= 5:
                                    # This is a new or very recent track
                                    pid_str = str(int(tid))
                                    current_name_raw = player_names.get(pid_str, "")
                                    
                                    # CRITICAL FIX: Handle case where current_name might be a list
                                    if isinstance(current_name_raw, list) and len(current_name_raw) > 0:
                                        current_name = str(current_name_raw[0])
                                    elif current_name_raw:
                                        current_name = str(current_name_raw)
                                    else:
                                        current_name = ""
                                    
                                    # Only recover if track is currently unnamed or has generic name
                                    if not current_name or (isinstance(current_name, str) and current_name.strip().startswith("Player ")):
                                        # Get this track's current position
                                        try:
                                            idx = list(detections.tracker_id).index(tid)
                                            bbox = detections.xyxy[idx]
                                            center_x = (bbox[0] + bbox[2]) / 2
                                            center_y = (bbox[1] + bbox[3]) / 2
                                            
                                            # Check distance to all disappeared players
                                            closest_player = None
                                            closest_distance = float('inf')
                                            
                                            for player_name, info in disappeared_tagged_players.items():
                                                # ENHANCED: Use predicted position if available, otherwise use last known position
                                                search_pos = info.get('predicted_pos')
                                                if search_pos is None:
                                                    search_pos = info['last_pos']
                                                else:
                                                    # Predicted position is a tuple (x, y)
                                                    search_pos = (search_pos[0], search_pos[1])
                                                
                                                dx = center_x - search_pos[0]
                                                dy = center_y - search_pos[1]
                                                distance = (dx**2 + dy**2) ** 0.5
                                                
                                                # ENHANCED: Adjust recovery distance based on how long player has been missing
                                                # Longer missing = allow larger search radius (they may have moved further)
                                                frames_missing = info.get('frames_since_seen', 0)
                                                adaptive_recovery_distance = OCCLUSION_RECOVERY_DISTANCE
                                                if frames_missing > 0:
                                                    # Allow 1.5x distance for every 30 frames missing (up to 2x max)
                                                    distance_multiplier = min(2.0, 1.0 + (frames_missing / 30.0) * 0.5)
                                                    adaptive_recovery_distance = OCCLUSION_RECOVERY_DISTANCE * distance_multiplier
                                                
                                                if distance < closest_distance and distance <= adaptive_recovery_distance:
                                                    closest_player = player_name
                                                    closest_distance = distance
                                                    closest_info = info
                                            
                                            # Recover identity if found close match
                                            if closest_player:
                                                player_names[pid_str] = closest_player
                                                
                                                # Restore team
                                                if closest_info['team']:
                                                    track_to_team_global[tid] = closest_info['team']
                                                
                                                # Restore anchor protection if they had it
                                                if closest_info['had_anchor']:
                                                    old_track_id = closest_info['track_id']
                                                    if old_track_id in track_anchor_protection:
                                                        # Transfer anchor protection to new track
                                                        track_anchor_protection[tid] = track_anchor_protection[old_track_id]
                                                
                                                # Remove from disappeared (they've been recovered)
                                                del disappeared_tagged_players[closest_player]
                                                
                                                if current_frame % 30 == 0:  # Occasional logging
                                                    print(f"  ðŸ” OCCLUSION RECOVERY: {closest_player} recovered! Track #{closest_info['track_id']} â†’ Track #{tid} ({closest_distance:.1f}px, {current_frame - closest_info['last_frame']}f gap)")
                                        except (ValueError, IndexError, KeyError):
                                            pass

                            # ENHANCED: Use Kalman predictions to maintain track_state during occlusions
                            # This allows tracks to be visible even when temporarily occluded
                            if use_enhanced_kalman and ENHANCED_TRACKING_AVAILABLE:
                                # Update track_state with Kalman predictions for missing tracks
                                for tid in list(enhanced_kalman_filters.keys()):
                                    if tid not in active_track_ids:
                                        # Track is missing - use Kalman prediction
                                        last_seen = track_last_seen.get(tid, current_frame)
                                        frames_since_seen = current_frame - last_seen
                                        
                                        # Only predict if track is within buffer (not permanently lost)
                                        if 0 < frames_since_seen <= track_buffer_scaled:
                                            try:
                                                kf = enhanced_kalman_filters[tid]
                                                # Predict position after frames_since_seen frames
                                                predicted_pos = kf.predict(dt=float(frames_since_seen))
                                                predicted_velocity = kf.get_velocity()
                                                
                                                # Update track_state with predicted position
                                                if tid in track_state:
                                                    last_bbox = track_state[tid].get('xyxy', [0, 0, 50, 50])
                                                    # Calculate predicted bbox from predicted center
                                                    box_width = last_bbox[2] - last_bbox[0]
                                                    box_height = last_bbox[3] - last_bbox[1]
                                                    
                                                    predicted_x = predicted_pos[0]
                                                    predicted_y = predicted_pos[1]
                                                    
                                                    # Update track_state with predicted bbox
                                                    track_state[tid] = {
                                                        'xyxy': [
                                                            predicted_x - box_width / 2,
                                                            predicted_y - box_height / 2,
                                                            predicted_x + box_width / 2,
                                                            predicted_y + box_height / 2
                                                        ],
                                                        'frame': current_frame,
                                                        'velocity': [predicted_velocity[0], predicted_velocity[1]],
                                                        'is_predicted': True  # Mark as predicted
                                                    }
                                            except Exception as e:
                                                # If prediction fails, keep existing track_state
                                                if current_frame % 200 == 0:
                                                    print(f"  âš  Kalman prediction for missing track {tid} failed: {e}")
                                
                                # Remove filters for tracks that have been inactive for longer than buffer
                                tracks_to_remove = []
                                for tid in enhanced_kalman_filters.keys():
                                    last_seen = track_last_seen.get(tid, current_frame)
                                    frames_since_seen = current_frame - last_seen
                                    # Only remove if track has been inactive for longer than buffer
                                    if frames_since_seen > track_buffer_scaled:
                                        tracks_to_remove.append(tid)
                                for tid in tracks_to_remove:
                                    del enhanced_kalman_filters[tid]
                                    if tid in track_last_seen:
                                        del track_last_seen[tid]
                                    # CRITICAL: Also remove from track_state to prevent stale predictions
                                    if tid in track_state:
                                        del track_state[tid]

                            if use_ema_smoothing and ENHANCED_TRACKING_AVAILABLE:
                                tracks_to_remove = []
                                for tid in ema_smoothers.keys():
                                    last_seen = track_last_seen.get(
                                        tid, current_frame)
                                    frames_since_seen = current_frame - last_seen
                                    # Only remove if track has been inactive
                                    # for longer than buffer
                                    if frames_since_seen > track_buffer_scaled:
                                        tracks_to_remove.append(tid)
                                for tid in tracks_to_remove:
                                    del ema_smoothers[tid]
                                    # CRITICAL: Also remove from track_state to
                                    # prevent stale predictions
                                    if tid in track_state:
                                        del track_state[tid]

                            # Clean up confidence history for inactive tracks
                            # (keep longer too)
                            tracks_to_remove = []
                            for tid in confidence_history.keys():
                                last_seen = track_last_seen.get(
                                    tid, current_frame)
                                frames_since_seen = current_frame - last_seen
                                # Only remove if track has been inactive for
                                # longer than buffer
                                if frames_since_seen > track_buffer_scaled:
                                    tracks_to_remove.append(tid)
                            for tid in tracks_to_remove:
                                if tid in confidence_history:
                                    del confidence_history[tid]

                            # === OPTIMIZED Re-ID MEMORY CLEANUP ===
                            # Clean up frequently but not every frame to reduce overhead
                            # Re-ID tracker accumulates tracks without cleanup - this is a major memory leak!
                            # torchreid does not auto-delete embeddings,
                            # causing GPU memory bloat and slower matching
                            current_frame = frame_data.get(
                                'frame_num', frame_count)
                            # ENHANCED: More aggressive cleanup to prevent memory bloat
                            # Clean up every 3 frames (was 5-10) to keep memory usage low
                            cleanup_frequency = 3
                            if reid_tracker is not None and current_frame % cleanup_frequency == 0:
                                try:
                                    # Get all active track IDs from current
                                    # detections
                                    active_track_ids_set = set(
                                        tid for tid in detections.tracker_id if tid is not None)

                                    # Clean up inactive tracks (but keep
                                    # recently lost tracks for reconnection)
                                    if hasattr(
                                            reid_tracker, 'track_features') and reid_tracker.track_features:
                                        reid_track_ids = set(
                                            reid_tracker.track_features.keys())
                                        inactive_reid_tracks = reid_track_ids - active_track_ids_set
                                        
                                        # ENHANCED: Add maximum track limit to prevent unbounded growth
                                        # If we have too many tracks total, be more aggressive
                                        total_tracks = len(reid_track_ids)
                                        MAX_TRACKS = 500  # Maximum tracks to keep in memory
                                        
                                        # If we exceed max tracks, remove oldest inactive tracks first
                                        if total_tracks > MAX_TRACKS:
                                            # Sort inactive tracks by last seen frame (oldest first)
                                            inactive_with_frames = []
                                            for tid in inactive_reid_tracks:
                                                if tid in track_state:
                                                    frames_since_seen = current_frame - track_state[tid]['frame']
                                                    inactive_with_frames.append((tid, frames_since_seen))
                                                else:
                                                    # No track_state info - assume very old
                                                    inactive_with_frames.append((tid, 999999))
                                            
                                            # Sort by frames since seen (oldest first)
                                            inactive_with_frames.sort(key=lambda x: x[1], reverse=True)
                                            
                                            # Remove enough tracks to get back under limit
                                            tracks_to_remove = []
                                            tracks_to_remove_count = total_tracks - MAX_TRACKS
                                            for tid, _ in inactive_with_frames[:tracks_to_remove_count]:
                                                tracks_to_remove.append(tid)
                                            
                                            # Also remove tracks that have been inactive for > 1.5x buffer
                                            # (more aggressive than before - was 2x)
                                            for tid in inactive_reid_tracks:
                                                if tid not in tracks_to_remove:  # Don't double-add
                                                    if tid in track_state:
                                                        frames_since_seen = current_frame - track_state[tid]['frame']
                                                        # ENHANCED: More aggressive - remove after 1.5x buffer (was 2x)
                                                        if frames_since_seen > track_buffer_scaled * 1.5:
                                                            tracks_to_remove.append(tid)
                                                    else:
                                                        # Track not in track_state - remove if we have many inactive tracks
                                                        if len(inactive_reid_tracks) > 30:  # Lowered from 50
                                                            tracks_to_remove.append(tid)
                                        else:
                                            # Normal cleanup: Remove tracks inactive for > 1.5x buffer
                                            tracks_to_remove = []
                                            for tid in inactive_reid_tracks:
                                                # Check how long this track has
                                                # been inactive
                                                if tid in track_state:
                                                    frames_since_seen = current_frame - \
                                                        track_state[tid]['frame']
                                                    # ENHANCED: More aggressive - remove after 1.5x buffer (was 2x)
                                                    if frames_since_seen > track_buffer_scaled * 1.5:
                                                        tracks_to_remove.append(
                                                            tid)
                                                else:
                                                    # Track not in track_state - might be very old, remove it
                                                    # ENHANCED: Lowered threshold from 50 to 30 for more aggressive cleanup
                                                    if len(
                                                            inactive_reid_tracks) > 30:  # Only if we have many inactive tracks
                                                        tracks_to_remove.append(
                                                            tid)

                                        # Remove only old inactive tracks
                                        if len(tracks_to_remove) > 0:
                                            for tid in tracks_to_remove:
                                                if hasattr(
                                                        reid_tracker, 'clear_track'):
                                                    reid_tracker.clear_track(
                                                        tid)

                                            # Log cleanup more frequently to show progress
                                            # (every 100 frames instead of 200)
                                            if len(
                                                    tracks_to_remove) > 3 and current_frame % 100 == 0:
                                                remaining_tracks = len(reid_track_ids) - len(tracks_to_remove)
                                                print(
                                                    f"ðŸ§¹ Re-ID cleanup: Removed {
                                                        len(tracks_to_remove)} old inactive tracks (kept {
                                                        len(active_track_ids_set)} active, {
                                                        remaining_tracks} total tracks in memory)")

                                    # Optional: Limit gallery size to prevent
                                    # unbounded growth
                                    # Reduced from 500 to 300 to improve performance
                                    # (prevents speed degradation from 6.1 â†’ 2.4 fps)
                                    MAX_GALLERY_SIZE = 300
                                    if hasattr(
                                            reid_tracker, 'gallery') and len(
                                            reid_tracker.gallery) > MAX_GALLERY_SIZE:
                                        # Keep only most recent N features
                                        reid_tracker.gallery = reid_tracker.gallery[-MAX_GALLERY_SIZE:]
                                        if current_frame % 500 == 0:
                                            print(
                                                f"ðŸ§¹ Re-ID gallery trimmed to {MAX_GALLERY_SIZE} entries")
                                except Exception as e:
                                    # Don't fail if cleanup fails, but log it
                                    if current_frame % 1000 == 0:  # Log errors less frequently
                                        print(f"âš  Re-ID cleanup failed: {e}")

                            # Clean up real-world position history for inactive
                            # tracks (keep longer too)
                            if homography_matrix is not None:
                                tracks_to_remove = []
                                # Only clean up position history (still keyed by track_id)
                                # Analytics are now keyed by player_name, so we don't delete them when track_id is removed
                                for tid in list(player_position_history_m.keys()):
                                    last_seen = track_last_seen.get(
                                        tid, current_frame)
                                    frames_since_seen = current_frame - last_seen
                                    # Only remove if track has been inactive
                                    # for longer than buffer
                                    if frames_since_seen > track_buffer_scaled:
                                        tracks_to_remove.append(tid)
                                for tid in tracks_to_remove:
                                    # Clean up position history (keyed by track_id)
                                    if tid in player_position_history_m:
                                        del player_position_history_m[tid]
                                    # NOTE: player_last_pos_m is now keyed by player_name, not track_id
                                    # We don't delete it here because the player might get a new track_id
                                    # Analytics are also keyed by player_name, so we don't delete them here
                                    # They will persist across track_id changes, which is what we want
                                    # (All analytics dictionaries are now keyed by player_name, not track_id)

                        # Filter out rejected IDs (coach, false positives from setup wizard)
                        # Unless track_referees is enabled, in which case we keep all detections
                        # CRITICAL: Check actual xyxy array size, not just
                        # len(detections)
                        xyxy_actual_len = 0
                        if hasattr(detections,
                                   'xyxy') and detections.xyxy is not None:
                            try:
                                xyxy_actual_len = detections.xyxy.shape[0] if len(
                                    detections.xyxy.shape) > 0 else 0
                            except (AttributeError, IndexError):
                                xyxy_actual_len = 0

                        if rejected_ids and len(detections) > 0 and xyxy_actual_len > 0 and xyxy_actual_len == len(
                                detections) and not track_referees:
                            # Store detections length when creating mask
                            detections_len_when_mask_created = len(detections)
                            # Create mask to filter out rejected track IDs
                            keep_mask = np.ones(len(detections), dtype=bool)
                            for i, track_id in enumerate(
                                    detections.tracker_id):
                                if track_id is not None and track_id in rejected_ids:
                                    keep_mask[i] = False

                            # Apply mask using safe helper function
                            if not np.all(keep_mask):
                                success, detections = safe_apply_detection_mask(
                                    detections, keep_mask,
                                    frame_num=frame_data.get(
                                        'frame_num', 'unknown')
                                )
                                if not success:
                                    # Mask application failed - detections
                                    # unchanged, continue processing
                                    pass

                        # Track active player count and handle substitutions
                        if enable_substitutions and len(detections) > 0:
                            active_track_ids = set(
                                tid for tid in detections.tracker_id if tid is not None)
                            active_count = len(active_track_ids)

                            # If we exceed max_players, prioritize field
                            # players over referees/bench
                            if active_count > max_players and not track_referees:
                                # Sort by confidence and keep top N
                                if len(detections) > max_players:
                                    # Get confidence scores
                                    confidences = detections.confidence
                                    # Sort by confidence (descending)
                                    sorted_indices = np.argsort(
                                        confidences)[::-1]
                                    # Keep top max_players
                                    keep_indices = sorted_indices[:max_players]
                                    detections = detections[keep_indices]

                        # Match players from previous analysis by frame number
                        # and position
                        if previous_frame_data and len(detections) > 0:
                            # Use the actual frame number from frame_data
                            actual_frame_num = frame_data.get(
                                'frame_num', frame_count)
                            matches = match_players_by_frame(
                                previous_frame_data,
                                actual_frame_num,
                                detections,
                                player_names,
                                max_distance_pixels=50
                            )
                            if matches > 0 and actual_frame_num % 100 == 0:  # Log every 100 frames
                                print(
                                    f"ðŸ“Š Frame {actual_frame_num}: Matched {matches} player(s) from previous analysis")

                        # Classify teams and get colors
                        # CRITICAL: Use persistent team assignments from track_to_team_global as base
                        # This prevents boxes from blinking gray when team classification temporarily fails
                        player_teams = dict(track_to_team_global)  # Start with persistent assignments
                        colors = []
                        # Handle case where tracker_id is None (before tracking
                        # is applied)
                        tracker_ids = detections.tracker_id
                        if tracker_ids is None:
                            # Create a list of None values with same length as
                            # xyxy
                            tracker_ids = [
                                None] * len(detections.xyxy) if len(detections.xyxy) > 0 else []

                        # CRITICAL FIX: Use full frame for team classification, not cropped batch_frame
                        # The detections.xyxy are already in full frame coordinates (translated from ROI)
                        # So we need to use the original full-resolution frame for team classification
                        # Get full frame from frame_data (stored when frame was queued)
                        frame_for_team_classification = batch_frame
                        # Get original dimensions for coordinate validation (xyxy is in original resolution)
                        original_w = frame_data.get('original_width', width)
                        original_h = frame_data.get('original_height', height)
                        if roi_bounds is not None and viz_color_mode == "team" and team_colors:
                            # Use stored full-resolution frame from frame_data
                            full_frame = frame_data.get('full_frame', None)
                            
                            if full_frame is not None:
                                # Verify frame dimensions match expected original resolution
                                full_h, full_w = full_frame.shape[:2]
                                if full_w == original_w and full_h == original_h:
                                    frame_for_team_classification = full_frame
                                else:
                                    # Frame dimensions don't match - might be wrong frame
                                    if frame_data.get('frame_num', 0) % 100 == 0:
                                        print(f"   âš  Team classification: Full frame dimensions mismatch (expected {original_w}x{original_h}, got {full_w}x{full_h})")
                                    frame_for_team_classification = batch_frame
                            else:
                                # Fallback: if full frame not stored, use batch_frame
                                # This shouldn't happen, but handle gracefully
                                if frame_data.get('frame_num', 0) % 100 == 0:
                                    print(f"   âš  Team classification: Full frame not stored in frame_data (using batch_frame as fallback)")
                                frame_for_team_classification = batch_frame

                        for i, (xyxy, track_id) in enumerate(
                                zip(detections.xyxy, tracker_ids)):
                            if track_id is not None:
                                # VALIDATION: Check if bbox coordinates are valid before processing
                                # CRITICAL: xyxy coordinates are in original (full) resolution after scaling
                                # Use original_w/original_h for validation, not frame_for_team_classification dimensions
                                # (which might be YOLO resolution if full_frame wasn't stored)
                                frame_h, frame_w = frame_for_team_classification.shape[:2]
                                # xyxy is in original resolution, so use original dimensions for validation
                                x1, y1, x2, y2 = xyxy
                                
                                # Filter out clearly invalid bboxes (way outside frame bounds)
                                # This catches coordinate transformation errors or tracker bugs
                                # Use original dimensions since xyxy is scaled to original resolution
                                max_valid_coord = max(original_w, original_h) * 2  # Allow some margin for edge cases
                                if (x1 < -max_valid_coord or x1 > max_valid_coord or
                                    y1 < -max_valid_coord or y1 > max_valid_coord or
                                    x2 < -max_valid_coord or x2 > max_valid_coord or
                                    y2 < -max_valid_coord or y2 > max_valid_coord):
                                    # Bbox is way outside valid range - skip this detection
                                    current_frame = frame_data.get('frame_num', 0)
                                    if current_frame % 100 == 0:
                                        print(f"   âš  Skipping invalid bbox for Detection #{i} (track_id={track_id}): coordinates out of range (bbox: {xyxy.tolist()}, original frame: {original_w}x{original_h}, team_classification frame: {frame_w}x{frame_h})")
                                    # Use default color for invalid detections
                                    colors.append((0, 165, 255))  # Orange for untracked/invalid
                                    continue
                                
                                # Classify team if using team color mode
                                team = None
                                if viz_color_mode == "team" and team_colors:
                                    # PRIORITY 1: Check player roster/gallery for team assignment
                                    track_id_str = str(int(track_id))
                                    player_name = player_names.get(track_id_str, None)
                                    if player_name and player_gallery:
                                        try:
                                            player_profile = player_gallery.get_player(player_name)
                                            if player_profile and player_profile.team:
                                                team = player_profile.team
                                                player_teams[track_id] = team
                                                track_to_team_global[track_id] = team
                                                # Skip color-based classification if roster has team
                                                if frame_data.get('frame_num', 0) < 2:
                                                    print(f"   âœ“ Team from roster: {player_name} = {team}")
                                        except:
                                            pass  # Silently fail if player not in gallery
                                    
                                    # PRIORITY 2: Use persistent team assignment if available
                                    if not team and track_id in track_to_team_global:
                                        team = track_to_team_global[track_id]
                                        player_teams[track_id] = team
                                    
                                    # PRIORITY 3: Color-based classification (fallback if no roster assignment)
                                    if not team:
                                        # CRITICAL FIX: Use full frame for team classification
                                        # Clamp bbox to frame bounds instead of skipping (handles edge cases and small players)
                                        
                                        # Clamp bbox coordinates to frame bounds (more lenient for edge cases)
                                        x1_clamped = max(0, min(x1, frame_w - 1))
                                        y1_clamped = max(0, min(y1, frame_h - 1))
                                        x2_clamped = max(x1_clamped + 1, min(x2, frame_w))  # Ensure width > 0
                                        y2_clamped = max(y1_clamped + 1, min(y2, frame_h))  # Ensure height > 0
                                        
                                        # Only skip if bbox is completely invalid (too small or completely outside)
                                        bbox_width = x2_clamped - x1_clamped
                                        bbox_height = y2_clamped - y1_clamped
                                        min_bbox_size = 10  # Minimum bbox size for classification (allows small players)
                                        
                                        if bbox_width >= min_bbox_size and bbox_height >= min_bbox_size:
                                            # Use clamped bbox for classification
                                            xyxy_clamped = [x1_clamped, y1_clamped, x2_clamped, y2_clamped]
                                            
                                            # Disable verbose debug - it slows processing significantly
                                            # Only enable for first frame if needed for diagnostics
                                            current_frame = frame_data.get('frame_num', 0)
                                            debug_classification = False  # (current_frame < 1) to enable for frame 0 only
                                            team = classify_player_team(
                                                frame_for_team_classification, xyxy_clamped, team_colors, debug=debug_classification)
                                            if team:
                                                player_teams[track_id] = team
                                                # CRITICAL: Also store in persistent global mapping
                                                track_to_team_global[track_id] = team
                                                if current_frame < 2:  # Log first 2 frames only
                                                    print(f"   âœ“ Team classification succeeded: Detection #{i} = {team}")
                                            else:
                                                # Team classification failed - use persistent assignment if available
                                                if track_id in track_to_team_global:
                                                    team = track_to_team_global[track_id]
                                                    player_teams[track_id] = team  # Use persistent team
                                            if current_frame % 500 == 0:  # Log less frequently (every 500 frames instead of 100)
                                                # Only log if bbox is valid but classification failed
                                                print(f"   âš  Team classification failed for Detection #{i} (bbox: {xyxy.tolist()}, frame: {frame_w}x{frame_h})")
                                                # Check if HSV ranges are configured and show team names
                                                team1_has_ranges = False
                                                team2_has_ranges = False
                                                if team_colors and 'team_colors' in team_colors:
                                                    team1_data = team_colors['team_colors'].get('team1', {})
                                                    team2_data = team_colors['team_colors'].get('team2', {})
                                                    team1_has_ranges = bool(team1_data.get('hsv_ranges'))
                                                    team2_has_ranges = bool(team2_data.get('hsv_ranges'))
                                                    team1_name = team1_data.get('name', 'team1')
                                                    team2_name = team2_data.get('name', 'team2')
                                                    print(f"      â†’ HSV ranges configured: {team1_name}={team1_has_ranges}, {team2_name}={team2_has_ranges}")
                                                    # Show actual HSV ranges if configured
                                                    if team1_has_ranges:
                                                        hsv1 = team1_data.get('hsv_ranges', {})
                                                        if 'lower' in hsv1 and 'upper' in hsv1:
                                                            print(f"      â†’ {team1_name} HSV: {hsv1['lower']} to {hsv1['upper']}")
                                                    if team2_has_ranges:
                                                        hsv2 = team2_data.get('hsv_ranges', {})
                                                        if 'lower' in hsv2 and 'upper' in hsv2:
                                                            print(f"      â†’ {team2_name} HSV: {hsv2['lower']} to {hsv2['upper']}")
                                                if team1_has_ranges or team2_has_ranges:
                                                    print(f"      â†’ Tip: Use 'ðŸŽ¨ Team Colors' button in Setup Wizard to adjust HSV ranges")
                                                    print(f"      â†’ Or check team_color_config.json and widen HSV ranges if needed")
                                        else:
                                            # Bbox too small or completely invalid - skip classification
                                            if frame_data.get('frame_num', 0) % 100 == 0:
                                                print(f"   âš  Team classification skipped for Detection #{i}: bbox too small or invalid (bbox: {xyxy.tolist()}, clamped: [{x1_clamped:.1f}, {y1_clamped:.1f}, {x2_clamped:.1f}, {y2_clamped:.1f}], size: {bbox_width:.1f}x{bbox_height:.1f}, min: {min_bbox_size}px, frame: {frame_w}x{frame_h})")

                                # Get color for this player
                                color = get_player_color(
                                    track_id, team, viz_color_mode, team_colors)
                                colors.append(color)
                            else:
                                # Orange for untracked (easy to debug)
                                colors.append((0, 165, 255))

                        # Create labels using get_label_text to respect label_type setting
                        labels = []
                        # Get current label settings (dynamic if available)
                        current_label_type = get_setting('label_type', label_type)
                        current_label_custom_text = get_setting('label_custom_text', label_custom_text)
                        # Use the same tracker_ids we prepared above (handles
                        # None case)
                        for i, (track_id, conf) in enumerate(
                                zip(tracker_ids, detections.confidence)):
                            if track_id is not None:
                                # Use player name if available, otherwise use
                                # ID
                                pid_str = str(int(track_id))
                                player_name = player_names.get(
                                    pid_str, f"#{track_id}")
                                
                                # Get team for this player (already classified earlier, or use persistent assignment)
                                team = player_teams.get(track_id) if track_id in player_teams else None
                                # CRITICAL: If team is None, check persistent global mapping before re-classifying
                                if team is None and track_id in track_to_team_global:
                                    team = track_to_team_global[track_id]
                                    player_teams[track_id] = team  # Update local dict with persistent value
                                elif team is None and viz_color_mode == "team" and team_colors:
                                    # Last resort: try to classify now (only if not in persistent mapping)
                                    if i < len(detections.xyxy):
                                        team = classify_player_team(
                                            batch_frame, detections.xyxy[i], team_colors)
                                        if team:
                                            player_teams[track_id] = team
                                            # Store in persistent mapping
                                            track_to_team_global[track_id] = team
                                
                                # Generate label text based on label type
                                label_text = get_label_text(
                                    player_name, track_id, team, 
                                    current_label_type, current_label_custom_text, player_names
                                )
                                labels.append(label_text)
                                # Note: colors are already set earlier in the loop, so we don't need to set them again
                            else:
                                labels.append(f"{conf:.2f}")
                                colors.append((255, 0, 255))

                        # TRACK MERGING: Detect and merge duplicate tracks (same player on multiple tracks)
                        # This reduces duplicate IDs significantly (e.g., Rocco on tracks #1, #6, #7)
                        current_frame = frame_data.get('frame_num', frame_count)
                        # ENHANCED: Check every frame when there are detections (immediate conflict resolution)
                        # Also check all named tracks (not just active) to catch conflicts across time
                        should_check_merging = len(detections.xyxy) > 0 and detections.tracker_id is not None
                        
                        if should_check_merging:
                            try:
                                # Get active tracks from current detections
                                active_track_ids = [tid for tid in detections.tracker_id if tid is not None] if (detections.tracker_id is not None) else []
                                
                                # ENHANCED: Also check all tracks that have player names assigned (even if not currently active)
                                # This catches conflicts where a player is on Track #1, then later on Track #5
                                all_named_tracks = set()
                                for tid_str, name in player_names.items():
                                    if name and not name.startswith("#") and not name.startswith("Player "):
                                        try:
                                            all_named_tracks.add(int(tid_str))
                                        except:
                                            pass
                                
                                # Combine active and named tracks
                                tracks_to_check = list(set(active_track_ids + list(all_named_tracks)))
                                
                                merges_performed = 0
                                
                                # Check all pairs of tracks (both active and named)
                                # Track which tracks have been merged to avoid duplicate merges
                                merged_tracks = set()  # Tracks that have been merged (should be skipped)
                                
                                for i, track1_id in enumerate(tracks_to_check):
                                    # Skip if this track was already merged
                                    if track1_id in merged_tracks:
                                        continue
                                    
                                    for j, track2_id in enumerate(tracks_to_check):
                                        if i >= j or track1_id == track2_id:
                                            continue
                                        
                                        # Skip if either track was already merged
                                        if track2_id in merged_tracks:
                                            continue
                                        
                                        # ENHANCED: Check for conflicts first (don't require distance if it's a known conflict)
                                        # If player_to_track_global shows a conflict, merge immediately
                                        track1_str = str(int(track1_id))
                                        track2_str = str(int(track2_id))
                                        name1 = player_names.get(track1_str, "")
                                        name2 = player_names.get(track2_str, "")
                                        
                                        # Check if this is a known conflict
                                        is_conflict = False
                                        if name1 and name2 and name1 == name2 and name1 in player_to_track_global:
                                            assigned_track = player_to_track_global[name1]
                                            if assigned_track == track1_id and track2_id != track1_id:
                                                is_conflict = True
                                            elif assigned_track == track2_id and track1_id != track2_id:
                                                is_conflict = True
                                        
                                        # ðŸ›¡ï¸ GLOBAL MERGE PREVENTION: Skip if either track has already been merged globally
                                        if track1_id in global_merged_tracks or track2_id in global_merged_tracks:
                                            continue  # Skip - one or both tracks have already been merged in a previous frame
                                        
                                        # Check if these tracks should be merged
                                        # Use more lenient distance (100px) and allow conflict-based merging
                                        check_distance = not is_conflict  # Skip distance check for known conflicts
                                        
                                        # ðŸ›¡ï¸ DIAGNOSTIC: Log merge attempt for anchor-protected tracks
                                        track1_protected = (track_anchor_protection is not None and track1_id in track_anchor_protection)
                                        track2_protected = (track_anchor_protection is not None and track2_id in track_anchor_protection)
                                        if track1_protected or track2_protected:
                                            prot1_name = track_anchor_protection[track1_id][0] if track1_protected else None
                                            prot2_name = track_anchor_protection[track2_id][0] if track2_protected else None
                                            if current_frame_num % 100 == 0 or current_frame_num <= 30:
                                                print(f"  ðŸ” MERGE CHECK: Track #{track1_id} ('{name1}', protected={track1_protected}, prot_name='{prot1_name}') vs Track #{track2_id} ('{name2}', protected={track2_protected}, prot_name='{prot2_name}')")
                                        
                                        should_merge = should_merge_tracks(track1_id, track2_id, track_state, player_names, detections, 
                                                              player_to_track_global=player_to_track_global, 
                                                              max_distance_pixels=100, check_distance=check_distance,
                                                              track_anchor_protection=track_anchor_protection,
                                                              current_frame=current_frame_num)
                                        
                                        if should_merge:
                                            # Determine which track to keep
                                            # If it's a conflict, keep the one assigned in player_to_track_global
                                            if is_conflict and name1 in player_to_track_global:
                                                assigned_track = player_to_track_global[name1]
                                                if assigned_track == track1_id:
                                                    keep_track = track1_id
                                                    merge_track = track2_id
                                                elif assigned_track == track2_id:
                                                    keep_track = track2_id
                                                    merge_track = track1_id
                                                else:
                                                    # Fallback: keep lower ID
                                                    keep_track = min(track1_id, track2_id)
                                                    merge_track = max(track1_id, track2_id)
                                            else:
                                                # No conflict - keep track with lower ID (usually older/stronger)
                                                keep_track = min(track1_id, track2_id)
                                                merge_track = max(track1_id, track2_id)
                                            
                                            # ðŸ›¡ï¸ DIAGNOSTIC: Log merge details before performing
                                            keep_protected = (track_anchor_protection is not None and keep_track in track_anchor_protection)
                                            merge_protected = (track_anchor_protection is not None and merge_track in track_anchor_protection)
                                            keep_name = player_names.get(str(int(keep_track)), "Unknown")
                                            merge_name = player_names.get(str(int(merge_track)), "Unknown")
                                            
                                            if keep_protected or merge_protected:
                                                keep_prot_name = track_anchor_protection[keep_track][0] if keep_protected else None
                                                merge_prot_name = track_anchor_protection[merge_track][0] if merge_protected else None
                                                print(f"  ðŸ”€ MERGE PERFORMING: Track #{merge_track} ('{merge_name}', protected={merge_protected}, prot='{merge_prot_name}') â†’ Track #{keep_track} ('{keep_name}', protected={keep_protected}, prot='{keep_prot_name}')")
                                            
                                            # Merge tracks
                                            updates = merge_tracks(
                                                keep_track_id, merge_track_id, player_names, track_state,
                                                player_to_track_global, jersey_to_track_global, track_to_team_global,
                                                player_to_team_global, track_name_confidence, detections,
                                                track_anchor_protection, track_anchor_assigned,
                                                track_merge_cooldown, current_frame_num
                                            )
                                            
                                            # HOTA-GUIDED TRACKING: Track merge improves association accuracy
                                            if hota_guided_tracker is not None and updates > 0:
                                                try:
                                                    # Track merge is already handled by merge_tracks
                                                    # HOTA will recalculate association accuracy on next evaluation
                                                    pass
                                                except:
                                                    pass  # Silently fail
                                            
                                            if updates > 0:
                                                merges_performed += 1
                                                player_name = player_names.get(str(int(keep_track)), "Unknown")
                                                
                                                # Mark merged track so it won't be checked again (this frame)
                                                merged_tracks.add(merge_track)
                                                
                                                # ðŸ›¡ï¸ GLOBAL MERGE TRACKING: Mark as merged globally (persistent across all frames)
                                                global_merged_tracks.add(merge_track)
                                                
                                                if is_processing:
                                                    print(f"  ðŸ”€ Track merge: Merged Track #{merge_track} into Track #{keep_track} ({player_name}) - {updates} mappings updated")
                                
                                if merges_performed > 0 and is_processing:
                                    print(f"  âœ“ Track merging: Performed {merges_performed} merge(s) at frame {current_frame}")
                            except Exception as e:
                                # Silently fail - merging is optional
                                if current_frame % 500 == 0:  # Log errors occasionally
                                    print(f"  âš  Track merging failed: {e}")

                        # LEARN BALL INTERACTION: Calculate distances for all named players
                        if ball_center is not None and player_gallery and len(detections.xyxy) > 0:
                            try:
                                for i, (xyxy, track_id) in enumerate(zip(detections.xyxy, tracker_ids)):
                                    if track_id is not None:
                                        pid_str = str(int(track_id))
                                        if pid_str in player_names:
                                            player_name = player_names[pid_str]
                                            player_id = player_name.lower().replace(' ', '_')
                                            
                                            # Calculate player center
                                            x1, y1, x2, y2 = xyxy
                                            player_center_x = (x1 + x2) / 2
                                            player_center_y = (y1 + y2) / 2
                                            
                                            # Calculate distance to ball
                                            distance_to_ball = np.sqrt(
                                                (ball_center[0] - player_center_x)**2 + 
                                                (ball_center[1] - player_center_y)**2
                                            )
                                            
                                            # Learn ball interaction (threshold: 50 pixels)
                                            player_gallery.learn_ball_interaction(player_id, distance_to_ball, interaction_threshold=50.0)
                            except Exception as e:
                                # Silently fail - learning is optional
                                pass

                        # Annotate frame based on visualization style
                        # In watch-only mode, only annotate if live viewer is enabled
                        # Note: batch_frame is already the correct resolution (full or resized)
                        # We'll read the original frame when writing to output
                        # For now, create annotation on batch_frame (will be
                        # scaled if needed)
                        should_annotate = not watch_only or show_live_viewer
                        if should_annotate:
                            # Get current visualization settings (dynamic if available)
                            current_viz_style = get_setting('viz_style', viz_style)
                            current_viz_color_mode = get_setting('viz_color_mode', viz_color_mode)
                            current_ellipse_width = get_setting('ellipse_width', ellipse_width)
                            current_ellipse_height = get_setting('ellipse_height', ellipse_height)
                            current_ellipse_outline_thickness = get_setting('ellipse_outline_thickness', ellipse_outline_thickness)
                            current_box_thickness = get_setting('box_thickness', box_thickness)
                            current_box_color = get_setting('box_color', box_color)
                            # SEPARATE CONTROLS: Bounding boxes vs Circles at feet (user requested)
                            current_show_bounding_boxes = get_setting('show_bounding_boxes', show_bounding_boxes if 'show_bounding_boxes' in locals() else True)
                            current_show_circles_at_feet = get_setting('show_circles_at_feet', show_circles_at_feet if 'show_circles_at_feet' in locals() else True)
                            current_label_color = get_setting('label_color', label_color) if 'label_color' in locals() else None
                            current_show_player_labels = get_setting('show_player_labels', show_player_labels)
                            current_label_font_scale = get_setting('label_font_scale', label_font_scale)
                            current_label_type = get_setting('label_type', label_type)
                            current_label_custom_text = get_setting('label_custom_text', label_custom_text)
                            current_label_font_face = get_setting('label_font_face', label_font_face)
                            current_player_viz_alpha = get_setting('player_viz_alpha', player_viz_alpha) if 'player_viz_alpha' in locals() else 255
                            # CRITICAL: Overlays are drawn on a COPY of the frame - never modify the original
                            # This ensures detection always uses the original frame, preventing drift
                            # Advanced blending modes and professional text rendering only affect visualization
                            if yolo_resolution == "full":
                                annotated_frame = batch_frame.copy()  # COPY - original frame untouched
                            else:
                                # For resized frames, we'll need to scale up annotations later
                                # Create a placeholder - actual frame will be read
                                # when writing
                                annotated_frame = batch_frame.copy()  # COPY - original frame untouched
                            
                            # SAFETY CHECK: Ensure annotated_frame is never used for detection
                            # Detection must always use batch_frame (original), not annotated_frame (with overlays)
                            # Advanced blending modes modify pixel values - if used for detection, would cause drift

                            # Draw raw YOLO detection boxes (before tracking) if enabled
                            current_show_yolo_boxes = get_setting('show_yolo_boxes', show_yolo_boxes)
                            if current_show_yolo_boxes and raw_yolo_detections is not None and raw_yolo_detections['xyxy'] is not None and len(raw_yolo_detections['xyxy']) > 0:
                                # Draw raw YOLO boxes in orange (BGR format: (0, 165, 255))
                                yolo_box_color = (0, 165, 255)  # Orange
                                yolo_box_thickness = 1  # Thinner than tracked boxes
                                for xyxy in raw_yolo_detections['xyxy']:
                                    x1, y1, x2, y2 = map(int, xyxy)
                                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), yolo_box_color, yolo_box_thickness)
                            
                            # SEPARATE CONTROLS: Draw boxes and circles independently (user requested)
                            # Note: This is for the preview/batch processing path
                            # The main video writing path (frame_to_write) uses the separate controls below
                            if len(detections.xyxy) > 0:
                                for i, (xyxy, color) in enumerate(
                                        zip(detections.xyxy, colors)):
                                    x1, y1, x2, y2 = map(int, xyxy)

                                    # Draw bounding box if enabled (separate control)
                                    if current_show_bounding_boxes:
                                        # Use dynamic box thickness and color
                                        box_color_to_use = current_box_color if current_box_color else color
                                        cv2.rectangle(
                                            annotated_frame, (x1, y1), (x2, y2), box_color_to_use, current_box_thickness)
                                        
                                        # DEBUG: Draw XY coordinate overlay to help diagnose bbox position issues
                                        # Show coordinates at top-left of bbox
                                        coord_text = f"({x1},{y1})-({x2},{y2})"
                                        center_x = (x1 + x2) // 2
                                        center_y = (y1 + y2) // 2
                                        foot_y = y2
                                        coord_text2 = f"C:({center_x},{center_y}) F:{foot_y}"
                                        
                                        # Get track_id for display
                                        track_id = None
                                        if detections.tracker_id is not None and i < len(detections.tracker_id):
                                            track_id = detections.tracker_id[i]
                                        
                                        # Draw coordinate text with background for readability
                                        font = cv2.FONT_HERSHEY_SIMPLEX
                                        font_scale = 0.4
                                        thickness = 1
                                        
                                        # Calculate text size
                                        (text_w1, text_h1), _ = cv2.getTextSize(coord_text, font, font_scale, thickness)
                                        (text_w2, text_h2), _ = cv2.getTextSize(coord_text2, font, font_scale, thickness)
                                        
                                        # Draw background rectangles
                                        cv2.rectangle(annotated_frame, (x1, y1 - text_h1 - text_h2 - 6), 
                                                    (x1 + max(text_w1, text_w2) + 4, y1), (0, 0, 0), -1)
                                        
                                        # Draw coordinate text
                                        cv2.putText(annotated_frame, coord_text, (x1 + 2, y1 - text_h2 - 4), 
                                                  font, font_scale, (0, 255, 255), thickness)  # Cyan
                                        cv2.putText(annotated_frame, coord_text2, (x1 + 2, y1 - 2), 
                                                  font, font_scale, (0, 255, 255), thickness)  # Cyan
                                        
                                        # Draw track ID if available
                                        if track_id is not None:
                                            track_text = f"ID:{track_id}"
                                            (text_w3, text_h3), _ = cv2.getTextSize(track_text, font, font_scale, thickness)
                                            cv2.rectangle(annotated_frame, (x1, y2 + 2), 
                                                        (x1 + text_w3 + 4, y2 + text_h3 + 4), (0, 0, 0), -1)
                                            cv2.putText(annotated_frame, track_text, (x1 + 2, y2 + text_h3 + 2), 
                                                      font, font_scale, (255, 255, 0), thickness)  # Yellow
                                    # Legacy support
                                    elif current_viz_style in ["box", "both"] and box_annotator:
                                        # DEBUG: Log when legacy style is used
                                        if frame_count % 1000 == 0:  # Log every 1000 frames to avoid spam
                                            print(f"  ðŸ”§ LEGACY STYLE: Using legacy viz_style='{current_viz_style}' for bounding boxes (frame {frame_count})")
                                        # Use dynamic box thickness and color
                                        box_color_to_use = current_box_color if current_box_color else color
                                        cv2.rectangle(
                                            annotated_frame, (x1, y1), (x2, y2), box_color_to_use, current_box_thickness)

                                    # ENHANCEMENT: Track player position for direction arrow and trail
                                    track_id = None
                                    if detections.tracker_id is not None and i < len(detections.tracker_id):
                                        track_id = int(detections.tracker_id[i]) if detections.tracker_id[i] is not None else None
                                    
                                    # Calculate current position (center of feet)
                                    center_x = int((x1 + x2) / 2)
                                    foot_y = y2
                                    
                                    # Track position history for direction and trail
                                    direction_angle = None
                                    if track_id is not None:
                                        if track_id not in player_position_history:
                                            player_position_history[track_id] = deque(maxlen=MAX_POSITION_HISTORY)
                                        
                                        # Add current position
                                        player_position_history[track_id].append((center_x, foot_y, frame_count))
                                        
                                        # Calculate direction from recent positions
                                        if len(player_position_history[track_id]) >= 2:
                                            import math
                                            # Use last 2 positions to calculate direction
                                            pos_history = list(player_position_history[track_id])
                                            # Handle both (x, y) and (x, y, frame_count) formats
                                            prev_pos = pos_history[-2]
                                            curr_pos = pos_history[-1]
                                            # Unpack based on length (backward compatible)
                                            if len(prev_pos) >= 3:
                                                prev_x, prev_y, _ = prev_pos
                                            else:
                                                prev_x, prev_y = prev_pos
                                            if len(curr_pos) >= 3:
                                                curr_x, curr_y, _ = curr_pos
                                            else:
                                                curr_x, curr_y = curr_pos
                                            
                                            # Calculate velocity vector
                                            dx = curr_x - prev_x
                                            dy = curr_y - prev_y
                                            distance = math.sqrt(dx*dx + dy*dy)
                                            
                                            # Only show arrow if player is moving (threshold: 2 pixels)
                                            if distance > 2:
                                                # Calculate angle (0 = right, Ï€/2 = down, Ï€ = left, 3Ï€/2 = up)
                                                direction_angle = math.atan2(dy, dx)
                                                # Cache velocity for trail visualization
                                                player_velocity_cache[track_id] = (dx, dy, direction_angle)
                                            else:
                                                # Use cached direction if available
                                                if track_id in player_velocity_cache:
                                                    _, _, direction_angle = player_velocity_cache[track_id]
                                    
                                    # Draw player trail (breadcrumb trail) if enabled
                                    current_show_player_trail = get_setting('show_player_trail', False)
                                    current_trail_length = get_setting('trail_length', 20)
                                    if current_show_player_trail and track_id is not None and track_id in player_position_history:
                                        trail_positions = list(player_position_history[track_id])
                                        if len(trail_positions) > 1:
                                            # Draw trail with fading opacity
                                            trail_points = trail_positions[-min(current_trail_length, len(trail_positions)):]
                                            for j, (trail_x, trail_y, _) in enumerate(trail_points):
                                                if j < len(trail_points) - 1:  # Don't draw line from last point
                                                    next_x, next_y, _ = trail_points[j + 1]
                                                    # Fade opacity based on age (older = more transparent)
                                                    trail_alpha = int(255 * (1.0 - j / len(trail_points)) * 0.6)
                                                    trail_color = tuple(int(c * trail_alpha / 255.0) for c in color)
                                                    cv2.line(annotated_frame, (int(trail_x), int(trail_y)), 
                                                            (int(next_x), int(next_y)), trail_color, 2, cv2.LINE_AA)

                                    # Draw circle at feet if enabled (separate control, always uses team colors)
                                    if current_show_circles_at_feet:
                                        # Calculate foot position (bottom of box)
                                        # Apply vertical offset (negative = above feet, positive = below feet)
                                        current_feet_vertical_offset = get_setting('feet_marker_vertical_offset', 50)
                                        foot_y = y2 + current_feet_vertical_offset
                                        # Draw ellipse at feet (always uses team colors)
                                        scale_factor = max(1.0, yolo_width / 1920.0)
                                        scaled_width = int(current_ellipse_width * scale_factor)
                                        scaled_height = int(current_ellipse_height * scale_factor)
                                        axes = (int(scaled_width / 2), int(scaled_height / 2))
                                        ellipse_color = blend_color_with_opacity(color, current_player_viz_alpha)
                                        
                                        # Get enhanced feet marker settings
                                        current_feet_style = get_setting('feet_marker_style', 'circle')
                                        current_feet_opacity = get_setting('feet_marker_opacity', 255)
                                        current_feet_glow = get_setting('feet_marker_enable_glow', False)
                                        current_feet_glow_intensity = get_setting('feet_marker_glow_intensity', 50)
                                        current_feet_shadow = get_setting('feet_marker_enable_shadow', False)
                                        current_feet_shadow_offset = get_setting('feet_marker_shadow_offset', 3)
                                        current_feet_shadow_opacity = get_setting('feet_marker_shadow_opacity', 128)
                                        current_feet_gradient = get_setting('feet_marker_enable_gradient', False)
                                        current_feet_pulse = get_setting('feet_marker_enable_pulse', False)
                                        current_feet_pulse_speed = get_setting('feet_marker_pulse_speed', 2.0)
                                        current_feet_particles = get_setting('feet_marker_enable_particles', False)
                                        current_feet_particle_count = get_setting('feet_marker_particle_count', 5)
                                        
                                        # ENHANCEMENT: Direction arrow setting
                                        current_show_direction_arrow = get_setting('show_direction_arrow', False)
                                        arrow_color_setting = get_setting('direction_arrow_color', None)
                                        arrow_color = None
                                        if arrow_color_setting:
                                            arrow_color = tuple(int(c) for c in arrow_color_setting) if isinstance(arrow_color_setting, (list, tuple)) else (255, 255, 255)
                                        
                                        # Use enhanced rendering if any advanced features are enabled
                                        if (current_feet_style != "circle" and current_feet_style != "ellipse") or \
                                           current_feet_glow or current_feet_shadow or \
                                           current_feet_gradient or current_feet_pulse or \
                                           current_feet_particles or current_feet_opacity < 255 or \
                                           current_show_direction_arrow:
                                            draw_enhanced_feet_marker(
                                                annotated_frame,
                                                (center_x, int(foot_y)), axes,
                                                current_feet_style, ellipse_color, current_feet_opacity,
                                                current_feet_glow, current_feet_glow_intensity,
                                                current_feet_shadow, current_feet_shadow_offset, current_feet_shadow_opacity,
                                                current_feet_gradient, current_feet_pulse, current_feet_pulse_speed, frame_count,
                                                current_feet_particles, current_feet_particle_count,
                                                current_ellipse_outline_thickness, fps,
                                                show_direction_arrow=current_show_direction_arrow,
                                                direction_angle=direction_angle,
                                                arrow_color=arrow_color
                                            )
                                        else:
                                            # Standard ellipse (legacy mode for performance)
                                            cv2.ellipse(annotated_frame, (center_x, int(foot_y)), axes, 0, 0, 360, ellipse_color, -1)
                                            # Only draw border if thickness > 0 (0 means no border)
                                            if current_ellipse_outline_thickness > 0:
                                                cv2.ellipse(annotated_frame, (center_x, int(foot_y)), axes, 0, 0, 360, (255, 255, 255), current_ellipse_outline_thickness)
                                            
                                            # Draw direction arrow separately if enabled (for standard mode)
                                            if current_show_direction_arrow and direction_angle is not None:
                                                arrow_col = arrow_color if arrow_color else (255, 255, 255)
                                                arrow_center = (center_x, int(foot_y) + axes[1] + 5)
                                                draw_direction_arrow(annotated_frame, arrow_center, direction_angle,
                                                                   arrow_length=20, arrow_color=arrow_col, 
                                                                   arrow_thickness=2, arrow_head_size=8)
                                    # Legacy support
                                    elif current_viz_style in ["circle", "both"] and circle_annotator:
                                        # DEBUG: Log when legacy style is used
                                        if frame_count % 1000 == 0:  # Log every 1000 frames to avoid spam
                                            print(f"  ðŸ”§ LEGACY STYLE: Using legacy viz_style='{current_viz_style}' for circles (frame {frame_count})")
                                        # Calculate center
                                        cx = int((x1 + x2) / 2)
                                        cy = int((y1 + y2) / 2)
                                        radius = 30
                                        # Manually draw circle with custom color
                                        cv2.circle(
                                            annotated_frame, (cx, cy), radius, color, 2)

                            # CRITICAL FIX: Draw labels manually with our custom colors (not supervision's auto-colors)
                            # This prevents vibrant random colors and ensures team colors are used
                            # ENHANCED: Only draw labels if show_player_labels is
                            # enabled (can hide to reduce clutter) - use dynamic setting
                            if current_show_player_labels and len(
                                    labels) > 0 and len(detections.xyxy) > 0:
                                tracker_ids = detections.tracker_id if detections.tracker_id is not None else [
                                    None] * len(detections.xyxy)
                                for i, (xyxy, label, color) in enumerate(
                                        zip(detections.xyxy, labels, colors)):
                                    if i >= len(tracker_ids):
                                        continue
                                    x1, y1, x2, y2 = map(int, xyxy)

                                    # Position label beside player (to the right of
                                    # ellipse) instead of above head
                                    # Calculate raw label position
                                    raw_label_x = x2 + 5  # Right side of bounding box + small margin
                                    raw_label_y = int((y1 + y2) / 2)  # Vertically centered on player
                                    
                                    # CRITICAL FIX: Adjustable label size (can be made smaller to reduce clutter)
                                    # Use GUI-adjustable font scale (default: 0.7,
                                    # can be reduced for less clutter) - use dynamic setting
                                    font_scale = current_label_font_scale  # Use dynamic font scale
                                    # Scale thickness with font size
                                    thickness = max(1, int(current_label_font_scale * 2.5))
                                    
                                    # Get font face
                                    font_face_str = current_label_font_face
                                    font_face = getattr(cv2, font_face_str, cv2.FONT_HERSHEY_SIMPLEX)
                                    
                                    (text_width, text_height), baseline = cv2.getTextSize(
                                        label, font_face, font_scale, thickness)

                                    # Smooth label position to prevent flickering (includes side-switching logic)
                                    track_id = tracker_ids[i] if i < len(tracker_ids) else None
                                    label_x, label_y = get_smoothed_label_position(
                                        track_id, raw_label_x, raw_label_y, text_width, yolo_width, x1, x2)

                                    # Draw larger text background for better
                                    # readability (semi-transparent black to prevent flashing)
                                    padding = 4  # Extra padding around text
                                    bg_x1 = label_x - padding
                                    bg_y1 = label_y - text_height - padding
                                    bg_x2 = label_x + text_width + padding
                                    bg_y2 = label_y + baseline + padding
                                    
                                    # Use semi-transparent black background (alpha blending) to prevent flashing
                                    # Create overlay with semi-transparent black
                                    overlay = annotated_frame.copy()
                                    cv2.rectangle(overlay,
                                                  (bg_x1, bg_y1),
                                                  (bg_x2, bg_y2),
                                                  (0, 0, 0), -1)  # Black background
                                    # Blend with alpha (0.6 = 60% opacity, reduces flashing)
                                    alpha = 0.6
                                    cv2.addWeighted(overlay, alpha, annotated_frame, 1 - alpha, 0, annotated_frame)

                                    # Draw label text with larger font and our
                                    # custom color (custom label color, team color, or gray)
                                    label_color_to_use = current_label_color if current_label_color else color
                                    cv2.putText(annotated_frame, label, (label_x, label_y),
                                                font_face, font_scale, label_color_to_use, thickness)

                        # Extract player centers
                        # PATCH 2: Separate pixel vs real-world coordinates - ensure pixel coords are used for drawing
                        player_centers = {}  # Pixel coordinates for drawing (px, py)
                        pixel_centers = {}  # Explicit pixel coordinates (for verification)
                        # Debug logging for first few frames
                        if current_frame_num <= 10:
                            detections_debug = "None" if detections is None else f"has xyxy={hasattr(detections, 'xyxy')}" if not hasattr(detections, 'xyxy') else f"xyxy len={len(detections.xyxy) if detections.xyxy is not None else 0}"
                            tracker_id_debug = "None" if detections is None or not hasattr(detections, 'tracker_id') else f"tracker_id={detections.tracker_id is not None}, len={len(detections.tracker_id) if detections.tracker_id is not None else 0}"
                            print(f"ðŸ” Frame {current_frame_num}: Before player_centers check - detections: {detections_debug}, {tracker_id_debug}")
                        
                        if detections is not None and hasattr(detections, 'xyxy') and detections.xyxy is not None and len(detections.xyxy) > 0:
                            # Handle case where tracker_id is None
                            tracker_ids = detections.tracker_id if detections.tracker_id is not None else [
                                None] * len(detections.xyxy)
                            tracks_with_id = 0
                            for i, (track_id, xyxy) in enumerate(
                                    zip(tracker_ids, detections.xyxy)):
                                # CRITICAL FIX: Skip None and -1 track IDs (invalid tracks)
                                if track_id is not None and track_id != -1:
                                    x1, y1, x2, y2 = xyxy
                                    # PATCH 2: Calculate pixel coordinates explicitly (for drawing)
                                    # FOOT-BASED TRACKING FIX: Use foot position (x at center, y at bottom) 
                                    # instead of center of body when foot_based_tracking is enabled
                                    if foot_based_tracking:
                                        # Use foot position: center X, bottom Y
                                        player_x = int((x1 + x2) / 2)
                                        player_y = int(y2)  # Bottom of box = foot position
                                    else:
                                        # Use center of body (legacy behavior)
                                        player_x = int((x1 + x2) / 2)
                                        player_y = int((y1 + y2) / 2)
                                    
                                    # PATCH 2: Store pixel coordinates explicitly (for verification and drawing)
                                    pixel_centers[track_id] = (player_x, player_y)
                                    
                                    # GSI: Update track history and apply smoothing if enabled
                                    track_id_int = int(track_id)
                                    if use_gsi and GSI_AVAILABLE:
                                        if track_id_int not in gsi_track_history:
                                            gsi_track_history[track_id_int] = []
                                        gsi_track_history[track_id_int].append((current_frame_num, float(player_x), float(player_y)))
                                        # Keep only last 100 frames per track
                                        if len(gsi_track_history[track_id_int]) > 100:
                                            gsi_track_history[track_id_int] = gsi_track_history[track_id_int][-100:]
                                        
                                        # Apply real-time GSI smoothing
                                        if track_id_int in gsi_track_history and len(gsi_track_history[track_id_int]) >= 2:
                                            try:
                                                smoothed_pos = apply_gsi_realtime(
                                                    {track_id_int: (float(player_x), float(player_y))},
                                                    {track_id_int: gsi_track_history[track_id_int]},
                                                    current_frame_num,
                                                    tau=gsi_tau if 'gsi_tau' in locals() else 10.0
                                                )
                                                if track_id_int in smoothed_pos:
                                                    player_x, player_y = smoothed_pos[track_id_int]
                                                    player_x, player_y = int(player_x), int(player_y)
                                            except Exception as e:
                                                # If GSI fails, use original position
                                                if current_frame_num % 500 == 0:
                                                    print(f"  âš  GSI smoothing failed for track {track_id_int}: {e}")
                                    
                                    player_centers[track_id] = (player_x, player_y)
                                    heatmap_data.append([player_x, player_y])
                                    tracks_with_id += 1
                                    
                                    # PATCH 2: Verification - log pixel coordinates (first 10 frames only)
                                    if current_frame_num <= 10 and i < 2:  # Log first 2 detections
                                        print(f"  âœ“ PATCH 2: Frame {current_frame_num}, Track {track_id} - Pixel coords: ({player_x}, {player_y}), Bbox: ({x1}, {y1})-({x2}, {y2})")
                                    
                                    # ðŸŽ¯ IDENTITY TRACKER: Update track bbox history and reconnect lost identities
                                    if identity_tracker is not None:
                                        bbox = [float(x1), float(y1), float(x2), float(y2)]
                                        identity_tracker.update_track(track_id_int, bbox, current_frame_num)
                                        
                                        # Check if this track lost its identity (track ID changed)
                                        # Try to reconnect using bbox position
                                        track_id_str = str(track_id_int)
                                        if track_id_str not in player_names or not player_names[track_id_str]:
                                            # Track has no identity - try to reconnect
                                            match_result = identity_tracker.find_player_by_position(
                                                bbox=bbox,
                                                frame_num=current_frame_num,
                                                exclude_track_ids=[track_id_int]  # Exclude current track
                                            )
                                            
                                            if match_result:
                                                old_track_id, player_name, confidence = match_result
                                                # Reconnect identity
                                                player_names[track_id_str] = player_name
                                                track_name_confidence[track_id_int] = (player_name, confidence, current_frame_num)
                                                
                                                # Update identity tracker with new track_id
                                                identity_tracker.assign_identity(
                                                    track_id=track_id_int,
                                                    player_name=player_name,
                                                    confidence=confidence,
                                                    frame_num=current_frame_num,
                                                    bbox=bbox
                                                )
                                                
                                                if current_frame_num % 50 == 0 or current_frame_num < 10:
                                                    print(f"  ðŸ”„ IDENTITY RECONNECTED: '{player_name}' reconnected from Track #{old_track_id} â†’ Track #{track_id_int} (bbox match)")
                                        
                                        # Also check if track has identity but it's not in player_names (sync issue)
                                        identity = identity_tracker.get_identity(track_id_int)
                                        if identity:
                                            player_name_from_tracker, conf, _ = identity
                                            if track_id_str not in player_names or player_names[track_id_str] != player_name_from_tracker:
                                                # Sync player_names with identity tracker
                                                player_names[track_id_str] = player_name_from_tracker
                                                if track_id_int not in track_name_confidence:
                                                    track_name_confidence[track_id_int] = (player_name_from_tracker, conf, current_frame_num)
                                                if current_frame_num % 100 == 0:
                                                    print(f"  ðŸ”„ IDENTITY SYNCED: Track #{track_id_int} â†’ '{player_name_from_tracker}' (from identity tracker)")
                            # DIAGNOSTIC: Log when player_centers is populated (helps debug CSV export issues)
                            current_frame = frame_data.get('frame_num', 0)
                            should_log = (current_frame <= 10) or (current_frame % 1000 == 0)
                            if should_log and tracks_with_id == 0:
                                print(f"  âš  Player centers: Frame {current_frame} has {len(detections.xyxy)} detections but 0 tracks with ID (all track_id are None)")
                            elif should_log and tracks_with_id > 0:
                                print(f"  âœ“ Player centers: Frame {current_frame} populated {tracks_with_id} tracks with ID (total detections: {len(detections.xyxy)})")
                        # DIAGNOSTIC: Log if player_centers is empty (helps debug CSV export issues)
                        else:
                            current_frame = frame_data.get('frame_num', 0)
                            should_log = (current_frame <= 10) or (current_frame % 1000 == 0)
                            if should_log:
                                detections_status = "None" if detections is None else f"empty xyxy" if not hasattr(detections, 'xyxy') or detections.xyxy is None else f"{len(detections.xyxy)} detections"
                                print(f"  âš  Player centers: No detections available at frame {current_frame} (detections: {detections_status})")

                        # Calculate possession
                        possession_player_id = None
                        distance_to_ball = None
                        if frame_data['ball_center'] is not None and player_centers:
                            possession_player_id, distance_to_ball = calculate_possession(
                                frame_data['ball_center'], player_centers, width, height
                            )

                        # Calculate player analytics (speed, movement angle, distance to ball, real-world position, distance traveled, etc.)
                        # ANALYTICS KEYED BY player_name (not track_id) - aggregates across all track_ids for same player
                        player_analytics = {}  # player_name -> {speed_mps, acceleration_mps2, movement_angle, distance_to_ball, distance_traveled_m, max_speed_mps, sprint_count, x_m, y_m}
                        if player_centers and homography_matrix is not None:
                            for track_id, (px, py) in player_centers.items():
                                # Get player_name for this track_id (use for all analytics - aggregates across track_id changes)
                                player_name = player_names.get(str(track_id), f"Player_{track_id}")
                                
                                # Transform player position to real-world coordinates
                                player_pos_m = transform_point_to_field((px, py), homography_matrix)
                                player_x_m = None
                                player_y_m = None
                                if player_pos_m is not None:
                                    player_x_m, player_y_m = player_pos_m
                                
                                # Initialize tracking variables if needed (keyed by player_name, not track_id)
                                if player_name not in player_distance_traveled_m:
                                    player_distance_traveled_m[player_name] = 0.0
                                if player_name not in player_max_speed_mps:
                                    player_max_speed_mps[player_name] = 0.0
                                if player_name not in player_sprint_count:
                                    player_sprint_count[player_name] = 0
                                if player_name not in player_possession_time:
                                    player_possession_time[player_name] = 0.0
                                if player_name not in player_direction_changes:
                                    player_direction_changes[player_name] = 0
                                if player_name not in player_speed_history:
                                    player_speed_history[player_name] = deque(maxlen=speed_average_window)
                                if player_name not in player_speed_zone_distances:
                                    player_speed_zone_distances[player_name] = {'walking': 0.0, 'jogging': 0.0, 'running': 0.0, 'sprinting': 0.0}
                                if player_name not in player_stationary_time:
                                    player_stationary_time[player_name] = 0.0
                                if player_name not in player_acceleration_events:
                                    player_acceleration_events[player_name] = 0
                                
                                # Calculate player speed, movement angle, and distance traveled
                                player_speed_mps = None
                                player_movement_angle = None
                                distance_traveled_this_frame = 0.0
                                # Use player_last_pos_m which stores the previous frame's position (more reliable than deque)
                                # Keyed by player_name to aggregate across track_id changes
                                if player_name in player_last_pos_m and player_last_pos_m[player_name] is not None:
                                    prev_pos_m = player_last_pos_m[player_name]  # Position from previous frame
                                    if prev_pos_m is not None and player_pos_m is not None:
                                        # Calculate time difference (assume 1 frame = 1/fps seconds)
                                        time_diff = 1.0 / fps if fps > 0 else 0
                                        if time_diff > 0:
                                            # Calculate speed
                                            player_speed_mps = calculate_trajectory_speed(prev_pos_m, player_pos_m, time_diff)
                                            # Calculate movement angle
                                            player_movement_angle = calculate_trajectory_angle(prev_pos_m, player_pos_m)
                                            # Calculate distance traveled this frame (in meters)
                                            distance_traveled_this_frame = np.sqrt(
                                                (player_pos_m[0] - prev_pos_m[0])**2 + 
                                                (player_pos_m[1] - prev_pos_m[1])**2
                                            )
                                            # Accumulate total distance traveled (aggregated by player_name)
                                            player_distance_traveled_m[player_name] += distance_traveled_this_frame
                                            
                                            # Update max speed (max across all track_ids for this player)
                                            if player_speed_mps is not None and player_speed_mps > player_max_speed_mps[player_name]:
                                                player_max_speed_mps[player_name] = player_speed_mps
                                            
                                            # Count sprints (speed > threshold, and wasn't sprinting in previous frame)
                                            if player_speed_mps is not None and player_speed_mps > sprint_threshold_mps:
                                                prev_speed = player_prev_speed_mps.get(player_name, 0.0)
                                                if prev_speed <= sprint_threshold_mps:  # Just entered sprint zone
                                                    player_sprint_count[player_name] += 1
                                
                                # Calculate acceleration (change in speed over time)
                                player_acceleration_mps2 = None
                                if player_name in player_prev_speed_mps and player_prev_speed_mps[player_name] is not None and player_speed_mps is not None:
                                    time_diff = 1.0 / fps if fps > 0 else 0
                                    if time_diff > 0:
                                        speed_change = player_speed_mps - player_prev_speed_mps[player_name]
                                        player_acceleration_mps2 = speed_change / time_diff
                                
                                # Store current speed for next frame's acceleration calculation
                                if player_speed_mps is not None:
                                    player_prev_speed_mps[player_name] = player_speed_mps
                                
                                # Calculate distance to ball for ALL players (not just possession player)
                                player_distance_to_ball = None
                                if frame_data['ball_center'] is not None:
                                    # Distance in pixels
                                    player_distance_to_ball = np.sqrt(
                                        (frame_data['ball_center'][0] - px)**2 + 
                                        (frame_data['ball_center'][1] - py)**2
                                    )
                                
                                # Calculate time in possession (accumulate when player has ball)
                                if possession_player_id == track_id:
                                    time_diff = 1.0 / fps if fps > 0 else 0
                                    player_possession_time[player_name] += time_diff
                                    
                                    # ðŸŽ¬ PER-PLAYER CLIP EXPORT: Export 10-second clips when possession > 5 seconds
                                    # Track possession events and export clips for highlight reels
                                    if player_possession_time[player_name] >= 5.0:  # 5 seconds threshold
                                        # Check if we've already exported a clip for this possession event
                                        if 'player_clip_events' not in locals():
                                            player_clip_events = {}  # track_id -> (start_frame, end_frame, exported)
                                        
                                        if track_id not in player_clip_events:
                                            # New possession event - mark start frame
                                            clip_start_frame = max(0, frame_count - int(5.0 * fps))  # 5 seconds before current
                                            clip_end_frame = min(total_frames - 1 if 'total_frames' in locals() else frame_count + int(5.0 * fps), frame_count + int(5.0 * fps))  # 5 seconds after
                                            player_clip_events[track_id] = (clip_start_frame, clip_end_frame, False)
                                        
                                        # Check if we should export this clip (at the end of possession or at clip_end_frame)
                                        clip_start, clip_end, exported = player_clip_events[track_id]
                                        if not exported and (possession_player_id != track_id or frame_count >= clip_end):
                                            # Export the clip
                                            try:
                                                player_name = player_names.get(str(track_id), f"Player_{track_id}")
                                                if player_name and player_name.strip():
                                                    # Sanitize player name for filename
                                                    safe_name = "".join(c for c in player_name if c.isalnum() or c in (' ', '-', '_')).strip()
                                                    safe_name = safe_name.replace(' ', '_')
                                                    
                                                    # Create clips directory
                                                    clips_dir = os.path.join(os.path.dirname(output_path), "player_clips")
                                                    os.makedirs(clips_dir, exist_ok=True)
                                                    
                                                    # Generate clip filename
                                                    clip_filename = f"{safe_name}_possession_f{clip_start}_to_f{clip_end}.mp4"
                                                    clip_path = os.path.join(clips_dir, clip_filename)
                                                    
                                                    # Export clip using ffmpeg (if available)
                                                    if 'input_path' in locals() and input_path:
                                                        try:
                                                            import subprocess
                                                            # Use ffmpeg to extract clip
                                                            cmd = [
                                                                'ffmpeg', '-y',  # Overwrite output
                                                                '-i', input_path,
                                                                '-ss', str(clip_start / fps),  # Start time in seconds
                                                                '-t', str((clip_end - clip_start) / fps),  # Duration in seconds
                                                                '-c', 'copy',  # Copy codec (fast, no re-encoding)
                                                                clip_path
                                                            ]
                                                            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
                                                            if result.returncode == 0:
                                                                print(f"  ðŸŽ¬ CLIP EXPORTED: {player_name} possession clip saved: {clip_filename} ({clip_end - clip_start} frames)")
                                                                player_clip_events[track_id] = (clip_start, clip_end, True)
                                                            else:
                                                                if frame_count % 1000 == 0:
                                                                    print(f"  âš  Clip export failed for {player_name}: {result.stderr[:200]}")
                                                        except Exception as e:
                                                            if frame_count % 1000 == 0:
                                                                print(f"  âš  Clip export error for {player_name}: {e}")
                                            except Exception as e:
                                                if frame_count % 1000 == 0:
                                                    print(f"  âš  Clip export setup error: {e}")
                                        
                                        # Reset possession time after exporting (to detect new events)
                                        if track_id in player_clip_events and player_clip_events[track_id][2]:  # Exported
                                            player_possession_time[player_name] = 0.0  # Reset for next event
                                else:
                                    # Player lost possession - reset clip tracking if they had an event
                                    if 'player_clip_events' in locals() and track_id in player_clip_events:
                                        clip_start, clip_end, exported = player_clip_events[track_id]
                                        if not exported:
                                            # Export clip now (possession ended before 5 seconds, but export what we have)
                                            try:
                                                player_name = player_names.get(str(track_id), f"Player_{track_id}")
                                                if player_name and player_name.strip():
                                                    safe_name = "".join(c for c in player_name if c.isalnum() or c in (' ', '-', '_')).strip()
                                                    safe_name = safe_name.replace(' ', '_')
                                                    clips_dir = os.path.join(os.path.dirname(output_path), "player_clips")
                                                    os.makedirs(clips_dir, exist_ok=True)
                                                    clip_filename = f"{safe_name}_possession_f{clip_start}_to_f{frame_count}.mp4"
                                                    clip_path = os.path.join(clips_dir, clip_filename)
                                                    
                                                    if 'input_path' in locals() and input_path:
                                                        import subprocess
                                                        cmd = [
                                                            'ffmpeg', '-y',
                                                            '-i', input_path,
                                                            '-ss', str(clip_start / fps),
                                                            '-t', str((frame_count - clip_start) / fps),
                                                            '-c', 'copy',
                                                            clip_path
                                                        ]
                                                        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
                                                        if result.returncode == 0:
                                                            print(f"  ðŸŽ¬ CLIP EXPORTED (short): {player_name} possession clip saved: {clip_filename}")
                                                            player_clip_events[track_id] = (clip_start, frame_count, True)
                                            except:
                                                pass
                                        # Remove from tracking
                                        del player_clip_events[track_id]
                                
                                # Calculate direction changes (>45Â° angle change)
                                if player_movement_angle is not None:
                                    if player_name in player_prev_movement_angle and player_prev_movement_angle[player_name] is not None:
                                        angle_change = abs(player_movement_angle - player_prev_movement_angle[player_name])
                                        # Handle wrap-around (e.g., 179Â° to -179Â° = 2Â° change, not 358Â°)
                                        if angle_change > 180:
                                            angle_change = 360 - angle_change
                                        if angle_change > 45:  # Significant direction change
                                            player_direction_changes[player_name] += 1
                                    player_prev_movement_angle[player_name] = player_movement_angle
                                
                                # Calculate rolling average speed
                                avg_speed_mps = None
                                if player_speed_mps is not None:
                                    player_speed_history[player_name].append(player_speed_mps)
                                    if len(player_speed_history[player_name]) > 0:
                                        avg_speed_mps = sum(player_speed_history[player_name]) / len(player_speed_history[player_name])
                                
                                # Calculate speed zone distances (walking, jogging, running, sprinting)
                                if distance_traveled_this_frame > 0 and player_speed_mps is not None:
                                    if player_speed_mps < walking_threshold:
                                        player_speed_zone_distances[player_name]['walking'] += distance_traveled_this_frame
                                    elif player_speed_mps < jogging_threshold:
                                        player_speed_zone_distances[player_name]['jogging'] += distance_traveled_this_frame
                                    elif player_speed_mps < running_threshold:
                                        player_speed_zone_distances[player_name]['running'] += distance_traveled_this_frame
                                    else:
                                        player_speed_zone_distances[player_name]['sprinting'] += distance_traveled_this_frame
                                
                                # Calculate time stationary (speed below threshold)
                                if player_speed_mps is not None and player_speed_mps < stationary_threshold_mps:
                                    time_diff = 1.0 / fps if fps > 0 else 0
                                    player_stationary_time[player_name] += time_diff
                                
                                # Count acceleration events (>2 m/sÂ²) - track previous acceleration
                                prev_accel = player_prev_acceleration_mps2.get(player_name, 0.0)
                                if player_acceleration_mps2 is not None and player_acceleration_mps2 > acceleration_event_threshold:
                                    # Only count if previous acceleration was below threshold (entering acceleration zone)
                                    if prev_accel <= acceleration_event_threshold:
                                        player_acceleration_events[player_name] += 1
                                # Store current acceleration for next frame
                                if player_acceleration_mps2 is not None:
                                    player_prev_acceleration_mps2[player_name] = player_acceleration_mps2
                                elif player_name not in player_prev_acceleration_mps2:
                                    player_prev_acceleration_mps2[player_name] = 0.0
                                
                                # Calculate distance from field center
                                distance_from_center_m = None
                                if field_center_m is not None and player_pos_m is not None:
                                    distance_from_center_m = np.sqrt(
                                        (player_pos_m[0] - field_center_m[0])**2 + 
                                        (player_pos_m[1] - field_center_m[1])**2
                                    )
                                
                                # Calculate distance from nearest goal
                                distance_from_goal_m = None
                                if goal1_pos_m is not None and goal2_pos_m is not None and player_pos_m is not None:
                                    dist_to_goal1 = np.sqrt(
                                        (player_pos_m[0] - goal1_pos_m[0])**2 + 
                                        (player_pos_m[1] - goal1_pos_m[1])**2
                                    )
                                    dist_to_goal2 = np.sqrt(
                                        (player_pos_m[0] - goal2_pos_m[0])**2 + 
                                        (player_pos_m[1] - goal2_pos_m[1])**2
                                    )
                                    distance_from_goal_m = min(dist_to_goal1, dist_to_goal2)
                                
                                # Calculate field zone (defensive, midfield, attacking)
                                field_zone = None
                                if field_dims is not None and player_pos_m is not None:
                                    field_length, field_width = field_dims
                                    x_pct = (player_pos_m[0] / field_length) * 100 if field_length > 0 else 50.0
                                    # Assume field is oriented: 0% = defensive end, 50% = midfield, 100% = attacking end
                                    if x_pct < 33.3:
                                        field_zone = "defensive"
                                    elif x_pct < 66.7:
                                        field_zone = "midfield"
                                    else:
                                        field_zone = "attacking"
                                
                                # Calculate field position as percentage (0-100%)
                                field_position_x_pct = None
                                field_position_y_pct = None
                                if field_dims is not None and player_pos_m is not None:
                                    field_length, field_width = field_dims
                                    field_position_x_pct = (player_pos_m[0] / field_length) * 100 if field_length > 0 else None
                                    field_position_y_pct = (player_pos_m[1] / field_width) * 100 if field_width > 0 else None
                                
                                # Calculate distance to nearest teammate and opponent
                                nearest_teammate_dist_m = None
                                nearest_opponent_dist_m = None
                                if player_pos_m is not None:
                                    # Get team assignment for this player (if available)
                                    player_team = None
                                    # Try to get team from player names/teams data
                                    try:
                                        if track_id in player_to_track_global.values():
                                            for pid, tid in player_to_track_global.items():
                                                if tid == track_id:
                                                    # Get team from player_teams if available
                                                    if hasattr(frame_data, 'player_teams') and pid in frame_data.get('player_teams', {}):
                                                        player_team = frame_data['player_teams'][pid]
                                                    break
                                    except:
                                        pass
                                    
                                    # Calculate distances to all other players
                                    min_teammate_dist = float('inf')
                                    min_opponent_dist = float('inf')
                                    for other_track_id, other_pos_m in player_last_pos_m.items():
                                        if other_track_id != track_id and other_pos_m is not None:
                                            dist = np.sqrt(
                                                (player_pos_m[0] - other_pos_m[0])**2 + 
                                                (player_pos_m[1] - other_pos_m[1])**2
                                            )
                                            # Try to determine if same team (simplified - would need team data)
                                            # For now, use distance to all players and let user filter by team in analysis
                                            if dist < min_teammate_dist:
                                                min_teammate_dist = dist
                                            if dist < min_opponent_dist:
                                                min_opponent_dist = dist
                                    
                                    if min_teammate_dist != float('inf'):
                                        nearest_teammate_dist_m = min_teammate_dist
                                    if min_opponent_dist != float('inf'):
                                        nearest_opponent_dist_m = min_opponent_dist
                                
                                # Store analytics keyed by player_name (aggregates across track_id changes)
                                player_analytics[player_name] = {
                                    'speed_mps': player_speed_mps,
                                    'acceleration_mps2': player_acceleration_mps2,
                                    'movement_angle': player_movement_angle,
                                    'distance_to_ball': player_distance_to_ball,
                                    'distance_traveled_m': player_distance_traveled_m[player_name],
                                    'max_speed_mps': player_max_speed_mps[player_name],
                                    'sprint_count': player_sprint_count[player_name],
                                    'possession_time_s': player_possession_time[player_name],
                                    'distance_from_center_m': distance_from_center_m,
                                    'distance_from_goal_m': distance_from_goal_m,
                                    'field_zone': field_zone,
                                    'field_position_x_pct': field_position_x_pct,
                                    'field_position_y_pct': field_position_y_pct,
                                    'direction_changes': player_direction_changes[player_name],
                                    'avg_speed_mps': avg_speed_mps,
                                    'distance_walking_m': player_speed_zone_distances[player_name]['walking'],
                                    'distance_jogging_m': player_speed_zone_distances[player_name]['jogging'],
                                    'distance_running_m': player_speed_zone_distances[player_name]['running'],
                                    'distance_sprinting_m': player_speed_zone_distances[player_name]['sprinting'],
                                    'time_stationary_s': player_stationary_time[player_name],
                                    'acceleration_events': player_acceleration_events[player_name],
                                    'nearest_teammate_dist_m': nearest_teammate_dist_m,
                                    'nearest_opponent_dist_m': nearest_opponent_dist_m,
                                    'x_m': player_x_m,
                                    'y_m': player_y_m
                                }
                                
                                # CRITICAL: Update player_last_pos_m for next frame's speed calculation (keyed by player_name)
                                if player_pos_m is not None:
                                    player_last_pos_m[player_name] = player_pos_m

                        # Calculate trajectory data (real-world coordinates,
                        # angle, speed)
                        ball_x_m = None
                        ball_y_m = None
                        trajectory_angle = None
                        ball_speed_mps = None

                        if frame_data['ball_center'] is not None and homography_matrix is not None:
                            # Transform ball position to real-world coordinates
                            ball_pos_m = transform_point_to_field(
                                frame_data['ball_center'], homography_matrix)
                            if ball_pos_m is not None:
                                ball_x_m, ball_y_m = ball_pos_m

                                # Calculate trajectory angle and speed if we
                                # have previous position
                                if prev_ball_pos_m is not None and prev_ball_frame is not None:
                                    # Calculate time difference
                                    time_diff = (
                                        frame_data['frame_num'] - prev_ball_frame) / fps if fps > 0 else 0

                                    if time_diff > 0:
                                        # Calculate trajectory angle
                                        trajectory_angle = calculate_trajectory_angle(
                                            prev_ball_pos_m, ball_pos_m)

                                        # Calculate speed
                                        ball_speed_mps = calculate_trajectory_speed(
                                            prev_ball_pos_m, ball_pos_m, time_diff)

                                # Update previous position for next frame
                                prev_ball_pos_m = ball_pos_m
                                prev_ball_frame = frame_data['frame_num']
                        else:
                            # Ball not detected - reset trajectory tracking if ball has been missing for too long
                            # (Allow brief gaps but reset after 1 second of missing ball)
                            if prev_ball_frame is not None:
                                frames_since_last_ball = frame_data['frame_num'] - \
                                    prev_ball_frame
                                if frames_since_last_ball > fps:  # More than 1 second
                                    prev_ball_pos_m = None
                                    prev_ball_frame = None

                        # Export to CSV
                        if export_csv and csv_writer is not None:
                            current_frame = frame_data.get('frame_num', 0)
                            # DIAGNOSTIC: Always log first few frames and then every 1000 frames to debug CSV export issues
                            should_log_diagnostic = (current_frame <= 10) or (current_frame % 1000 == 0)
                            if len(player_centers) == 0 and should_log_diagnostic:
                                # Log diagnostic info
                                detections_count = len(detections.xyxy) if detections is not None and hasattr(detections, 'xyxy') and detections.xyxy is not None else 0
                                # Count only valid (non-None, non--1) tracker IDs
                                if detections is not None and detections.tracker_id is not None:
                                    tracker_ids_list = list(detections.tracker_id)
                                    tracker_ids_count = sum(1 for tid in tracker_ids_list if tid is not None and tid != -1)
                                    tracker_ids_none_count = sum(1 for tid in tracker_ids_list if tid is None)
                                    tracker_ids_neg1_count = sum(1 for tid in tracker_ids_list if tid == -1)
                                else:
                                    tracker_ids_count = 0
                                    tracker_ids_none_count = 0
                                    tracker_ids_neg1_count = 0
                                detections_has_tracker_id = detections is not None and detections.tracker_id is not None
                                if tracker_ids_none_count > 0 or tracker_ids_neg1_count > 0:
                                    print(f"  âš  CSV Export: player_centers is empty at frame {current_frame} (detections: {detections_count}, valid tracks: {tracker_ids_count}, None IDs: {tracker_ids_none_count}, -1 IDs: {tracker_ids_neg1_count}, tracker_id array exists: {detections_has_tracker_id})")
                                else:
                                    print(f"  âš  CSV Export: player_centers is empty at frame {current_frame} (detections: {detections_count}, valid tracks: {tracker_ids_count}, tracker_id array exists: {detections_has_tracker_id})")
                            elif len(player_centers) > 0 and (current_frame <= 10 or current_frame % 1000 == 0):
                                # Log when player_centers IS populated (to confirm it's working)
                                print(f"  âœ“ CSV Export: player_centers has {len(player_centers)} players at frame {current_frame}")
                            
                            if player_centers:
                                csv_export_stats['frames_with_players'] += 1
                                for player_id, (px,
                                                py) in player_centers.items():
                                    csv_export_stats['total_player_rows'] += 1
                                    conf = 0.0
                                    
                                    # Get player name from player_names dictionary
                                    player_id_str = str(player_id)
                                    player_name = player_names.get(player_id_str, '') if 'player_names' in locals() else ''
                                    
                                    # ðŸŽ¯ IDENTITY TRACKER: Fallback to identity tracker if player_names is empty
                                    if not player_name and identity_tracker is not None:
                                        identity = identity_tracker.get_identity(player_id)
                                        if identity:
                                            player_name, _, _ = identity
                                            # Sync player_names with identity tracker
                                            player_names[player_id_str] = player_name
                                            if current_frame_num % 100 == 0:
                                                print(f"  ðŸ”„ CSV SYNC: Track #{player_id} â†’ '{player_name}' (from identity tracker)")
                                    
                                    # Get team from track_to_team_global
                                    player_team = track_to_team_global.get(player_id, '') if 'track_to_team_global' in locals() else ''
                                    
                                    # Get bbox from detections if available
                                    bbox_x1, bbox_y1, bbox_x2, bbox_y2 = '', '', '', ''
                                    bbox_valid = False
                                    if detections is not None and detections.tracker_id is not None:
                                        for det_idx, tid in enumerate(detections.tracker_id):
                                            if tid == player_id and det_idx < len(detections.xyxy):
                                                bbox = detections.xyxy[det_idx]
                                                bbox_x1, bbox_y1, bbox_x2, bbox_y2 = float(bbox[0]), float(bbox[1]), float(bbox[2]), float(bbox[3])
                                                
                                                # FILTER: Skip off-frame detections (bbox outside frame bounds)
                                                # These are often false positives in grass/off-screen areas
                                                # Check if bbox is significantly outside frame bounds
                                                frame_margin = 50  # Allow 50px margin outside frame
                                                if (bbox_x1 < -frame_margin or bbox_x2 > width + frame_margin or 
                                                    bbox_y1 < -frame_margin or bbox_y2 > height + frame_margin):
                                                    # Bbox is off-frame - skip this row
                                                    if current_frame % 100 == 0:
                                                        print(f"  ðŸš« FILTERED: Track #{player_id} bbox off-frame at frame {current_frame} (bbox: [{bbox_x1:.1f}, {bbox_y1:.1f}, {bbox_x2:.1f}, {bbox_y2:.1f}], frame: {width}x{height})")
                                                    continue  # Skip writing this row
                                                
                                                bbox_valid = True
                                                break
                                    
                                    # If bbox is invalid/off-frame, skip this row
                                    if not bbox_valid and bbox_x1 == '':
                                        continue  # No valid bbox found - skip row
                                    
                                    # Check if this is an anchor frame for this player
                                    is_anchor = False
                                    if 'anchor_frames' in locals() and anchor_frames is not None:
                                        current_frame_num = frame_data.get('frame_num', 0)
                                        if current_frame_num in anchor_frames:
                                            for anchor in anchor_frames[current_frame_num]:
                                                # Check if this anchor matches the current player (by track_id or player_name)
                                                anchor_track_id = anchor.get('track_id')
                                                anchor_player_name = anchor.get('player_name', '')
                                                if (anchor_track_id is not None and anchor_track_id == player_id) or \
                                                   (anchor_player_name and anchor_player_name == player_name):
                                                    is_anchor = True
                                                    break
                                    
                                    # Get analytics for this player (keyed by player_name, not track_id)
                                    analytics = player_analytics.get(player_name, {})
                                    player_x_m = analytics.get('x_m')
                                    player_y_m = analytics.get('y_m')
                                    player_speed_mps = analytics.get('speed_mps')
                                    player_acceleration_mps2 = analytics.get('acceleration_mps2')
                                    player_movement_angle = analytics.get('movement_angle')
                                    player_distance_to_ball = analytics.get('distance_to_ball')
                                    distance_traveled_m = analytics.get('distance_traveled_m', 0.0)
                                    max_speed_mps = analytics.get('max_speed_mps', 0.0)
                                    sprint_count = analytics.get('sprint_count', 0)
                                    possession_time_s = analytics.get('possession_time_s', 0.0)
                                    distance_from_center_m = analytics.get('distance_from_center_m')
                                    distance_from_goal_m = analytics.get('distance_from_goal_m')
                                    field_zone = analytics.get('field_zone', '')
                                    field_position_x_pct = analytics.get('field_position_x_pct')
                                    field_position_y_pct = analytics.get('field_position_y_pct')
                                    direction_changes = analytics.get('direction_changes', 0)
                                    avg_speed_mps = analytics.get('avg_speed_mps')
                                    distance_walking_m = analytics.get('distance_walking_m', 0.0)
                                    distance_jogging_m = analytics.get('distance_jogging_m', 0.0)
                                    distance_running_m = analytics.get('distance_running_m', 0.0)
                                    distance_sprinting_m = analytics.get('distance_sprinting_m', 0.0)
                                    time_stationary_s = analytics.get('time_stationary_s', 0.0)
                                    acceleration_events = analytics.get('acceleration_events', 0)
                                    nearest_teammate_dist_m = analytics.get('nearest_teammate_dist_m')
                                    nearest_opponent_dist_m = analytics.get('nearest_opponent_dist_m')
                                    
                                    # Apply unit conversions if requested
                                    if use_imperial_units:
                                        ball_x_m = meters_to_feet(ball_x_m) if ball_x_m is not None else None
                                        ball_y_m = meters_to_feet(ball_y_m) if ball_y_m is not None else None
                                        ball_speed_mps = mps_to_mph(ball_speed_mps) if ball_speed_mps is not None else None
                                        player_x_m = meters_to_feet(player_x_m) if player_x_m is not None else None
                                        player_y_m = meters_to_feet(player_y_m) if player_y_m is not None else None
                                        player_speed_mps = mps_to_mph(player_speed_mps) if player_speed_mps is not None else None
                                        player_acceleration_mps2 = mps2_to_fts2(player_acceleration_mps2) if player_acceleration_mps2 is not None else None
                                        distance_traveled_m = meters_to_feet(distance_traveled_m)
                                        max_speed_mps = mps_to_mph(max_speed_mps)
                                        distance_from_center_m = meters_to_feet(distance_from_center_m) if distance_from_center_m is not None else None
                                        distance_from_goal_m = meters_to_feet(distance_from_goal_m) if distance_from_goal_m is not None else None
                                        avg_speed_mps = mps_to_mph(avg_speed_mps) if avg_speed_mps is not None else None
                                        distance_walking_m = meters_to_feet(distance_walking_m)
                                        distance_jogging_m = meters_to_feet(distance_jogging_m)
                                        distance_running_m = meters_to_feet(distance_running_m)
                                        distance_sprinting_m = meters_to_feet(distance_sprinting_m)
                                        nearest_teammate_dist_m = meters_to_feet(nearest_teammate_dist_m) if nearest_teammate_dist_m is not None else None
                                        nearest_opponent_dist_m = meters_to_feet(nearest_opponent_dist_m) if nearest_opponent_dist_m is not None else None
                                    
                                    try:
                                        csv_writer.writerow([
                                            frame_data['frame_num'], frame_data['timestamp'],
                                            frame_data['ball_center'][0] if frame_data['ball_center'] else '',
                                            frame_data['ball_center'][1] if frame_data['ball_center'] else '',
                                            frame_data['ball_detected'],
                                            ball_x_m if ball_x_m is not None else '',
                                            ball_y_m if ball_y_m is not None else '',
                                            trajectory_angle if trajectory_angle is not None else '',
                                            ball_speed_mps if ball_speed_mps is not None else '',
                                            player_id, player_name, px, py,
                                            player_x_m if player_x_m is not None else '',
                                            player_y_m if player_y_m is not None else '',
                                            player_speed_mps if player_speed_mps is not None else '',
                                            player_acceleration_mps2 if player_acceleration_mps2 is not None else '',
                                            player_movement_angle if player_movement_angle is not None else '',
                                            player_distance_to_ball if player_distance_to_ball is not None else '',
                                            distance_traveled_m,
                                            max_speed_mps,
                                            sprint_count,
                                            possession_time_s,
                                            distance_from_center_m if distance_from_center_m is not None else '',
                                            distance_from_goal_m if distance_from_goal_m is not None else '',
                                            field_zone,
                                            field_position_x_pct if field_position_x_pct is not None else '',
                                            field_position_y_pct if field_position_y_pct is not None else '',
                                            direction_changes,
                                            avg_speed_mps if avg_speed_mps is not None else '',
                                            distance_walking_m,
                                            distance_jogging_m,
                                        distance_running_m,
                                        distance_sprinting_m,
                                        time_stationary_s,
                                        acceleration_events,
                                        nearest_teammate_dist_m if nearest_teammate_dist_m is not None else '',
                                        nearest_opponent_dist_m if nearest_opponent_dist_m is not None else '',
                                        conf,
                                        possession_player_id if possession_player_id == player_id else '',
                                        player_team,
                                        1 if is_anchor else 0,  # is_anchor: 1 if anchor frame, 0 otherwise
                                        bbox_x1 if bbox_x1 != '' else '',
                                        bbox_y1 if bbox_y1 != '' else '',
                                        bbox_x2 if bbox_x2 != '' else '',
                                        bbox_y2 if bbox_y2 != '' else ''
                                    ])
                                    
                                        # Periodic flush to prevent buffer buildup (every 100 frames)
                                        if frame_data['frame_num'] % 100 == 0 and csv_file:
                                            csv_file.flush()
                                            os.fsync(csv_file.fileno())  # Force write to disk
                                        
                                    except OSError as e:
                                        if e.errno == 28:  # No space left on device
                                            # Get disk usage info for better error message
                                            try:
                                                csv_drive = os.path.splitdrive(csv_filename)[0] + '\\'
                                                disk_usage = shutil.disk_usage(csv_drive)
                                                free_gb = disk_usage.free / (1024**3)
                                                total_gb = disk_usage.total / (1024**3)
                                                used_gb = disk_usage.used / (1024**3)
                                                
                                                print(f"\nâŒ DISK SPACE ERROR at frame {frame_data.get('frame_num', 'unknown')}")
                                                print(f"   Drive: {csv_drive}")
                                                print(f"   Free space: {free_gb:.2f}GB / {total_gb:.2f}GB total ({used_gb:.2f}GB used)")
                                                print(f"   CSV file location: {csv_filename}")
                                                
                                                # Check temp directory too
                                                temp_dir = os.environ.get('TEMP') or os.environ.get('TMP') or 'C:\\Windows\\Temp'
                                                try:
                                                    temp_drive = os.path.splitdrive(temp_dir)[0] + '\\'
                                                    if temp_drive != csv_drive:
                                                        temp_usage = shutil.disk_usage(temp_drive)
                                                        temp_free_gb = temp_usage.free / (1024**3)
                                                        print(f"   Temp directory drive ({temp_drive}): {temp_free_gb:.2f}GB free")
                                                        if temp_free_gb < 0.5:
                                                            print(f"   âš  WARNING: Temp directory drive is also low on space!")
                                                except:
                                                    pass
                                                
                                                print(f"\n   Solutions:")
                                                print(f"   1. Free up space on {csv_drive} (need at least 1-2GB)")
                                                print(f"   2. Change output location to a drive with more space")
                                                print(f"   3. Clean Windows temp files: %TEMP% and %TMP%")
                                                print(f"   4. Disable CSV export if not needed")
                                                
                                            except Exception as disk_check_error:
                                                print(f"   Could not check disk space: {disk_check_error}")
                                            
                                            # Close CSV file gracefully
                                            if csv_file:
                                                try:
                                                    csv_file.close()
                                                except:
                                                    pass
                                            
                                            raise  # Re-raise to stop processing
                                        else:
                                            raise  # Re-raise other OSErrors
                            else:
                                csv_export_stats['frames_with_empty_centers'] += 1
                                try:
                                    csv_writer.writerow([
                                        frame_data['frame_num'], frame_data['timestamp'],
                                        frame_data['ball_center'][0] if frame_data['ball_center'] else '',
                                        frame_data['ball_center'][1] if frame_data['ball_center'] else '',
                                        frame_data['ball_detected'],
                                        ball_x_m if ball_x_m is not None else '',
                                        ball_y_m if ball_y_m is not None else '',
                                        trajectory_angle if trajectory_angle is not None else '',
                                        ball_speed_mps if ball_speed_mps is not None else '',
                                        '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''
                                    ])
                                    
                                    # Periodic flush to prevent buffer buildup (every 100 frames)
                                    if frame_data['frame_num'] % 100 == 0 and csv_file:
                                        csv_file.flush()
                                        os.fsync(csv_file.fileno())  # Force write to disk
                                        
                                except OSError as e:
                                    if e.errno == 28:  # No space left on device
                                        # Get disk usage info for better error message
                                        try:
                                            csv_drive = os.path.splitdrive(csv_filename)[0] + '\\'
                                            disk_usage = shutil.disk_usage(csv_drive)
                                            free_gb = disk_usage.free / (1024**3)
                                            total_gb = disk_usage.total / (1024**3)
                                            used_gb = disk_usage.used / (1024**3)
                                            
                                            print(f"\nâŒ DISK SPACE ERROR at frame {frame_data.get('frame_num', 'unknown')}")
                                            print(f"   Drive: {csv_drive}")
                                            print(f"   Free space: {free_gb:.2f}GB / {total_gb:.2f}GB total ({used_gb:.2f}GB used)")
                                            print(f"   CSV file location: {csv_filename}")
                                            
                                            # Check temp directory too
                                            temp_dir = os.environ.get('TEMP') or os.environ.get('TMP') or 'C:\\Windows\\Temp'
                                            try:
                                                temp_drive = os.path.splitdrive(temp_dir)[0] + '\\'
                                                if temp_drive != csv_drive:
                                                    temp_usage = shutil.disk_usage(temp_drive)
                                                    temp_free_gb = temp_usage.free / (1024**3)
                                                    print(f"   Temp directory drive ({temp_drive}): {temp_free_gb:.2f}GB free")
                                                    if temp_free_gb < 0.5:
                                                        print(f"   âš  WARNING: Temp directory drive is also low on space!")
                                            except:
                                                pass
                                            
                                            print(f"\n   Solutions:")
                                            print(f"   1. Free up space on {csv_drive} (need at least 1-2GB)")
                                            print(f"   2. Change output location to a drive with more space")
                                            print(f"   3. Clean Windows temp files: %TEMP% and %TMP%")
                                            print(f"   4. Disable CSV export if not needed")
                                            
                                        except Exception as disk_check_error:
                                            print(f"   Could not check disk space: {disk_check_error}")
                                        
                                        # Close CSV file gracefully
                                        if csv_file:
                                            try:
                                                csv_file.close()
                                            except:
                                                pass
                                        
                                        raise  # Re-raise to stop processing
                                    else:
                                        raise  # Re-raise other OSErrors

                        # CRITICAL FIX: Update track state for smooth interpolation/extrapolation
                        # This keeps boxes visible even on unprocessed frames,
                        # preventing blinking
                        if detections is not None and len(detections) > 0:
                            tracker_ids = detections.tracker_id if detections.tracker_id is not None else [
                                None] * len(detections.xyxy)
                            
                            # VELOCITY CONSTRAINTS: Track corrections for logging
                            velocity_corrections_count = 0
                            
                            for i, (track_id, xyxy) in enumerate(
                                    zip(tracker_ids, detections.xyxy)):
                                if track_id is not None:
                                    # VELOCITY CONSTRAINTS: Validate and correct velocity before updating track state
                                    prev_xyxy = None
                                    prev_frame = None
                                    if track_id in track_state:
                                        prev_xyxy = track_state[track_id]['xyxy']
                                        prev_frame = track_state[track_id]['frame']
                                    
                                    # Validate velocity and correct if needed
                                    corrected_xyxy, velocity_exceeded = validate_track_velocity(
                                        track_id, xyxy, prev_xyxy, prev_frame, 
                                        frame_data['frame_num'], fps,
                                        max_velocity_mps=10.0,
                                        pixels_per_meter=pixels_per_meter,
                                        enable_velocity_constraints=enable_velocity_constraints
                                    )
                                    
                                    if velocity_exceeded:
                                        velocity_corrections_count += 1
                                        # Use corrected position
                                        xyxy = corrected_xyxy
                                    
                                    # Calculate velocity from previous position
                                    # if available
                                    velocity = [0.0, 0.0]
                                    if prev_xyxy is not None and prev_frame is not None:
                                        frame_diff = frame_data['frame_num'] - prev_frame
                                        if frame_diff > 0:
                                            # Calculate velocity (pixels per frame)
                                            prev_center_x = (prev_xyxy[0] + prev_xyxy[2]) / 2
                                            prev_center_y = (prev_xyxy[1] + prev_xyxy[3]) / 2
                                            curr_center_x = (xyxy[0] + xyxy[2]) / 2
                                            curr_center_y = (xyxy[1] + xyxy[3]) / 2
                                            velocity[0] = (curr_center_x - prev_center_x) / frame_diff
                                            velocity[1] = (curr_center_y - prev_center_y) / frame_diff

                                    # Update track state with smoothed position (and corrected position if velocity was exceeded)
                                    track_state[track_id] = {
                                        'xyxy': xyxy.copy(),
                                        'frame': frame_data['frame_num'],
                                        'velocity': velocity
                                    }
                            
                            # Log velocity corrections periodically
                            if velocity_corrections_count > 0 and frame_data.get('frame_num', 0) % 100 == 0:
                                # PERFORMANCE: Reduced velocity constraint logging (was every frame with corrections)
                                if frame_data.get('frame_num', 0) % 500 == 0:  # Log every 500 frames
                                    print(f"âš¡ Velocity constraints: Corrected {velocity_corrections_count} impossible jump(s) at frame {frame_data.get('frame_num', 'unknown')}")

                        # Store detections for this processed frame (don't
                        # store annotated_frame to save memory)
                        last_detections = detections
                        # CRITICAL FIX: Don't store annotated_frame reference - it's from batch processing and may be reused
                        # We'll recreate annotations when writing output to
                        # avoid frame reference issues
                        last_annotated_frame = None  # Don't store reference to avoid frame reuse issues
                        # Store raw YOLO detections if available
                        stored_raw_yolo_frame = None
                        if 'raw_yolo_detections' in locals() and raw_yolo_detections is not None:
                            stored_raw_yolo_frame = raw_yolo_detections
                        
                        frame_detections[frame_data['frame_num']] = {
                            'raw_yolo_detections': stored_raw_yolo_frame,  # Store raw YOLO detections for visualization
                            'detections': detections,
                            'possession_player_id': possession_player_id  # Store possession info for drawing
                            # Don't store annotated_frame - we'll recreate
                            # annotations when needed to save memory
                        }

                        # CRITICAL: Clean up old frame_detections to prevent memory leak
                        # Keep frames within a window for output writing (larger cache = better pipelining)
                        # Detections are small (just bboxes, IDs, confidences),
                        # so 2-3 seconds is fine
                        # Keep 2.5 seconds of frames for better pipelining
                        max_frames_to_keep = int(fps * 2.5)
                        frames_to_cleanup = [f for f in frame_detections.keys()
                                             if frame_data['frame_num'] - f > max_frames_to_keep]
                        for f in frames_to_cleanup:
                            # Explicitly delete to free memory
                            if f in frame_detections:
                                del frame_detections[f]

                        # Note: Don't call gc.collect() here - let the batch-level GC handle it
                        # Calling it too frequently can actually slow things
                        # down

                        # Update progress with degradation detection
                        if frame_data['frame_num'] % 100 == 0:
                            progress = (
                                frame_data['frame_num'] / total_frames) * 100
                            elapsed = time.time() - start_time
                            if elapsed > 0:
                                rate = frame_data['frame_num'] / elapsed
                                eta = (
                                    total_frames - frame_data['frame_num']) / rate if rate > 0 else 0

                                # Track performance degradation
                                if not hasattr(
                                        combined_analysis_optimized, 'last_rate'):
                                    combined_analysis_optimized.last_rate = rate
                                    combined_analysis_optimized.degradation_warnings = 0

                                # Detect severe degradation (>50% slowdown)
                                if rate < combined_analysis_optimized.last_rate * \
                                        0.5 and frame_data['frame_num'] > 500:
                                    combined_analysis_optimized.degradation_warnings += 1
                                    if combined_analysis_optimized.degradation_warnings <= 3:
                                        print(f"âš  WARNING: Performance degraded {((1 - rate / combined_analysis_optimized.last_rate) * 100):.0f}% "
                                              f"({combined_analysis_optimized.last_rate:.1f} â†’ {rate:.1f} fps)")
                                        if combined_analysis_optimized.degradation_warnings == 3:
                                            print(
                                                f"âš  CRITICAL: Severe degradation detected. Consider:")
                                            print(
                                                f"   - Reducing batch size: --batch-size 4")
                                            print(
                                                f"   - Processing every 2nd frame: --process-every-nth 2")
                                            print(
                                                f"   - Lower YOLO resolution: --yolo-resolution 720p")

                                combined_analysis_optimized.last_rate = rate

                                # Add memory diagnostics
                                frame_detections_size = len(frame_detections)
                                memory_mb = sys.getsizeof(
                                    frame_detections) / (1024 * 1024)

                                # Track Re-ID tracker size if available
                                reid_tracks = 0
                                if reid_tracker is not None and hasattr(
                                        reid_tracker, 'track_features'):
                                    reid_tracks = len(
                                        reid_tracker.track_features)

                                # Track active tracking structures
                                active_kalman = len(
                                    enhanced_kalman_filters) if enhanced_kalman_filters else 0
                                active_ema = len(
                                    ema_smoothers) if ema_smoothers else 0

                                # Monitor GPU memory usage
                                gpu_mem_info = ""
                                if device == 'cuda' and torch.cuda.is_available():
                                    try:
                                        allocated = torch.cuda.memory_allocated(
                                            cuda_device_id) / 1e9
                                        reserved = torch.cuda.memory_reserved(
                                            cuda_device_id) / 1e9
                                        gpu_mem_info = f" | GPU: {
                                            allocated:.2f}/{
                                            reserved:.2f}GB"
                                    except:
                                        pass

                                # Performance trend indicator
                                trend = "ðŸ“ˆ" if rate >= combined_analysis_optimized.last_rate * \
                                    0.95 else "ðŸ“‰" if rate < combined_analysis_optimized.last_rate * 0.9 else "âž¡ï¸"

                                print(f"Progress: {progress:.1f}% ({frame_data['frame_num']}/{total_frames} frames) | "
                                      f"Rate: {
                                    rate:.1f} fps {trend} | ETA: {
                                    eta / 60:.1f} min | "
                                    f"Cache: {frame_detections_size} frames | "
                                    f"Re-ID: {reid_tracks} tracks | "
                                    f"Filters: K={active_kalman} E={active_ema}{gpu_mem_info}")

                                # PERFORMANCE: Reduced emergency optimization frequency and made it less aggressive
                                # Emergency optimizations (gc.collect()) can actually slow things down if called too often
                                if rate < 1.0 and frame_data['frame_num'] > 100 and frame_data['frame_num'] % 500 == 0:
                                    # Only run emergency optimizations every 500 frames to avoid making things worse
                                    print(
                                        f"âš  CRITICAL: Rate dropped below 1.0 fps. Applying emergency optimizations...")
                                    # Less aggressive cleanup - only one gc.collect() instead of two
                                    gc.collect()
                                    if device == 'cuda' and torch.cuda.is_available():
                                        torch.cuda.empty_cache()
                                    print(
                                        f"âœ“ Emergency cleanup completed. Continuing...")
                    except Exception as e:
                        # Catch any iteration or processing errors for this
                        # frame
                        import traceback
                        frame_num = frame_data.get(
                            'frame_num', 'unknown') if 'frame_data' in locals() else 'unknown'
                        print(f"âš  Error processing frame {frame_num}: {e}")
                        print(f"   Error type: {type(e).__name__}")
                        traceback.print_exc()
                        # Create empty detections to continue processing
                        try:
                            detections = sv.Detections.empty()
                        except:
                            # If empty() fails, create minimal detections
                            # object
                            detections = type('Detections', (), {
                                'xyxy': np.array([]).reshape(0, 4),
                                'tracker_id': np.array([]),
                                'confidence': np.array([])
                            })()
                        # Ensure attributes are valid
                        if not hasattr(
                                detections, 'xyxy') or detections.xyxy is None:
                            detections.xyxy = np.array([]).reshape(0, 4)
                        if not hasattr(
                                detections,
                                'tracker_id') or detections.tracker_id is None:
                            # CRITICAL: tracker_id should be None (not empty array) if no tracking IDs assigned yet
                            # Empty arrays (size 0) cause issues when applying
                            # masks - supervision library can't handle them
                            detections.tracker_id = None
                        if not hasattr(
                                detections,
                                'confidence') or detections.confidence is None:
                            detections.confidence = np.array([])
                        # Store empty detections to continue (don't store
                        # annotated_frame to save memory)
                        try:
                            # Store raw YOLO detections if available
                            stored_raw_yolo = None
                            if 'raw_yolo_detections' in locals() and raw_yolo_detections is not None:
                                stored_raw_yolo = raw_yolo_detections
                            
                            frame_detections[frame_num if isinstance(frame_num, int) else frame_count] = {
                                'raw_yolo_detections': stored_raw_yolo,  # Store raw YOLO detections for visualization
                                'detections': detections
                                # Don't store annotated_frame - we'll recreate
                                # annotations when needed
                            }
                        except:
                            pass  # If we can't store, just continue

                # Clear batch and force garbage collection to free memory
                # Get last frame number before clearing
                # CRITICAL FIX: Use the maximum frame number from the batch, not just the last one
                # This ensures we get the actual highest frame number processed
                if frame_data_queue:
                    last_frame_in_batch = max([fd.get('frame_num', frame_count) for fd in frame_data_queue])
                else:
                    last_frame_in_batch = frame_count

                # Explicitly delete all frame copies
                for f in frame_queue:
                    del f
                del frame_queue
                del frame_data_queue
                del results
                frame_queue = []
                frame_data_queue = []

                # Force aggressive garbage collection periodically to prevent memory buildup
                # Balance between memory management and performance
                # Full GC every 10 batches, light GC every 5 batches
                if last_frame_in_batch % (effective_batch_size * 10) == 0:
                    # Full aggressive GC
                    gc.collect()
                    gc.collect()  # Call twice to ensure cleanup
                    # Clear GPU cache to prevent memory fragmentation
                    if device == 'cuda' and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                elif last_frame_in_batch % (effective_batch_size * 5) == 0:
                    # Light GC every 5 batches
                    gc.collect()
                    # Light GPU cache clear
                    if device == 'cuda' and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                # Don't GC on every batch - too expensive

        # Write ALL frames to output (not just processed ones) to maintain correct playback speed
        # This ensures audio sync is maintained even when processing every Nth
        # frame
        if track_players_flag and model is not None:
            # Reload player names periodically to pick up edits made during analysis
            # This allows users to edit player_names.json and have changes take
            # effect without restarting
            if frame_count - \
                    last_player_names_reload >= 100:  # Reload every 100 frames (~4 seconds at 24fps)
                try:
                    # CRITICAL FIX: Preserve ALL existing names (don't let seed files overwrite analysis-assigned names)
                    # Analysis-assigned names from previous tracking data are
                    # more accurate than seed files
                    preserved_names = {}
                    for k, v in player_names.items():
                        # CRITICAL FIX: Handle case where v might be a list
                        if isinstance(v, list) and len(v) > 0:
                            v_str = str(v[0]).strip()
                        elif v:
                            v_str = str(v).strip()
                        else:
                            v_str = ""
                        if v_str:
                            preserved_names[k] = v_str

                    # CRITICAL FIX: Only reload from player_names.json, NOT from PlayerTagsSeed files
                    # Seed files should only be loaded once at startup, not during periodic reloads
                    # This prevents seed data from overwriting correctly
                    # assigned player names
                    new_player_names = load_player_names(
                        input_path, include_seed_files=False)

                    # CRITICAL FIX: Smart merge - only update names that are:
                    # 1. New (not in existing player_names), OR
                    # 2. Empty in existing player_names, OR
                    # 3. Explicitly edited in player_names.json (user wants to override)
                    # DO NOT overwrite names that were assigned during analysis
                    for track_id, name in new_player_names.items():
                        # CRITICAL FIX: Handle case where name might be a list
                        if isinstance(name, list) and len(name) > 0:
                            name_str = str(name[0]).strip()
                        elif name:
                            name_str = str(name).strip()
                        else:
                            name_str = ""
                        
                        if name_str:  # Only process non-empty names
                            # Only update if:
                            # - Track ID doesn't exist yet, OR
                            # - Existing name is empty/unset, OR
                            # - User explicitly wants to override (name in player_names.json but different)
                            if track_id not in preserved_names:
                                # New track ID - add it
                                player_names[track_id] = name_str
                            elif not preserved_names.get(track_id, "").strip():
                                # Existing but empty - fill it
                                player_names[track_id] = name_str
                            # Otherwise, keep the preserved name (don't
                            # overwrite analysis-assigned names)

                    last_player_names_reload = frame_count
                except Exception as e:
                    # If reload fails, continue with existing names (don't
                    # crash analysis)
                    pass

            # Clean up old frame_detections periodically to prevent memory leak
            if frame_count % 100 == 0:  # Every 100 frames
                # Keep 2.5 seconds of frames for better pipelining
                max_frames_to_keep = int(fps * 2.5)
                frames_to_cleanup = [f for f in frame_detections.keys()
                                     if frame_count - f > max_frames_to_keep]
                for f in frames_to_cleanup:
                    if f in frame_detections:
                        del frame_detections[f]
                # Only force GC if we removed a significant number of frames
                # GC is expensive, so only do it when necessary
                if len(frames_to_cleanup) > 50:  # Only if we removed many frames
                    gc.collect()

            # Determine which frame to write (with annotations if processed)
            # OPTIMIZATION: In watch-only mode without live viewer, skip expensive frame copying/annotation
            if watch_only and not show_live_viewer:
                # Skip frame annotation in watch-only mode without viewer (much faster)
                frame_to_write = None
            else:
                # CRITICAL FIX: Always create a copy to avoid modifying the original frame
                # This prevents overwriting issues and ensures smooth continuous drawing
                frame_to_write = frame.copy()  # CRITICAL: Copy to avoid modifying original frame
            detections = None
            

            # CRITICAL FIX: If this frame isn't processed yet, use the most recent processed frame's detections
            # This prevents blank frames between batches
            # OPTIMIZATION: Skip annotation work in watch-only mode without viewer
            if watch_only and not show_live_viewer:
                # Skip all annotation work - just process for learning
                use_frame = None
                detections = None
            elif frame_count in frame_detections:
                use_frame = frame_count
            elif len(frame_detections) > 0:
                # Use most recent processed frame
                use_frame = max([f for f in frame_detections.keys() if f <= frame_count], default=None)
                if use_frame is None and len(frame_detections) > 0:
                    # No frames processed yet, skip drawing
                    use_frame = None
            else:
                use_frame = None
                
            if use_frame is not None and use_frame in frame_detections and frame_to_write is not None:
                # This frame (or a recent frame) was processed - get detections
                det_data = frame_detections[use_frame]  # Use the most recent processed frame
                detections = det_data.get('detections')
                if detections is None:
                    detections = sv.Detections.empty()
                

                if yolo_resolution != "full" and len(detections) > 0:
                    # Annotations are already scaled in detections.xyxy, so draw directly
                    # Re-create labels and colors
                    # player_names already loaded and updated dynamically
                    colors = []
                    # CRITICAL FIX: Classify team for EACH player individually
                    # (not just first one)
                    tracker_ids_list = detections.tracker_id if detections.tracker_id is not None else [
                        None] * len(detections.xyxy)
                    for i, track_id in enumerate(tracker_ids_list):
                        if track_id is not None:
                            team = None
                            if viz_color_mode == "team" and team_colors:
                                # Classify team for THIS specific player (not
                                # all players)
                                if i < len(detections.xyxy):
                                    team = classify_player_team(
                                        frame, detections.xyxy[i], team_colors)
                            color = get_player_color(
                                track_id, team, viz_color_mode, team_colors)
                            colors.append(color)
                        else:
                            # Orange for untracked (easy to debug)
                            colors.append((0, 165, 255))

                    # Get possession info for this frame (if available)
                    possession_player_id = det_data.get(
                        'possession_player_id') if 'possession_player_id' in det_data else None

                    # Get track IDs for possession checking
                    tracker_ids = detections.tracker_id if detections.tracker_id is not None else [
                        None] * len(detections.xyxy)

                    # Get current visualization settings (dynamic if available)
                    current_viz_style = get_setting('viz_style', viz_style)
                    current_box_thickness = get_setting('box_thickness', box_thickness)
                    current_box_shrink_factor = get_setting('box_shrink_factor', box_shrink_factor)
                    current_box_color = get_setting('box_color', box_color)
                    current_ellipse_width = get_setting('ellipse_width', ellipse_width)
                    current_ellipse_height = get_setting('ellipse_height', ellipse_height)
                    current_ellipse_outline_thickness = get_setting('ellipse_outline_thickness', ellipse_outline_thickness)
                    current_player_viz_alpha = get_setting('player_viz_alpha', player_viz_alpha) if 'player_viz_alpha' in locals() else 255
                    # SEPARATE CONTROLS: Bounding boxes vs Circles at feet (user requested)
                    current_show_bounding_boxes = get_setting('show_bounding_boxes', show_bounding_boxes if 'show_bounding_boxes' in locals() else True)
                    current_show_circles_at_feet = get_setting('show_circles_at_feet', show_circles_at_feet if 'show_circles_at_feet' in locals() else True)
                    
                    # Draw annotations with ellipse at feet (Option C: Ellipse
                    # + triangle for ball possession)
                    for i, (xyxy, color) in enumerate(
                            zip(detections.xyxy, colors)):
                        x1, y1, x2, y2 = map(int, xyxy)
                        track_id = tracker_ids[i] if i < len(
                            tracker_ids) else None
                        
                        # TRACK JUMP HIGHLIGHT: Highlight the target track if we're jumping to it
                        if track_jump_target_id is not None and track_id == track_jump_target_id and track_jump_highlight_frames > 0:
                            # Use bright yellow/cyan for highlighting
                            highlight_color = (0, 255, 255)  # Cyan in BGR
                            # Draw thicker outline
                            cv2.rectangle(frame_to_write, (x1, y1), (x2, y2), highlight_color, 5)
                            # Add text label
                            cv2.putText(frame_to_write, f"TRACK #{track_id} - CONFIRM?", 
                                       (x1, y1 - 10), cv2.FONT_HERSHEY_BOLD, 1.0, highlight_color, 2)
                            track_jump_highlight_frames -= 1
                            if track_jump_highlight_frames <= 0:
                                track_jump_target_id = None
                                track_jump_target_frame = None

                        # CRITICAL FIX: Respect viz_style setting - "Boxes" should show boxes, not ellipses
                        # Ellipses are only for "circle" or "both" modes when foot_based_tracking is enabled
                        # USER REQUEST: Keep circles at feet (current
                        # position), but disable trailing circles (movement
                        # history)
                        
                        
                        # SEPARATE CONTROL: Draw team-colored circles at feet (user requested)
                        # Circles always use team colors (ignore custom box color)
                        if current_show_circles_at_feet:
                            # Draw ellipse at feet position
                            # Apply vertical offset (negative = above feet, positive = below feet)
                            current_feet_vertical_offset = get_setting('feet_marker_vertical_offset', 50)
                            foot_y = y2 + current_feet_vertical_offset  # Bottom of box = feet position
                            center_x = int((x1 + x2) / 2)

                            # Draw ellipse at feet position (horizontal orientation)
                            ellipse_center = (center_x, int(foot_y))
                            
                            # AUTO-SCALE: Make ellipses bigger for higher resolutions
                            # For 4K (3840x2160), scale up by ~2x so they're visible
                            scale_factor = max(1.0, yolo_width / 1920.0)  # Scale based on width
                            scaled_width = int(current_ellipse_width * scale_factor)
                            scaled_height = int(current_ellipse_height * scale_factor)
                            axes = (int(scaled_width / 2), int(scaled_height / 2))

                            # CRITICAL: Circles always use team colors (ignore custom box color)
                            # This ensures circles are always team-colored as user prefers
                            ellipse_color = blend_color_with_opacity(color, current_player_viz_alpha)
                            
                            # Get enhanced feet marker settings
                            current_feet_style = get_setting('feet_marker_style', 'circle')
                            current_feet_opacity = get_setting('feet_marker_opacity', 255)
                            current_feet_glow = get_setting('feet_marker_enable_glow', False)
                            current_feet_glow_intensity = get_setting('feet_marker_glow_intensity', 50)
                            current_feet_shadow = get_setting('feet_marker_enable_shadow', False)
                            current_feet_shadow_offset = get_setting('feet_marker_shadow_offset', 3)
                            current_feet_shadow_opacity = get_setting('feet_marker_shadow_opacity', 128)
                            current_feet_gradient = get_setting('feet_marker_enable_gradient', False)
                            current_feet_pulse = get_setting('feet_marker_enable_pulse', False)
                            current_feet_pulse_speed = get_setting('feet_marker_pulse_speed', 2.0)
                            current_feet_particles = get_setting('feet_marker_enable_particles', False)
                            current_feet_particle_count = get_setting('feet_marker_particle_count', 5)
                            
                            # Use enhanced rendering if any advanced features are enabled
                            if (current_feet_style != "circle" and current_feet_style != "ellipse") or \
                               current_feet_glow or current_feet_shadow or \
                               current_feet_gradient or current_feet_pulse or \
                               current_feet_particles or current_feet_opacity < 255:
                                draw_enhanced_feet_marker(
                                    frame_to_write,
                                    ellipse_center, axes,
                                    current_feet_style, ellipse_color, current_feet_opacity,
                                    current_feet_glow, current_feet_glow_intensity,
                                    current_feet_shadow, current_feet_shadow_offset, current_feet_shadow_opacity,
                                    current_feet_gradient, current_feet_pulse, current_feet_pulse_speed, frame_count,
                                    current_feet_particles, current_feet_particle_count,
                                    current_ellipse_outline_thickness, fps
                                )
                            else:
                                # Standard ellipse (legacy mode for performance)
                                cv2.ellipse(frame_to_write, ellipse_center, axes, 0, 0, 360, ellipse_color, -1)
                                # Only draw border if thickness > 0 (0 means no border)
                                if current_ellipse_outline_thickness > 0:
                                    cv2.ellipse(frame_to_write, ellipse_center, axes, 0, 0, 360, (255, 255, 255), current_ellipse_outline_thickness)

                            # Draw triangle indicator if player has ball
                            if show_ball_possession and track_id is not None and possession_player_id is not None:
                                if int(track_id) == int(possession_player_id):
                                    # Draw upward triangle above ellipse (ball
                                    # possession indicator)
                                    # Proportional to ellipse
                                    triangle_size = max(
                                        8, int(ellipse_height * 0.8))
                                    triangle_top = (center_x, int(
                                        foot_y) - int(ellipse_height / 2) - triangle_size)
                                    triangle_left = (
                                        center_x - triangle_size, int(foot_y) - int(ellipse_height / 2))
                                    triangle_right = (
                                        center_x + triangle_size, int(foot_y) - int(ellipse_height / 2))
                                    triangle_points = np.array(
                                        [triangle_top, triangle_left, triangle_right], np.int32)
                                    # Draw filled triangle (blue for ball
                                    # possession)
                                    cv2.fillPoly(
                                        img=frame_to_write, pts=[triangle_points], color=(
                                            255, 0, 0))  # Blue in BGR
                                    # Draw outline
                                    cv2.polylines(
                                        img=frame_to_write, pts=[triangle_points], isClosed=True, color=(
                                            255, 255, 255), thickness=1)

                        # CRITICAL FIX: "Boxes" mode should always show boxes,
                        # regardless of foot_based_tracking
                        
                        
                        # Calculate center and dimensions for all shapes
                        center_x = int((x1 + x2) / 2)
                        center_y = int((y1 + y2) / 2)
                        box_width = x2 - x1
                        box_height = y2 - y1
                        
                        # Apply shrink factor if needed
                        if current_box_shrink_factor > 0:
                            new_width = box_width * (1 - current_box_shrink_factor * 2)
                            new_height = box_height * (1 - current_box_shrink_factor * 2)
                            tight_x1 = int(center_x - new_width / 2)
                            tight_y1 = int(center_y - new_height / 2)
                            tight_x2 = int(center_x + new_width / 2)
                            tight_y2 = int(center_y + new_height / 2)
                        else:
                            tight_x1, tight_y1, tight_x2, tight_y2 = x1, y1, x2, y2
                        
                        # SEPARATE CONTROL: Draw bounding boxes (user requested)
                        # Boxes use custom color if enabled, otherwise team colors
                        if current_show_bounding_boxes:
                            # Draw bounding box (use custom color if specified, otherwise use team color)
                            box_color_to_use = current_box_color if current_box_color else color
                            cv2.rectangle(frame_to_write, (tight_x1, tight_y1), (tight_x2, tight_y2), box_color_to_use, current_box_thickness)
                            
                            # DEBUG: Draw XY coordinate overlay to help diagnose bbox position issues
                            # Show coordinates at top-left of bbox
                            coord_text = f"({tight_x1},{tight_y1})-({tight_x2},{tight_y2})"
                            center_x = (tight_x1 + tight_x2) // 2
                            center_y = (tight_y1 + tight_y2) // 2
                            foot_y = tight_y2
                            coord_text2 = f"C:({center_x},{center_y}) F:{foot_y}"
                            
                            # Draw coordinate text with background for readability
                            font = cv2.FONT_HERSHEY_SIMPLEX
                            font_scale = 0.4
                            thickness = 1
                            
                            # Calculate text size
                            (text_w1, text_h1), _ = cv2.getTextSize(coord_text, font, font_scale, thickness)
                            (text_w2, text_h2), _ = cv2.getTextSize(coord_text2, font, font_scale, thickness)
                            
                            # Draw background rectangles
                            cv2.rectangle(frame_to_write, (tight_x1, tight_y1 - text_h1 - text_h2 - 6), 
                                        (tight_x1 + max(text_w1, text_w2) + 4, tight_y1), (0, 0, 0), -1)
                            
                            # Draw coordinate text
                            cv2.putText(frame_to_write, coord_text, (tight_x1 + 2, tight_y1 - text_h2 - 4), 
                                      font, font_scale, (0, 255, 255), thickness)  # Cyan
                            cv2.putText(frame_to_write, coord_text2, (tight_x1 + 2, tight_y1 - 2), 
                                      font, font_scale, (0, 255, 255), thickness)  # Cyan
                            
                            # Draw track ID if available
                            if track_id is not None:
                                track_text = f"ID:{track_id}"
                                (text_w3, text_h3), _ = cv2.getTextSize(track_text, font, font_scale, thickness)
                                cv2.rectangle(frame_to_write, (tight_x1, tight_y2 + 2), 
                                            (tight_x1 + text_w3 + 4, tight_y2 + text_h3 + 4), (0, 0, 0), -1)
                                cv2.putText(frame_to_write, track_text, (tight_x1 + 2, tight_y2 + text_h3 + 2), 
                                          font, font_scale, (255, 255, 0), thickness)  # Yellow
                        
                        # Legacy style support (for compatibility)
                        elif current_viz_style in ["box", "both"]:
                            # DEBUG: Log when legacy style is used
                            if frame_count % 1000 == 0:  # Log every 1000 frames to avoid spam
                                print(f"  ðŸ”§ LEGACY STYLE: Using legacy viz_style='{current_viz_style}' for bounding boxes (frame {frame_count})")
                            # Draw bounding box (use custom color if specified, otherwise use team color)
                            box_color_to_use = current_box_color if current_box_color else color
                            cv2.rectangle(frame_to_write, (tight_x1, tight_y1), (tight_x2, tight_y2), box_color_to_use, current_box_thickness)
                        
                        if current_viz_style == "star":
                            # Draw 5-pointed star
                            outer_radius = min(box_width, box_height) // 2
                            inner_radius = outer_radius // 2
                            points = []
                            for i in range(10):
                                angle = i * np.pi / 5 - np.pi / 2
                                r = outer_radius if i % 2 == 0 else inner_radius
                                px = int(center_x + r * np.cos(angle))
                                py = int(center_y + r * np.sin(angle))
                                points.append([px, py])
                            pts = np.array(points, np.int32)
                            cv2.fillPoly(frame_to_write, [pts], color)
                            cv2.polylines(frame_to_write, [pts], True, (255, 255, 255), current_box_thickness)
                        
                        if current_viz_style == "diamond":
                            # Draw diamond (rotated square)
                            points = np.array([
                                [center_x, tight_y1],  # Top
                                [tight_x2, center_y],  # Right
                                [center_x, tight_y2],  # Bottom
                                [tight_x1, center_y]   # Left
                            ], np.int32)
                            cv2.fillPoly(frame_to_write, [points], color)
                            cv2.polylines(frame_to_write, [points], True, (255, 255, 255), current_box_thickness)
                        
                        if current_viz_style == "hexagon":
                            # Draw hexagon
                            radius = min(box_width, box_height) // 2
                            points = []
                            for i in range(6):
                                angle = i * np.pi / 3
                                px = int(center_x + radius * np.cos(angle))
                                py = int(center_y + radius * np.sin(angle))
                                points.append([px, py])
                            pts = np.array(points, np.int32)
                            cv2.fillPoly(frame_to_write, [pts], color)
                            cv2.polylines(frame_to_write, [pts], True, (255, 255, 255), current_box_thickness)
                        
                        if current_viz_style == "arrow":
                            # Draw arrow pointing up
                            arrow_size = min(box_width, box_height) // 2
                            points = np.array([
                                [center_x, tight_y1],  # Top point
                                [center_x - arrow_size // 2, tight_y1 + arrow_size // 2],  # Left
                                [center_x - arrow_size // 4, tight_y1 + arrow_size // 2],  # Left inner
                                [center_x - arrow_size // 4, tight_y2],  # Bottom left
                                [center_x + arrow_size // 4, tight_y2],  # Bottom right
                                [center_x + arrow_size // 4, tight_y1 + arrow_size // 2],  # Right inner
                                [center_x + arrow_size // 2, tight_y1 + arrow_size // 2]   # Right
                            ], np.int32)
                            cv2.fillPoly(frame_to_write, [points], color)
                            cv2.polylines(frame_to_write, [points], True, (255, 255, 255), current_box_thickness)
                        
                        # Draw ball possession indicator (for all shapes)
                        if show_ball_possession and track_id is not None and possession_player_id is not None:
                            if int(track_id) == int(possession_player_id):
                                # Draw upward triangle above shape (ball possession indicator)
                                triangle_size = 8
                                triangle_top = (center_x, tight_y1 - triangle_size)
                                triangle_left = (center_x - triangle_size, tight_y1)
                                triangle_right = (center_x + triangle_size, tight_y1)
                                triangle_points = np.array([triangle_top, triangle_left, triangle_right], np.int32)
                                # Draw filled triangle (blue for ball possession)
                                cv2.fillPoly(frame_to_write, pts=[triangle_points], color=(255, 0, 0))  # Blue in BGR
                                # Draw outline
                                cv2.polylines(frame_to_write, pts=[triangle_points], isClosed=True, color=(255, 255, 255), thickness=1)

                        # Draw circles if circle mode is enabled and
                        # foot_based_tracking is off
                        if current_viz_style in [
                                "circle", "both"] and not foot_based_tracking:
                            cx, cy = int((x1 + x2) / 2), int((y1 + y2) / 2)
                            cv2.circle(frame_to_write, (cx, cy), 30, color, 2)
                else:
                    # Full resolution - annotations were done on full res frame
                    
                    # CRITICAL FIX: Use copy to avoid modifying original frame
                    # frame_to_write already set to frame.copy() above, so ball annotations are included
                    # Draw player annotations from detections
                    # detections already assigned above
                    if detections is not None and len(detections) > 0:
                        # player_names already loaded and updated dynamically
                        colors = []
                        labels = []
                        # Handle case where tracker_id is None
                        tracker_ids = detections.tracker_id if detections.tracker_id is not None else [
                            None] * len(detections.xyxy)
                        # CRITICAL FIX: Classify team for EACH player
                        # individually (not just first one)
                        # Get current label settings (dynamic if available)
                        current_label_type = get_setting('label_type', label_type)
                        current_label_custom_text = get_setting('label_custom_text', label_custom_text)
                        
                        for i, (track_id, conf) in enumerate(
                                zip(tracker_ids, detections.confidence)):
                            if track_id is not None:
                                pid_str = str(int(track_id))
                                player_name = player_names.get(
                                    pid_str, f"#{track_id}")
                                
                                team = None
                                if viz_color_mode == "team" and team_colors:
                                    # Classify team for THIS specific player
                                    # (not all players)
                                    if i < len(detections.xyxy):
                                        team = classify_player_team(
                                            frame, detections.xyxy[i], team_colors)
                                        # Diagnostic logging removed to reduce console spam
                                else:
                                    # Only warn if team color mode is enabled but team colors aren't available
                                    # Don't warn if color mode is intentionally set to something other than 'team'
                                    if viz_color_mode == "team" and not team_colors:
                                        # Only print once per analysis run (on first frame, first player)
                                        if frame_data.get('frame_num', 0) == 0 and i == 0:
                                            print(
                                                f"âš  Team colors not applied: Color mode is 'team' but no team colors loaded")
                                
                                # Generate label text based on label type
                                label_text = get_label_text(
                                    player_name, track_id, team, 
                                    current_label_type, current_label_custom_text, player_names
                                )
                                labels.append(label_text)
                                
                                color = get_player_color(
                                    track_id, team, viz_color_mode, team_colors)
                                colors.append(color)
                            else:
                                labels.append(f"{conf:.2f}")
                                colors.append((255, 0, 255))

                        # Get possession info for this frame (if available)
                        possession_player_id = None
                        if 'possession_player_id' in frame_data:
                            possession_player_id = frame_data['possession_player_id']

                        # Get current visualization settings (dynamic if available)
                        current_viz_style = get_setting('viz_style', viz_style)
                        # SEPARATE CONTROLS: Bounding boxes vs Circles at feet (user requested)
                        current_show_bounding_boxes = get_setting('show_bounding_boxes', show_bounding_boxes if 'show_bounding_boxes' in locals() else True)
                        current_show_circles_at_feet = get_setting('show_circles_at_feet', show_circles_at_feet if 'show_circles_at_feet' in locals() else True)
                        current_box_thickness = get_setting('box_thickness', box_thickness)
                        current_box_shrink_factor = get_setting('box_shrink_factor', box_shrink_factor)

                        # Draw annotations with ellipse at feet (Option C:
                        # Ellipse + triangle for ball possession)
                        for i, (xyxy, color) in enumerate(
                                zip(detections.xyxy, colors)):
                            x1, y1, x2, y2 = map(int, xyxy)
                            track_id = tracker_ids[i] if i < len(
                                tracker_ids) else None

                            # CRITICAL FIX: Respect viz_style setting - "Boxes" should show boxes, not ellipses
                            # Ellipses are only for "circle" or "both" modes when foot_based_tracking is enabled
                            # USER REQUEST: Keep circles at feet (current
                            # position), but disable trailing circles (movement
                            # history)
                            
                            
                            # SEPARATE CONTROL: Draw team-colored circles at feet (user requested)
                            # Circles always use team colors (ignore custom box color)
                            if current_show_circles_at_feet:
                                # Draw ellipse at feet position
                                # Apply vertical offset (negative = above feet, positive = below feet)
                                current_feet_vertical_offset = get_setting('feet_marker_vertical_offset', 50)
                                foot_y = y2 + current_feet_vertical_offset  # Bottom of box = feet position
                                center_x = int((x1 + x2) / 2)

                                # Draw ellipse at feet position (horizontal orientation)
                                ellipse_center = (center_x, int(foot_y))
                                
                                # AUTO-SCALE: Make ellipses bigger for higher resolutions
                                # For 4K (3840x2160), scale up by ~2x so they're visible
                                scale_factor = max(1.0, width / 1920.0)  # Scale based on full width
                                scaled_width = int(current_ellipse_width * scale_factor)
                                scaled_height = int(current_ellipse_height * scale_factor)
                                axes = (int(scaled_width / 2), int(scaled_height / 2))

                                # CRITICAL: Circles always use team colors (ignore custom box color)
                                # This ensures circles are always team-colored as user prefers
                                ellipse_color_to_use = blend_color_with_opacity(color, current_player_viz_alpha)
                                cv2.ellipse(
                                    frame_to_write, ellipse_center, axes, 0, 0, 360, ellipse_color_to_use, -1)  # Filled
                                # Draw white outline for visibility (thickness is adjustable)
                                cv2.ellipse(
                                    frame_to_write,
                                    ellipse_center,
                                    axes,
                                    0,
                                    0,
                                    360,
                                    (255, 255, 255),
                                    current_ellipse_outline_thickness)  # Adjustable white outline thickness
                            # Legacy support
                            elif current_viz_style in ["circle", "both"]:
                                # DEBUG: Log when legacy style is used
                                if frame_count % 1000 == 0:  # Log every 1000 frames to avoid spam
                                    print(f"  ðŸ”§ LEGACY STYLE: Using legacy viz_style='{current_viz_style}' for circles (frame {frame_count})")
                                # Draw ellipse at feet (only for circle/both modes)
                                # Apply vertical offset (negative = above feet, positive = below feet)
                                current_feet_vertical_offset = get_setting('feet_marker_vertical_offset', 50)
                                foot_y = y2 + current_feet_vertical_offset  # Bottom of box = feet position
                                center_x = int((x1 + x2) / 2)

                                # Draw ellipse at feet position (horizontal orientation)
                                ellipse_center = (center_x, int(foot_y))
                                
                                # AUTO-SCALE: Make ellipses bigger for higher resolutions
                                # For 4K (3840x2160), scale up by ~2x so they're visible
                                scale_factor = max(1.0, width / 1920.0)  # Scale based on full width
                                scaled_width = int(ellipse_width * scale_factor)
                                scaled_height = int(ellipse_height * scale_factor)
                                axes = (int(scaled_width / 2), int(scaled_height / 2))

                                # Draw filled ellipse at feet (use custom color if specified, otherwise use team color)
                                base_ellipse_color = current_box_color if current_box_color else color
                                # Apply opacity to ellipse color (use dynamic setting if available)
                                ellipse_color_to_use = blend_color_with_opacity(base_ellipse_color, current_player_viz_alpha)
                                cv2.ellipse(
                                    frame_to_write, ellipse_center, axes, 0, 0, 360, ellipse_color_to_use, -1)  # Filled
                                # Draw THICK white outline for visibility (3px instead of 1px)
                                cv2.ellipse(
                                    frame_to_write,
                                    ellipse_center,
                                    axes,
                                        0,
                                        0,
                                        360,
                                        (255, 255, 255),
                                        3)  # THICK white outline

                                # Draw triangle indicator if player has ball
                                if show_ball_possession and track_id is not None and possession_player_id is not None:
                                    if int(track_id) == int(
                                            possession_player_id):
                                        # Draw upward triangle above ellipse
                                        # (ball possession indicator)
                                        # Proportional to ellipse
                                        triangle_size = max(
                                            8, int(ellipse_height * 0.8))
                                        triangle_top = (center_x, int(
                                            foot_y) - int(ellipse_height / 2) - triangle_size)
                                        triangle_left = (
                                            center_x - triangle_size, int(foot_y) - int(ellipse_height / 2))
                                        triangle_right = (
                                            center_x + triangle_size, int(foot_y) - int(ellipse_height / 2))
                                        triangle_points = np.array(
                                            [triangle_top, triangle_left, triangle_right], np.int32)
                                        # Draw filled triangle (blue for ball
                                        # possession)
                                        cv2.fillPoly(
                                            img=frame_to_write, pts=[triangle_points], color=(
                                                255, 0, 0))  # Blue in BGR
                                        # Draw outline
                                        cv2.polylines(
                                            img=frame_to_write, pts=[triangle_points], isClosed=True, color=(
                                                255, 255, 255), thickness=1)

                            # Calculate center and dimensions for all shapes
                            center_x = int((x1 + x2) / 2)
                            center_y = int((y1 + y2) / 2)
                            box_width = x2 - x1
                            box_height = y2 - y1
                            
                            # Apply shrink factor if needed
                            if current_box_shrink_factor > 0:
                                new_width = box_width * (1 - current_box_shrink_factor * 2)
                                new_height = box_height * (1 - current_box_shrink_factor * 2)
                                tight_x1 = int(center_x - new_width / 2)
                                tight_y1 = int(center_y - new_height / 2)
                                tight_x2 = int(center_x + new_width / 2)
                                tight_y2 = int(center_y + new_height / 2)
                            else:
                                tight_x1, tight_y1, tight_x2, tight_y2 = x1, y1, x2, y2
                            
                            # Draw based on style
                            # SEPARATE CONTROL: Draw bounding boxes (user requested)
                            # Boxes use custom color if enabled, otherwise team colors
                            if current_show_bounding_boxes:
                                # Draw bounding box (use custom color if specified, otherwise use team color)
                                box_color_to_use = current_box_color if current_box_color else color
                                cv2.rectangle(frame_to_write, (tight_x1, tight_y1), (tight_x2, tight_y2), box_color_to_use, current_box_thickness)
                            # Legacy support
                            elif current_viz_style in ["box", "both"]:
                                # DEBUG: Log when legacy style is used
                                if frame_count % 1000 == 0:  # Log every 1000 frames to avoid spam
                                    print(f"  ðŸ”§ LEGACY STYLE: Using legacy viz_style='{current_viz_style}' for bounding boxes (frame {frame_count})")
                                # Draw bounding box (use custom color if specified, otherwise use team color)
                                box_color_to_use = current_box_color if current_box_color else color
                                cv2.rectangle(frame_to_write, (tight_x1, tight_y1), (tight_x2, tight_y2), box_color_to_use, current_box_thickness)
                            
                            if current_viz_style == "star":
                                # Draw 5-pointed star
                                outer_radius = min(box_width, box_height) // 2
                                inner_radius = outer_radius // 2
                                points = []
                                for j in range(10):
                                    angle = j * np.pi / 5 - np.pi / 2
                                    r = outer_radius if j % 2 == 0 else inner_radius
                                    px = int(center_x + r * np.cos(angle))
                                    py = int(center_y + r * np.sin(angle))
                                    points.append([px, py])
                                pts = np.array(points, np.int32)
                                cv2.fillPoly(frame_to_write, [pts], color)
                                cv2.polylines(frame_to_write, [pts], True, (255, 255, 255), current_box_thickness)
                            
                            if current_viz_style == "diamond":
                                # Draw diamond (rotated square)
                                points = np.array([
                                    [center_x, tight_y1],  # Top
                                    [tight_x2, center_y],  # Right
                                    [center_x, tight_y2],  # Bottom
                                    [tight_x1, center_y]   # Left
                                ], np.int32)
                                cv2.fillPoly(frame_to_write, [points], color)
                                cv2.polylines(frame_to_write, [points], True, (255, 255, 255), current_box_thickness)
                            
                            if current_viz_style == "hexagon":
                                # Draw hexagon
                                radius = min(box_width, box_height) // 2
                                points = []
                                for j in range(6):
                                    angle = j * np.pi / 3
                                    px = int(center_x + radius * np.cos(angle))
                                    py = int(center_y + radius * np.sin(angle))
                                    points.append([px, py])
                                pts = np.array(points, np.int32)
                                cv2.fillPoly(frame_to_write, [pts], color)
                                cv2.polylines(frame_to_write, [pts], True, (255, 255, 255), current_box_thickness)
                            
                            if current_viz_style == "arrow":
                                # Draw arrow pointing up
                                arrow_size = min(box_width, box_height) // 2
                                points = np.array([
                                    [center_x, tight_y1],  # Top point
                                    [center_x - arrow_size // 2, tight_y1 + arrow_size // 2],  # Left
                                    [center_x - arrow_size // 4, tight_y1 + arrow_size // 2],  # Left inner
                                    [center_x - arrow_size // 4, tight_y2],  # Bottom left
                                    [center_x + arrow_size // 4, tight_y2],  # Bottom right
                                    [center_x + arrow_size // 4, tight_y1 + arrow_size // 2],  # Right inner
                                    [center_x + arrow_size // 2, tight_y1 + arrow_size // 2]   # Right
                                ], np.int32)
                                cv2.fillPoly(frame_to_write, [points], color)
                                cv2.polylines(frame_to_write, [points], True, (255, 255, 255), current_box_thickness)
                            
                            # Draw ball possession indicator (for all shapes)
                            if show_ball_possession and track_id is not None and possession_player_id is not None:
                                if int(track_id) == int(possession_player_id):
                                    # Draw upward triangle above shape (ball possession indicator)
                                    triangle_size = 8
                                    triangle_top = (center_x, tight_y1 - triangle_size)
                                    triangle_left = (center_x - triangle_size, tight_y1)
                                    triangle_right = (center_x + triangle_size, tight_y1)
                                    triangle_points = np.array([triangle_top, triangle_left, triangle_right], np.int32)
                                    # Draw filled triangle (blue for ball possession)
                                    cv2.fillPoly(frame_to_write, pts=[triangle_points], color=(255, 0, 0))  # Blue in BGR
                                    # Draw outline
                                    cv2.polylines(frame_to_write, pts=[triangle_points], isClosed=True, color=(255, 255, 255), thickness=1)

                            # Draw circles if circle mode is enabled and
                            # foot_based_tracking is off
                            if current_viz_style in [
                                    "circle", "both"] and not foot_based_tracking:
                                cx, cy = int((x1 + x2) / 2), int((y1 + y2) / 2)
                                cv2.circle(
                                    frame_to_write, (cx, cy), 30, color, 2)

                        # CRITICAL FIX: Draw labels manually with our custom colors (not supervision's auto-colors)
                        # This prevents vibrant random colors and ensures team colors are used
                        # ENHANCED: Only draw labels if show_player_labels is
                        # enabled (can hide to reduce clutter)
                        if show_player_labels and len(
                                labels) > 0 and len(detections.xyxy) > 0:
                            tracker_ids = detections.tracker_id if detections.tracker_id is not None else [
                                None] * len(detections.xyxy)
                            for i, (xyxy, label, color) in enumerate(
                                    zip(detections.xyxy, labels, colors)):
                                if i >= len(tracker_ids):
                                    continue
                                x1, y1, x2, y2 = map(int, xyxy)

                                # Position label beside player (to the right of
                                # ellipse) instead of above head
                                # Calculate raw label position
                                raw_label_x = x2 + 5  # Right side of bounding box + small margin
                                raw_label_y = int((y1 + y2) / 2)  # Vertically centered on player
                                
                                # Smooth label position to prevent flickering
                                track_id = tracker_ids[i] if i < len(tracker_ids) else None
                                # Calculate text size first for side-switching logic
                                font_scale = label_font_scale
                                thickness = max(1, int(label_font_scale * 2.5))
                                (text_width, text_height), baseline = cv2.getTextSize(
                                    label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)
                                
                                label_x, label_y = get_smoothed_label_position(
                                    track_id, raw_label_x, raw_label_y, text_width, width, x1, x2)

                                # REMOVED: Black background rectangle (user request - covers too much field)
                                # Text will be drawn directly without background for better visibility

                                # Draw label text with larger font and our
                                # custom color (custom label color, team color, or gray)
                                label_color_to_use = current_label_color if current_label_color else color
                                cv2.putText(frame_to_write, label, (label_x, label_y),
                                            cv2.FONT_HERSHEY_SIMPLEX, font_scale, label_color_to_use, thickness)
                                
                                # Draw analytics if enabled and available
                                if analytics_preferences and track_id is not None:
                                    # Get player_name for this track_id (analytics are now keyed by player_name)
                                    player_name_overlay = player_names.get(str(track_id), f"Player_{track_id}")
                                    # Get analytics for this player from player_analytics dict (keyed by player_name)
                                    player_analytics_dict = player_analytics.get(player_name_overlay, {})
                                    if player_analytics_dict:
                                        analytics_lines = get_analytics_text_for_player(
                                            player_analytics_dict, analytics_preferences, use_imperial_units
                                        )
                                        if analytics_lines:
                                            # Draw analytics text below label
                                            analytics_y = label_y + text_height + 5
                                            # Get analytics font scale from settings (default: 0.5, but use 0.35 for "with_player" mode)
                                            # For banner/panel modes, use full scale; for "with_player", use smaller scale
                                            # Try to get from overlay_metadata if available, otherwise use defaults
                                            analytics_font_scale_base = 0.5
                                            analytics_position = "with_player"
                                            if overlay_metadata and hasattr(overlay_metadata, 'visualization_settings'):
                                                analytics_font_scale_base = overlay_metadata.visualization_settings.get("analytics_font_scale", 0.5)
                                                analytics_position = overlay_metadata.visualization_settings.get("analytics_position", "with_player")
                                            elif 'analytics_font_scale' in locals():
                                                analytics_font_scale_base = locals().get('analytics_font_scale', 0.5)
                                            if analytics_position == "with_player":
                                                analytics_font_scale = analytics_font_scale_base * 0.7  # Smaller for "with_player" mode
                                            else:
                                                analytics_font_scale = analytics_font_scale_base  # Full scale for banners/panels
                                            analytics_thickness = max(1, int(analytics_font_scale))
                                            analytics_line_height = int(10 * analytics_font_scale / 0.35)  # Scale with font
                                            
                                            # Draw semi-transparent background for analytics (reduced flashing)
                                            if len(analytics_lines) > 0:
                                                max_line_width = 0
                                                max_analytics_lines = 8  # Match the display limit
                                                for line in analytics_lines[:max_analytics_lines]:
                                                    (w, h), _ = cv2.getTextSize(
                                                        line, cv2.FONT_HERSHEY_SIMPLEX, analytics_font_scale, analytics_thickness
                                                    )
                                                    max_line_width = max(max_line_width, w)
                                                
                                                if max_line_width > 0:
                                                    # Calculate panel bounds with padding
                                                    panel_x1 = max(0, label_x - 2)
                                                    panel_y1 = max(0, analytics_y - 2)
                                                    panel_x2 = min(width, label_x + max_line_width + 4)
                                                    panel_y2 = min(height, analytics_y + min(max_analytics_lines, len(analytics_lines)) * analytics_line_height + 2)
                                                    
                                                    # Only draw if panel is visible and valid
                                                    if panel_x2 > panel_x1 and panel_y2 > panel_y1:
                                                        # Draw semi-transparent background (reduced alpha to minimize flashing)
                                                        overlay = frame_to_write.copy()
                                                        cv2.rectangle(overlay,
                                                                     (panel_x1, panel_y1),
                                                                     (panel_x2, panel_y2),
                                                                     (0, 0, 0), -1)  # Black background
                                                        # Use lower alpha (0.5 instead of 0.6) to reduce flashing visibility
                                                        cv2.addWeighted(overlay, 0.5, frame_to_write, 0.5, 0, frame_to_write)
                                            
                                            # Draw analytics text (limit to 8 lines to show more metrics)
                                            # Increased from 3 to 8 to show more analytics when user selects "Select All"
                                            max_analytics_lines = 8
                                            for j, line in enumerate(analytics_lines[:max_analytics_lines]):
                                                text_y = analytics_y + j * analytics_line_height
                                                cv2.putText(frame_to_write, line, (label_x, text_y),
                                                           cv2.FONT_HERSHEY_SIMPLEX, analytics_font_scale, (255, 255, 255), analytics_thickness)

                # CRITICAL FIX: After drawing processed detections, ONLY fill gaps with predicted boxes if enabled
                # This prevents "beating pulse" by ensuring smooth continuity even when tracks are temporarily lost
                # Check track_state for tracks that were seen recently but aren't in current detections
                # NOTE: We only draw interpolated tracks if they're NOT already in current detections
                # This prevents duplicate drawing and multiple IDs for the same player
                # USER REQUEST: Only show predicted boxes if explicitly enabled
                # (prevents floating labels)
                if show_predicted_boxes and len(track_state) > 0:
                    # Get track IDs from current detections (if any)
                    current_track_ids = set()
                    if detections is not None and detections.tracker_id is not None:
                        current_track_ids = set(
                            [int(tid) for tid in detections.tracker_id if tid is not None])

                    # Find tracks in track_state that aren't in current detections (fill gaps)
                    # CRITICAL: Only show predicted boxes for tracks that are
                    # ACTUALLY missing (not just temporarily occluded)
                    missing_tracks = {tid: state for tid, state in track_state.items()
                                      if tid not in current_track_ids}

                    if len(missing_tracks) > 0:
                        # Build predicted boxes for missing tracks
                        interpolated_xyxy = []
                        interpolated_tracker_ids = []
                        interpolated_conf = []

                        for track_id, state in missing_tracks.items():
                            frames_since_seen = frame_count - state['frame']
                            
                            # Calculate max prediction frames from duration setting
                            # Get current prediction_duration (dynamic if available)
                            current_prediction_duration = get_setting('prediction_duration', prediction_duration)
                            # Convert seconds to frames based on video FPS
                            max_prediction_frames = int(current_prediction_duration * fps) if fps > 0 else int(current_prediction_duration * 30)  # Fallback to 30fps if FPS unknown
                            # Ensure minimum of 1 frame
                            max_prediction_frames = max(1, max_prediction_frames)

                            # CRITICAL FIX: Only draw predicted boxes if
                            # show_predicted_boxes is enabled
                            if show_predicted_boxes and frames_since_seen <= max_prediction_frames:
                                # USER REQUEST: Use last known position ONLY - NO velocity prediction
                                # This prevents labels from floating away from
                                # players
                                xyxy = state['xyxy'].copy()
                                # Don't move the box at all - just show it at last known position
                                # This prevents floating labels

                                # CRITICAL: Validate box is reasonable before
                                # adding
                                box_width = xyxy[2] - xyxy[0]
                                box_height = xyxy[3] - xyxy[1]
                                if box_width >= 10 and box_height >= 10:
                                    center_x = (xyxy[0] + xyxy[2]) / 2
                                    center_y = (xyxy[1] + xyxy[3]) / 2
                                    margin = 50
                                    if not (
                                            center_x < margin or center_x > width -
                                            margin or center_y < margin or center_y > height -
                                            margin):
                                        interpolated_xyxy.append(xyxy)
                                        interpolated_tracker_ids.append(
                                            track_id)
                                        interpolated_conf.append(0.5)

                        # Draw missing tracks as predicted boxes (identical
                        # styling to detected boxes for seamless tracking)
                        if len(interpolated_xyxy) > 0:
                            for i, (xyxy, track_id) in enumerate(
                                    zip(interpolated_xyxy, interpolated_tracker_ids)):
                                x1, y1, x2, y2 = map(int, xyxy)
                                team = None
                                if viz_color_mode == "team" and team_colors:
                                    team = classify_player_team(
                                        frame, xyxy, team_colors)
                                color = get_player_color(
                                    track_id, team, viz_color_mode, team_colors)
                                # CRITICAL FIX: Use full color intensity (no
                                # fading) for seamless appearance

                                # USER FIX: Don't draw ellipses for interpolated/predicted boxes
                                # This prevents "trailing dots" effect when players move
                                # Ellipses are only drawn for actual detected positions (not interpolated)
                                # The interpolated boxes are at the last known position, so if the player
                                # moved, you'd see ellipses at both old and new positions (trailing effect)
                                # DISABLED: No ellipses for interpolated tracks

                                # Calculate center of predicted box for marker placement
                                center_x = int((x1 + x2) / 2)
                                center_y = int((y1 + y2) / 2)

                                # Draw predicted marker with custom style, size and color
                                # CRITICAL FIX: Draw predicted markers regardless of viz_style
                                # (they're a separate feature - track ID decay visualization)
                                # Get current prediction settings (dynamic if available)
                                current_prediction_style = get_setting('prediction_style', prediction_style)
                                current_prediction_size = get_setting('prediction_size', prediction_size)
                                current_prediction_color = get_setting('prediction_color', prediction_color)
                                
                                # Use custom color if provided, otherwise use default yellow
                                if current_prediction_color is not None:
                                    pred_color = current_prediction_color
                                else:
                                    pred_color = (0, 255, 255)  # Default yellow in BGR
                                
                                # Draw based on selected style
                                if current_prediction_style == "dot":
                                    cv2.circle(frame_to_write, (center_x, center_y), current_prediction_size, pred_color, -1)
                                    cv2.circle(frame_to_write, (center_x, center_y), current_prediction_size, (255, 255, 255), 1)
                                elif current_prediction_style == "box":
                                    half_size = current_prediction_size
                                    cv2.rectangle(frame_to_write, (center_x - half_size, center_y - half_size),
                                                (center_x + half_size, center_y + half_size), pred_color, 2)
                                elif current_prediction_style == "cross":
                                    half_size = current_prediction_size
                                    cv2.line(frame_to_write, (center_x - half_size, center_y), (center_x + half_size, center_y), pred_color, 2)
                                    cv2.line(frame_to_write, (center_x, center_y - half_size), (center_x, center_y + half_size), pred_color, 2)
                                elif current_prediction_style == "x":
                                    half_size = int(current_prediction_size * 0.7)
                                    cv2.line(frame_to_write, (center_x - half_size, center_y - half_size),
                                            (center_x + half_size, center_y + half_size), pred_color, 2)
                                    cv2.line(frame_to_write, (center_x - half_size, center_y + half_size),
                                            (center_x + half_size, center_y - half_size), pred_color, 2)
                                elif current_prediction_style == "arrow":
                                    arrow_size = current_prediction_size
                                    points = np.array([
                                        [center_x, center_y - arrow_size],  # Top point
                                        [center_x - arrow_size // 2, center_y],  # Left
                                        [center_x + arrow_size // 2, center_y]   # Right
                                    ], np.int32)
                                    cv2.fillPoly(frame_to_write, [points], pred_color)
                                elif current_prediction_style == "diamond":
                                    half_size = current_prediction_size
                                    points = np.array([
                                        [center_x, center_y - half_size],  # Top
                                        [center_x + half_size, center_y],   # Right
                                        [center_x, center_y + half_size],   # Bottom
                                        [center_x - half_size, center_y]    # Left
                                    ], np.int32)
                                    cv2.fillPoly(frame_to_write, [points], pred_color)
                                    cv2.polylines(frame_to_write, [points], True, (255, 255, 255), 1)

                                # Draw label using label_annotator for consistent styling with detected tracks
                                # This ensures all labels look identical (no
                                # visual distinction between detected and
                                # predicted)
                                pid_str = str(int(track_id))
                                player_name_raw = player_names.get(
                                    pid_str, f"#{track_id}")
                                # CRITICAL FIX: Handle case where player_name might be a list
                                if isinstance(player_name_raw, list) and len(player_name_raw) > 0:
                                    player_name = str(player_name_raw[0])
                                elif player_name_raw:
                                    player_name = str(player_name_raw)
                                else:
                                    player_name = f"#{track_id}"
                                # Limit track_id display to 2 digits (last 2
                                # digits)
                                display_id = int(track_id) % 100
                                if player_name.startswith("#"):
                                    # Just ID, no confidence for cleaner look
                                    label = f"#{display_id}"
                                else:
                                    # Show name with 2-digit ID
                                    player_name_clean = player_name.split('(')[0].strip() if isinstance(player_name, str) else str(player_name)
                                    label = f"{player_name_clean}"
                                
                                # Create a temporary Detections object for this single interpolated track
                                # This allows us to use label_annotator for
                                # consistent styling
                                single_detection = sv.Detections(
                                    xyxy=np.array([xyxy], dtype=np.float32),
                                    confidence=np.array(
                                        [0.5], dtype=np.float32),
                                    tracker_id=np.array(
                                        [track_id], dtype=np.int32),
                                    class_id=np.array([0], dtype=np.int32)
                                )
                                
                                # CRITICAL FIX: Draw label manually with our custom color (not supervision's auto-colors)
                                # This prevents vibrant random colors and ensures team colors are used
                                # ENHANCED: Only draw labels if
                                # show_player_labels is enabled (can hide to
                                # reduce clutter)
                                if show_player_labels:
                                    x1, y1, x2, y2 = map(int, xyxy)
                                    
                                    # Position label beside player (to the
                                    # right of ellipse) instead of above head
                                    # Calculate raw label position
                                    raw_label_x = x2 + 5  # Right side of bounding box + small margin
                                    raw_label_y = int((y1 + y2) / 2)  # Vertically centered on player
                                    
                                    # CRITICAL FIX: Adjustable label size (can be made smaller to reduce clutter)
                                    # Use GUI-adjustable font scale (default:
                                    # 0.7, can be reduced for less clutter)
                                    font_scale = label_font_scale  # Use GUI-adjustable font scale
                                    # Scale thickness with font size
                                    thickness = max(
                                        1, int(label_font_scale * 2.5))
                                    (text_width, text_height), baseline = cv2.getTextSize(
                                        label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)
                                    
                                    # Smooth label position to prevent flickering (includes side-switching logic)
                                    label_x, label_y = get_smoothed_label_position(
                                        track_id, raw_label_x, raw_label_y, text_width, width, x1, x2)
                                    
                                    # Draw semi-transparent black background to prevent flashing (if label is visible)
                                    # Only draw background if label position is stable (reduces flashing)
                                    if label_x >= 0 and label_y >= 0 and label_x < width and label_y < height:
                                        padding = 4
                                        bg_x1 = max(0, label_x - padding)
                                        bg_y1 = max(0, label_y - text_height - padding)
                                        bg_x2 = min(width, label_x + text_width + padding)
                                        bg_y2 = min(height, label_y + baseline + padding)
                                        
                                        # Use semi-transparent black background (alpha blending) to prevent flashing
                                        overlay = frame_to_write.copy()
                                        cv2.rectangle(overlay, (bg_x1, bg_y1), (bg_x2, bg_y2), (0, 0, 0), -1)
                                        # Blend with alpha (0.6 = 60% opacity, reduces flashing)
                                        alpha = 0.6
                                        cv2.addWeighted(overlay, alpha, frame_to_write, 1 - alpha, 0, frame_to_write)
                                    
                                    # Draw label text with larger font and our
                                    # custom color (custom label color, team color, or gray)
                                    label_color_to_use = current_label_color if current_label_color else color
                                    cv2.putText(frame_to_write, label, (label_x, label_y),
                                                cv2.FONT_HERSHEY_SIMPLEX, font_scale, label_color_to_use, thickness)
            
            elif len(track_state) > 0:
                # CRITICAL FIX: This frame wasn't processed - interpolate/extrapolate tracks to prevent blinking
                # Use last known track positions (or predict using velocity) to keep boxes visible
                # CRITICAL FIX: Use copy to avoid modifying original frame
                # (ball annotations already on it)
                frame_to_write = frame.copy()  # Copy to avoid modifying original frame
                
                # Create interpolated detections from track state to keep boxes
                # visible
                if len(track_state) > 0:
                    # Find the most recent processed frame to get detection
                    # structure
                    recent_frame = max(
                        [f for f in frame_detections.keys() if f <= frame_count], default=None)
                    if recent_frame is not None:
                        recent_detections = frame_detections[recent_frame].get(
                            'detections')
                        if recent_detections is not None and len(
                                recent_detections) > 0:
                            # Build interpolated detections from track state
                            interpolated_xyxy = []
                            interpolated_tracker_ids = []
                            interpolated_conf = []
                            
                            for track_id, state in track_state.items():
                                frames_since_seen = frame_count - \
                                    state['frame']
                                # Calculate max prediction frames from duration setting
                                # Get current prediction_duration (dynamic if available)
                                current_prediction_duration = get_setting('prediction_duration', prediction_duration)
                                # Convert seconds to frames based on video FPS
                                max_prediction_frames = int(current_prediction_duration * fps) if fps > 0 else int(current_prediction_duration * 30)  # Fallback to 30fps if FPS unknown
                                # CRITICAL FIX: max_prediction_frames must be at least as large as batch_size
                                # Otherwise frames between batches will have no visualization (causing flashing)
                                # Use batch_size + 2 extra frames for safety margin
                                # This ensures all frames between processed batches show interpolated tracks
                                max_prediction_frames = max(max_prediction_frames, batch_size + 2)
                                # CRITICAL FIX: Only draw predicted boxes if show_predicted_boxes is enabled
                                # Only show tracks that were seen very recently
                                # (much shorter than full buffer)
                                if show_predicted_boxes and frames_since_seen <= max_prediction_frames:
                                    # USER REQUEST: Use last known position ONLY - NO velocity prediction
                                    # This prevents labels from floating away
                                    # from players
                                    xyxy = state['xyxy'].copy()
                                    # Don't move the box at all - just show it at last known position
                                    # This prevents floating labels
                                    
                                    # CRITICAL: Validate box is reasonable before adding
                                    # Check if box is too small (likely false
                                    # positive)
                                    box_width = xyxy[2] - xyxy[0]
                                    box_height = xyxy[3] - xyxy[1]
                                    if box_width < 10 or box_height < 10:
                                        continue  # Skip tiny boxes
                                    
                                    # Check if box center is in reasonable area
                                    # (not in corners/edges where players
                                    # shouldn't be)
                                    center_x = (xyxy[0] + xyxy[2]) / 2
                                    center_y = (xyxy[1] + xyxy[3]) / 2
                                    margin = 50  # Margin from edges
                                    if center_x < margin or center_x > width - \
                                            margin or center_y < margin or center_y > height - margin:
                                        # Box is too close to edge - might be a
                                        # false positive, skip it
                                        continue
                                    
                                    interpolated_xyxy.append(xyxy)
                                    interpolated_tracker_ids.append(track_id)
                                    # Lower confidence for predicted positions
                                    interpolated_conf.append(0.5)
                                else:
                                    # Too old or disabled - skip this track
                                    # (don't show predicted box)
                                    continue
                            
                            if len(interpolated_xyxy) > 0:
                                # Create interpolated detections object
                                interpolated_detections = sv.Detections(
                                    xyxy=np.array(
                                        interpolated_xyxy, dtype=np.float32),
                                    confidence=np.array(
                                        interpolated_conf, dtype=np.float32),
                                    tracker_id=np.array(
                                        interpolated_tracker_ids, dtype=np.int32),
                                    class_id=np.array(
                                        [0] * len(interpolated_xyxy), dtype=np.int32)
                                )
                                
                                # Draw interpolated boxes (identical styling to
                                # detected boxes for seamless tracking)
                                colors = []
                                labels = []
                                for track_id in interpolated_tracker_ids:
                                    pid_str = str(int(track_id))
                                    player_name_raw = player_names.get(
                                        pid_str, f"#{track_id}")
                                    # CRITICAL FIX: Handle case where player_name might be a list
                                    if isinstance(player_name_raw, list) and len(player_name_raw) > 0:
                                        player_name = str(player_name_raw[0])
                                    elif player_name_raw:
                                        player_name = str(player_name_raw)
                                    else:
                                        player_name = f"#{track_id}"
                                    # CRITICAL FIX: Use same label format as detected boxes (no "(pred)" indicator)
                                    # ENHANCED: Limit ID to 2 digits for
                                    # cleaner display
                                    # Last 2 digits only
                                    display_id = int(track_id) % 100
                                    if player_name.startswith("#"):
                                        labels.append(
                                            f"#{display_id}")  # Just ID
                                    else:
                                        # Show name with 2-digit ID
                                        player_name_clean = player_name.split('(')[0].strip() if isinstance(player_name, str) else str(player_name)
                                        labels.append(
                                            f"{player_name_clean} #{display_id}")
                                    
                                    team = None
                                    if viz_color_mode == "team" and team_colors:
                                        # CRITICAL FIX: Classify team for THIS specific interpolated track
                                        # Find the xyxy for this track_id
                                        track_xyxy = None
                                        for j, (interp_xyxy, interp_tid) in enumerate(
                                                zip(interpolated_xyxy, interpolated_tracker_ids)):
                                            if interp_tid == track_id:
                                                track_xyxy = interp_xyxy
                                                break
                                        if track_xyxy is not None:
                                            team = classify_player_team(
                                                frame, track_xyxy, team_colors)
                                    color = get_player_color(
                                        track_id, team, viz_color_mode, team_colors)
                                    # CRITICAL FIX: Use full color intensity
                                    # (no fading) for seamless appearance
                                    colors.append(color)
                                
                                # CRITICAL FIX: Draw interpolated boxes with custom style, size and color
                                # Get current prediction settings (dynamic if available)
                                current_prediction_style = get_setting('prediction_style', prediction_style)
                                current_prediction_size = get_setting('prediction_size', prediction_size)
                                current_prediction_color = get_setting('prediction_color', prediction_color)
                                
                                # Use custom color if provided, otherwise use default yellow
                                if current_prediction_color is not None:
                                    pred_color = current_prediction_color
                                else:
                                    pred_color = (0, 255, 255)  # Default yellow in BGR
                                
                                for i, (xyxy, color) in enumerate(
                                        zip(interpolated_xyxy, colors)):
                                    x1, y1, x2, y2 = map(int, xyxy)
                                    
                                    # Draw marker at center of predicted position
                                    center_x = int((x1 + x2) / 2)
                                    center_y = int((y1 + y2) / 2)
                                    
                                    # Draw based on selected style
                                    if current_prediction_style == "dot":
                                        cv2.circle(frame_to_write, (center_x, center_y), current_prediction_size, pred_color, -1)
                                        cv2.circle(frame_to_write, (center_x, center_y), current_prediction_size, (255, 255, 255), 1)
                                    elif current_prediction_style == "box":
                                        half_size = current_prediction_size
                                        cv2.rectangle(frame_to_write, (center_x - half_size, center_y - half_size),
                                                    (center_x + half_size, center_y + half_size), pred_color, 2)
                                    elif current_prediction_style == "cross":
                                        half_size = current_prediction_size
                                        cv2.line(frame_to_write, (center_x - half_size, center_y), (center_x + half_size, center_y), pred_color, 2)
                                        cv2.line(frame_to_write, (center_x, center_y - half_size), (center_x, center_y + half_size), pred_color, 2)
                                    elif current_prediction_style == "x":
                                        half_size = int(current_prediction_size * 0.7)
                                        cv2.line(frame_to_write, (center_x - half_size, center_y - half_size),
                                                (center_x + half_size, center_y + half_size), pred_color, 2)
                                        cv2.line(frame_to_write, (center_x - half_size, center_y + half_size),
                                                (center_x + half_size, center_y - half_size), pred_color, 2)
                                    elif current_prediction_style == "arrow":
                                        arrow_size = current_prediction_size
                                        points = np.array([
                                            [center_x, center_y - arrow_size],  # Top point
                                            [center_x - arrow_size // 2, center_y],  # Left
                                            [center_x + arrow_size // 2, center_y]   # Right
                                        ], np.int32)
                                        cv2.fillPoly(frame_to_write, [points], pred_color)
                                    elif current_prediction_style == "diamond":
                                        half_size = current_prediction_size
                                        points = np.array([
                                            [center_x, center_y - half_size],  # Top
                                            [center_x + half_size, center_y],   # Right
                                            [center_x, center_y + half_size],   # Bottom
                                            [center_x - half_size, center_y]    # Left
                                        ], np.int32)
                                        cv2.fillPoly(frame_to_write, [points], pred_color)
                                        cv2.polylines(frame_to_write, [points], True, (255, 255, 255), 1)
                                
                                # CRITICAL FIX: Draw labels manually with our custom colors (not supervision's auto-colors)
                                # This prevents vibrant random colors and ensures team colors are used
                                # ENHANCED: Only draw labels if
                                # show_player_labels is enabled (can hide to
                                # reduce clutter)
                                if show_player_labels and len(
                                        labels) > 0 and len(interpolated_xyxy) > 0:
                                    for i, (xyxy, label) in enumerate(
                                            zip(interpolated_xyxy, labels)):
                                        if i >= len(colors):
                                            continue
                                        color = colors[i]
                                        x1, y1, x2, y2 = map(int, xyxy)
                                        
                                        # Position label beside player (to the
                                        # right of ellipse) instead of above
                                        # head
                                        # Calculate raw label position
                                        raw_label_x = x2 + 5  # Right side of bounding box + small margin
                                        raw_label_y = int((y1 + y2) / 2)  # Vertically centered on player
                                        
                                        # CRITICAL FIX: Adjustable label size (can be made smaller to reduce clutter)
                                        # Use GUI-adjustable font scale
                                        # (default: 0.7, can be reduced for
                                        # less clutter)
                                        font_scale = label_font_scale  # Use GUI-adjustable font scale
                                        # Scale thickness with font size
                                        thickness = max(
                                            1, int(label_font_scale * 2.5))
                                        (text_width, text_height), baseline = cv2.getTextSize(
                                            label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)
                                        
                                        # Smooth label position to prevent flickering (includes side-switching logic)
                                        # Note: For interpolated tracks, we may not have track_id in the same format
                                        # Try to extract track_id from labels or use index
                                        track_id = None
                                        if i < len(interpolated_tracker_ids):
                                            track_id = interpolated_tracker_ids[i]
                                        label_x, label_y = get_smoothed_label_position(
                                            track_id, raw_label_x, raw_label_y, text_width, width, x1, x2)
                                        
                                        # Draw larger text background for
                                        # REMOVED: Black background rectangle (user request - covers too much field)
                                        # Text will be drawn directly without background for better visibility
                                        
                                        # Draw label text with larger font and
                                        # our custom color (custom label color, team color, or gray)
                                        label_color_to_use = current_label_color if current_label_color else color
                                        cv2.putText(frame_to_write, label, (label_x, label_y),
                                                    cv2.FONT_HERSHEY_SIMPLEX, font_scale, label_color_to_use, thickness)
            else:
                # No processed frames yet - write frame (ball annotations should already be on original frame)
                # OPTIMIZATION: Skip frame copy in watch-only mode without viewer
                if not (watch_only and not show_live_viewer):
                    # CRITICAL FIX: Use copy to ensure consistent drawing
                    # Ball annotations are already on 'frame' from track_ball_in_frame, so copy includes them
                    if frame_to_write is None:
                        frame_to_write = frame.copy()  # Copy to ensure consistent drawing (includes ball annotations)
                else:
                    frame_to_write = None
            
            # LIVE VIEWER: Display frame if enabled (works in both watch-only and normal mode)
            # OPTIMIZATION: Skip display if no frame to write (watch-only without viewer)
            if frame_to_write is None and watch_only and not show_live_viewer:
                # Skip display - we're in watch-only mode without viewer, no annotation needed
                pass
            elif show_live_viewer and live_viewer_window and dynamic_settings:
                try:
                    # OPTIMIZATION: Only check window size every 10 frames to reduce overhead
                    if not hasattr(dynamic_settings, '_cached_window_size') or frame_count % 10 == 0:
                        try:
                            window_rect = cv2.getWindowImageRect(live_viewer_window)
                            if window_rect and len(window_rect) >= 4:
                                window_width = window_rect[2]
                                window_height = window_rect[3]
                                # Cache window size
                                if not hasattr(dynamic_settings, '_cached_window_size'):
                                    dynamic_settings._cached_window_size = (window_width, window_height)
                                else:
                                    # Update cache if window was resized (check for >5px change)
                                    old_w, old_h = dynamic_settings._cached_window_size
                                    if abs(old_w - window_width) > 5 or abs(old_h - window_height) > 5:
                                        dynamic_settings._cached_window_size = (window_width, window_height)
                        except:
                            pass
                    
                    # OPTIMIZATION: Apply viewer downscaling for performance
                    display_frame = frame_to_write.copy()
                    if viewer_downscale_target:
                        frame_h, frame_w = display_frame.shape[:2]
                        target_w, target_h = viewer_downscale_target
                        # Only downscale if frame is larger than target
                        if frame_w > target_w or frame_h > target_h:
                            scale_w = target_w / frame_w
                            scale_h = target_h / frame_h
                            scale = min(scale_w, scale_h)
                            new_w = int(frame_w * scale)
                            new_h = int(frame_h * scale)
                            display_frame = cv2.resize(display_frame, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
                    
                    # Use cached window size for scaling (only if not using downscaling)
                    if not viewer_downscale_target and hasattr(dynamic_settings, '_cached_window_size'):
                        window_width, window_height = dynamic_settings._cached_window_size
                        if window_width > 0 and window_height > 0:
                            frame_h, frame_w = display_frame.shape[:2]
                            # Only scale if window is significantly different from frame size
                            if abs(window_width - frame_w) > 10 or abs(window_height - frame_h) > 10:
                                scale_w = window_width / frame_w
                                scale_h = window_height / frame_h
                                scale = min(scale_w, scale_h)
                                if abs(scale - 1.0) > 0.01:  # Only resize if scale is significantly different
                                    new_w = int(frame_w * scale)
                                    new_h = int(frame_h * scale)
                                    display_frame = cv2.resize(display_frame, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
                    
                    # Display the annotated frame (threaded or blocking)
                    if viewer_threaded and viewer_queue is not None:
                        # Non-blocking: add to queue
                        try:
                            def key_handler(key):
                                nonlocal live_viewer_window
                                if key == ord('q') or key == 27:
                                    print("\nâš  Live viewer closed by user (pressed 'q')")
                                    cv2.destroyWindow(live_viewer_window)
                                    live_viewer_window = None
                                    viewer_thread_running = False
                                elif key == ord(' ') or key == 32:
                                    dynamic_settings.paused = not dynamic_settings.paused
                                    if dynamic_settings.paused:
                                        print("\nâ¸ PAUSED")
                                    else:
                                        print("\nâ–¶ RESUMED")
                                elif key == ord('r') or key == ord('R'):
                                    if dynamic_settings.paused:
                                        dynamic_settings.paused = False
                                        print("\nâ–¶ RESUMED")
                            
                            if not viewer_queue.full():
                                viewer_queue.put((display_frame, key_handler), block=False)
                        except:
                            pass  # Queue full, skip frame
                    else:
                        # Blocking display
                        cv2.imshow(live_viewer_window, display_frame)
                    
                    # Handle keyboard input (only for blocking mode, threaded mode handles in callback)
                    if not viewer_threaded:
                        # CRITICAL: Use 1ms delay to prevent blocking while still processing events
                        wait_delay = 1  # 1ms - keeps window responsive, prevents freezing
                        key = cv2.waitKey(wait_delay) & 0xFF
                        
                        if key == ord('q') or key == 27:  # 'q' or ESC
                            print("\nâš  Live viewer closed by user (pressed 'q')")
                            cv2.destroyWindow(live_viewer_window)
                            live_viewer_window = None
                        elif key == ord(' ') or key == 32:  # SPACEBAR - toggle pause
                            dynamic_settings.paused = not dynamic_settings.paused
                            if dynamic_settings.paused:
                                print("\nâ¸ PAUSED - Adjust settings in GUI, then press SPACEBAR or 'r' to resume")
                            else:
                                print("\nâ–¶ RESUMED")
                        elif key == ord('r') or key == ord('R'):  # 'r' - resume
                            if dynamic_settings.paused:
                                dynamic_settings.paused = False
                                print("\nâ–¶ RESUMED")
                    
                    # If paused, keep displaying current frame and checking for resume
                    if dynamic_settings.paused:
                        # Keep showing the current frame while paused
                        # Loop to keep checking for resume key (use minimal delay to prevent freezing)
                        while dynamic_settings.paused:
                            key = cv2.waitKey(1) & 0xFF  # 1ms delay - keeps window responsive
                            if key == ord(' ') or key == 32:  # SPACEBAR - resume
                                dynamic_settings.paused = False
                                print("\nâ–¶ RESUMED")
                                break
                            elif key == ord('r') or key == ord('R'):  # 'r' - resume
                                dynamic_settings.paused = False
                                print("\nâ–¶ RESUMED")
                                break
                            elif key == ord('q') or key == 27:  # 'q' or ESC - close
                                print("\nâš  Live viewer closed by user (pressed 'q')")
                                cv2.destroyWindow(live_viewer_window)
                                live_viewer_window = None
                                break
                            # Redisplay frame to keep it visible (use cached window size)
                            if hasattr(dynamic_settings, '_cached_window_size'):
                                try:
                                    window_width, window_height = dynamic_settings._cached_window_size
                                    if window_width > 0 and window_height > 0:
                                        frame_h, frame_w = frame_to_write.shape[:2]
                                        if abs(window_width - frame_w) > 10 or abs(window_height - frame_h) > 10:
                                            scale_w = window_width / frame_w
                                            scale_h = window_height / frame_h
                                            scale = min(scale_w, scale_h)
                                            if abs(scale - 1.0) > 0.01:
                                                new_w = int(frame_w * scale)
                                                new_h = int(frame_h * scale)
                                                frame_to_write = cv2.resize(frame_to_write, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
                                except:
                                    pass
                            try:
                                cv2.imshow(live_viewer_window, frame_to_write)
                            except:
                                break  # Window was closed
                except Exception as e:
                    # Silently fail if window was closed
                    if "window" not in str(e).lower():
                        pass
            elif show_live_viewer and live_viewer_window:
                # Fallback if dynamic_settings not available
                try:
                    # OPTIMIZATION: Only check window size every 10 frames
                    if frame_count % 10 == 0:
                        try:
                            window_rect = cv2.getWindowImageRect(live_viewer_window)
                            if window_rect and len(window_rect) >= 4:
                                window_width = window_rect[2]
                                window_height = window_rect[3]
                                if not hasattr(combined_analysis_optimized, '_fallback_window_size'):
                                    combined_analysis_optimized._fallback_window_size = (window_width, window_height)
                                else:
                                    old_w, old_h = combined_analysis_optimized._fallback_window_size
                                    if abs(old_w - window_width) > 5 or abs(old_h - window_height) > 5:
                                        combined_analysis_optimized._fallback_window_size = (window_width, window_height)
                        except:
                            pass
                    
                    # OPTIMIZATION: Apply viewer downscaling for performance
                    display_frame = frame_to_write.copy()
                    if viewer_downscale_target:
                        frame_h, frame_w = display_frame.shape[:2]
                        target_w, target_h = viewer_downscale_target
                        # Only downscale if frame is larger than target
                        if frame_w > target_w or frame_h > target_h:
                            scale_w = target_w / frame_w
                            scale_h = target_h / frame_h
                            scale = min(scale_w, scale_h)
                            new_w = int(frame_w * scale)
                            new_h = int(frame_h * scale)
                            display_frame = cv2.resize(display_frame, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
                    
                    # Use cached window size (only if not using downscaling)
                    if not viewer_downscale_target and hasattr(combined_analysis_optimized, '_fallback_window_size'):
                        window_width, window_height = combined_analysis_optimized._fallback_window_size
                        if window_width > 0 and window_height > 0:
                            frame_h, frame_w = display_frame.shape[:2]
                            if abs(window_width - frame_w) > 10 or abs(window_height - frame_h) > 10:
                                scale_w = window_width / frame_w
                                scale_h = window_height / frame_h
                                scale = min(scale_w, scale_h)
                                if abs(scale - 1.0) > 0.01:
                                    new_w = int(frame_w * scale)
                                    new_h = int(frame_h * scale)
                                    display_frame = cv2.resize(display_frame, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
                    
                    # Display (threaded or blocking)
                    if viewer_threaded and viewer_queue is not None:
                        try:
                            if not viewer_queue.full():
                                viewer_queue.put((display_frame, None), block=False)
                        except:
                            pass
                    else:
                        cv2.imshow(live_viewer_window, display_frame)
                        key = cv2.waitKey(1) & 0xFF
                    if key == ord('q') or key == 27:
                        print("\nâš  Live viewer closed by user (pressed 'q')")
                        cv2.destroyWindow(live_viewer_window)
                        live_viewer_window = None
                except Exception as e:
                    if "window" not in str(e).lower():
                        pass
            
            # Export overlay metadata (before writing frames)
            # CRITICAL FIX: Create overlay metadata for ALL frames, not just frames with detections
            # This ensures 100% coverage for consistent rendering in playback viewer
            if overlay_metadata is not None:
                # Use frame_count directly (all frames should have metadata, even if empty)
                actual_frame_num = frame_count
                
                # CRITICAL FIX: Use most recent processed frame's detections (like video writing does)
                # This prevents gaps when frames are processed in batches (process_every_nth_frame > 1)
                # For non-processed frames, use the most recent processed frame to maintain smooth tracking
                if actual_frame_num in frame_detections:
                    use_frame_for_overlay = actual_frame_num
                elif len(frame_detections) > 0:
                    # Use most recent processed frame (prevents gaps between batches)
                    use_frame_for_overlay = max([f for f in frame_detections.keys() if f <= actual_frame_num], default=None)
                    if use_frame_for_overlay is None and len(frame_detections) > 0:
                        use_frame_for_overlay = min(frame_detections.keys())  # Use earliest if no frames before this one
                else:
                    use_frame_for_overlay = None
                
                # Get frame data from most recent processed frame (or current if available)
                if use_frame_for_overlay is not None and use_frame_for_overlay in frame_detections:
                    frame_data = frame_detections[use_frame_for_overlay].copy()  # Copy to avoid modifying original
                    # Update frame_num to match actual frame for consistency
                    frame_data['frame_num'] = actual_frame_num
                else:
                    # No processed frames yet - create empty overlay
                    frame_data = {}
                
                # Get raw YOLO detections if available
                raw_yolo_detections = frame_data.get('raw_yolo_detections')
                
                # Always create overlay metadata entry, even if frame has no detections
                try:
                    players_overlay = []
                    
                    # Collect player overlay data
                    if 'detections' in frame_data and frame_data['detections'] is not None:
                        detections = frame_data['detections']
                        tracker_ids = detections.tracker_id if detections.tracker_id is not None else [None] * len(detections.xyxy)
                        
                        for i, (xyxy, track_id) in enumerate(zip(detections.xyxy, tracker_ids)):
                            if track_id is None:
                                continue
                            
                            x1, y1, x2, y2 = map(float, xyxy)
                            
                            # CRITICAL FIX: Clamp bbox coordinates to frame bounds to prevent boxes from being drawn outside frame
                            # This fixes issues where coordinates might be in wrong coordinate system or incorrectly scaled
                            # Use original frame dimensions (width, height are available in this scope)
                            x1 = max(0, min(x1, width - 1))
                            y1 = max(0, min(y1, height - 1))
                            x2 = max(x1 + 1, min(x2, width))  # Ensure width > 0
                            y2 = max(y1 + 1, min(y2, height))  # Ensure height > 0
                            
                            # Skip invalid boxes (too small or completely outside frame)
                            if x2 <= x1 or y2 <= y1 or (x2 - x1) < 5 or (y2 - y1) < 5:
                                continue
                            
                            center_x = (x1 + x2) / 2
                            center_y = (y1 + y2) / 2
                            
                            # Get player info
                            track_id_str = str(int(track_id))
                            player_name = player_names.get(track_id_str, None)
                            team = track_to_team_global.get(track_id, None) if 'track_to_team_global' in locals() else None
                            jersey_number = None  # Will be populated from player_roster if available
                            
                            # Get color
                            color = get_player_color(track_id, team, viz_color_mode, team_colors) if 'team_colors' in locals() else (128, 128, 128)
                            
                            # Get analytics if available (keyed by player_name, not track_id)
                            analytics = None
                            if actual_frame_num in player_analytics and player_name and player_name in player_analytics[actual_frame_num]:
                                analytics = player_analytics[actual_frame_num][player_name]
                            
                            # Check if player has ball (from possession_player_id)
                            has_ball = False
                            if 'possession_player_id' in frame_data:
                                possession_id = frame_data.get('possession_player_id')
                                if possession_id is not None and int(track_id) == int(possession_id):
                                    has_ball = True
                            
                            # ENHANCEMENT: Get velocity and position history for direction arrow and trail
                            velocity = None
                            direction_angle = None
                            position_history_list = None
                            if track_id_int in player_velocity_cache:
                                vx, vy, angle = player_velocity_cache[track_id_int]
                                velocity = (vx, vy)
                                direction_angle = angle
                            if track_id_int in player_position_history:
                                # Convert deque to list of (x, y) tuples
                                position_history_list = [(x, y) for x, y, _ in list(player_position_history[track_id_int])]
                            
                            player_overlay = create_player_overlay_data(
                                track_id=int(track_id),
                                bbox=(x1, y1, x2, y2),
                                center=(center_x, center_y),
                                player_name=player_name,
                                team=team,
                                jersey_number=jersey_number,
                                confidence=detections.confidence[i] if detections.confidence is not None else 1.0,
                                color=color,
                                speed=analytics.get('speed_mps') if analytics else None,
                                distance_to_ball=analytics.get('distance_to_ball') if analytics else None,
                                has_ball=has_ball,
                                velocity=velocity,
                                direction_angle=direction_angle,
                                position_history=position_history_list
                            )
                            players_overlay.append(player_overlay)
                    
                    # Collect ball overlay data
                    ball_overlay = None
                    # Get ball center from current frame's ball tracking (not from frame_data)
                    # ball_pts is a deque that tracks recent ball positions
                    if ball_pts and len(ball_pts) > 0:
                        ball_center = ball_pts[-1]  # Most recent ball position
                        
                        # CRITICAL FIX: Filter erratic ball trail points
                        # Remove points that jump too far (likely false positives)
                        MAX_BALL_JUMP_PX = 200  # Maximum pixels ball can jump between frames
                        filtered_trail = []
                        prev_point = None
                        
                        for point in ball_pts:
                            if prev_point is None:
                                # First point - always include
                                filtered_trail.append(point)
                                prev_point = point
                            else:
                                # Check distance from previous point
                                dx = point[0] - prev_point[0]
                                dy = point[1] - prev_point[1]
                                distance = np.sqrt(dx*dx + dy*dy)
                                
                                # Only include if jump is reasonable
                                if distance <= MAX_BALL_JUMP_PX:
                                    filtered_trail.append(point)
                                    prev_point = point
                                # If jump is too large, skip this point but keep previous
                                # (prev_point stays the same)
                        
                        ball_trail = filtered_trail if len(filtered_trail) > 0 else [ball_center]
                        ball_speed = None
                        # Try to get ball speed from analytics if available
                        if actual_frame_num in player_analytics:
                            # Ball analytics might be stored differently - check frame_data first
                            if 'ball_speed_mps' in frame_data:
                                ball_speed = frame_data['ball_speed_mps']
                        
                        ball_overlay = create_ball_overlay_data(
                            center=ball_center,
                            detected=True,
                            trail=ball_trail,
                            speed=ball_speed
                        )
                    
                    # Add analytics data
                    analytics_data = None
                    if actual_frame_num in player_analytics:
                        analytics_data = player_analytics[actual_frame_num]
                    
                    # Collect trajectories for players
                    trajectories = []
                    if overlay_metadata is not None:  # Only collect trajectories if overlay metadata is enabled
                        for player in players_overlay:
                            track_id = player["track_id"]
                            center = (player["center"][0], player["center"][1])
                            
                            # Initialize trajectory deque if needed
                            if track_id not in player_trajectories:
                                player_trajectories[track_id] = deque(maxlen=30)  # Keep last 30 positions
                            
                            # Add current position
                            player_trajectories[track_id].append(center)
                            
                            # Create trajectory data if we have enough points
                            if len(player_trajectories[track_id]) >= 2:
                                trajectory = create_trajectory_data(
                                    track_id=track_id,
                                    points=list(player_trajectories[track_id]),
                                    color=player["color"]
                                )
                                trajectories.append(trajectory)
                    
                    # Collect predicted boxes for lost tracks (if enabled)
                    predicted_boxes_overlay = []
                    if show_predicted_boxes and 'track_state' in locals() and len(track_state) > 0:
                        # Get current track IDs from detections
                        current_track_ids = set()
                        if 'detections' in frame_data and frame_data['detections'] is not None:
                            detections = frame_data['detections']
                            if detections.tracker_id is not None:
                                current_track_ids = set([int(tid) for tid in detections.tracker_id if tid is not None])
                        
                        # Find lost tracks (in track_state but not in current detections)
                        missing_tracks = {tid: state for tid, state in track_state.items() if tid not in current_track_ids}
                        
                        if len(missing_tracks) > 0:
                            # Calculate max prediction frames from duration setting
                            current_prediction_duration = get_setting('prediction_duration', prediction_duration) if 'prediction_duration' in locals() else 0.3
                            max_prediction_frames = int(current_prediction_duration * fps) if fps > 0 else int(current_prediction_duration * 30)
                            max_prediction_frames = max(1, max_prediction_frames)
                            
                            # Get prediction settings
                            current_prediction_style = get_setting('prediction_style', prediction_style) if 'prediction_style' in locals() else "dot"
                            current_prediction_size = get_setting('prediction_size', prediction_size) if 'prediction_size' in locals() else 5
                            
                            for track_id, state in missing_tracks.items():
                                frames_since_seen = actual_frame_num - state.get('frame', actual_frame_num)
                                
                                # Only include if within prediction duration
                                if frames_since_seen <= max_prediction_frames:
                                    xyxy = state.get('xyxy', None)
                                    if xyxy is not None:
                                        x1, y1, x2, y2 = map(float, xyxy)
                                        center_x = (x1 + x2) / 2
                                        center_y = (y1 + y2) / 2
                                        
                                        # Validate box
                                        box_width = x2 - x1
                                        box_height = y2 - y1
                                        if box_width >= 10 and box_height >= 10:
                                            # Get color for predicted box
                                            team = track_to_team_global.get(track_id, None) if 'track_to_team_global' in locals() else None
                                            color = get_player_color(track_id, team, viz_color_mode, team_colors) if 'team_colors' in locals() and 'viz_color_mode' in locals() else (0, 255, 255)  # Default yellow
                                            
                                            # Use custom prediction color if specified
                                            current_prediction_color = get_setting('prediction_color', prediction_color) if 'prediction_color' in locals() else None
                                            if current_prediction_color is not None:
                                                color = current_prediction_color
                                            
                                            predicted_box = create_predicted_box_data(
                                                track_id=int(track_id),
                                                bbox=(x1, y1, x2, y2),
                                                center=(center_x, center_y),
                                                color=color,
                                                style=current_prediction_style,
                                                size=current_prediction_size
                                            )
                                            predicted_boxes_overlay.append(predicted_box)
                    
                    # Add trajectories and predicted boxes to overlay data
                    overlay_data_with_trajectories = {
                        "players": players_overlay,
                        "ball": ball_overlay,
                        "trajectories": trajectories
                    }
                    
                    # Add predicted boxes if any
                    if predicted_boxes_overlay:
                        overlay_data_with_trajectories["predicted_boxes"] = predicted_boxes_overlay
                    
                    # Add raw YOLO detections to overlay data if available
                    if raw_yolo_detections is not None:
                        # Convert numpy arrays to lists for JSON serialization
                        overlay_data_with_trajectories['raw_yolo_detections'] = {
                            'xyxy': raw_yolo_detections['xyxy'].tolist() if isinstance(raw_yolo_detections['xyxy'], np.ndarray) else raw_yolo_detections['xyxy'],
                            'confidence': raw_yolo_detections['confidence'].tolist() if isinstance(raw_yolo_detections['confidence'], np.ndarray) else raw_yolo_detections['confidence'],
                            'class_id': raw_yolo_detections['class_id'].tolist() if isinstance(raw_yolo_detections['class_id'], np.ndarray) else raw_yolo_detections['class_id']
                        }
                    
                    # Add to overlay metadata using actual frame number
                    # CRITICAL: Always add overlay entry, even if empty (ensures 100% coverage)
                    overlay_metadata.overlays[actual_frame_num] = overlay_data_with_trajectories
                    if analytics_data:
                        overlay_metadata.analytics_data[actual_frame_num] = analytics_data
                except Exception as e:
                    # Silently fail - overlay export is optional
                    if frame_count % 500 == 0:
                        print(f"âš  Overlay metadata export error: {e}")
                    # Even on error, create empty overlay entry to maintain frame count
                    try:
                        overlay_metadata.overlays[actual_frame_num] = {
                            "players": [],
                            "ball": None,
                            "trajectories": []
                        }
                    except:
                        pass  # If this also fails, skip this frame
            
            # Write base video (clean, no overlays) if enabled
            if base_video_writer is not None and base_video_writer.isOpened():
                try:
                    base_video_writer.write(frame.copy())  # Write clean frame
                except Exception as e:
                    if frame_count % 500 == 0:
                        print(f"âš  Base video write error: {e}")
            
            # Write frames at output frame rate (downsample if needed)
            # CRITICAL FIX: Write frames immediately when no downsampling for smooth playback
            # Ensure every frame is written in order to prevent choppy playback
            if not watch_only and enable_video_encoding and out is not None:  # Skip writing if encoding disabled
                # CRITICAL FIX: Ensure frame_to_write is not None before writing
                # If frame_to_write is None, create a copy of the original frame (ball annotations should already be on it)
                if frame_to_write is None:
                    frame_to_write = frame.copy()  # Copy frame (ball annotations should already be on original frame)
                
                if output_frame_skip > 1:
                    # Downsampling: batch frames and write every Nth frame
                    if frame_to_write is not None:
                        frames_to_write.append(frame_to_write)
                    if len(frames_to_write) >= output_frame_skip or frame_count == total_frames - 1:
                        # Write the first frame from the batch (downsampling)
                        if len(frames_to_write) > 0 and frames_to_write[0] is not None:
                            out.write(frames_to_write[0])
                        frames_to_write = []
                else:
                    # No downsampling: write each frame immediately for smooth playback
                    # Write frame directly (don't queue) to ensure smooth, consistent playback
                    if frame_to_write is not None:
                        out.write(frame_to_write)
                    else:
                        # CRITICAL FIX: If frame_to_write is None but we're not in watch-only mode,
                        # we should still write the frame (ball annotations are on original frame)
                        # This ensures frames are always written even if there are no player detections
                        frame_to_write = frame.copy()  # Copy includes ball annotations from track_ball_in_frame
                        out.write(frame_to_write)
        else:
            # No player tracking - write frame directly
            # LIVE VIEWER: Display frame in watch-only mode if enabled
            if watch_only and show_live_viewer and live_viewer_window:
                try:
                    frame_copy = frame.copy()
                    
                    # Get current window size to maintain aspect ratio when resizing
                    # OPTIMIZATION: Only check window size every 10 frames
                    if frame_count % 10 == 0:
                        try:
                            window_rect = cv2.getWindowImageRect(live_viewer_window)
                            if window_rect and len(window_rect) >= 4:
                                window_width = window_rect[2]
                                window_height = window_rect[3]
                                if not hasattr(combined_analysis_optimized, '_fallback_window_size'):
                                    combined_analysis_optimized._fallback_window_size = (window_width, window_height)
                                else:
                                    old_w, old_h = combined_analysis_optimized._fallback_window_size
                                    if abs(old_w - window_width) > 5 or abs(old_h - window_height) > 5:
                                        combined_analysis_optimized._fallback_window_size = (window_width, window_height)
                        except:
                            pass
                    
                    # Use cached window size
                    if hasattr(combined_analysis_optimized, '_fallback_window_size'):
                        window_width, window_height = combined_analysis_optimized._fallback_window_size
                        if window_width > 0 and window_height > 0:
                            frame_h, frame_w = frame_copy.shape[:2]
                            if abs(window_width - frame_w) > 10 or abs(window_height - frame_h) > 10:
                                scale_w = window_width / frame_w
                                scale_h = window_height / frame_h
                                scale = min(scale_w, scale_h)
                                if abs(scale - 1.0) > 0.01:
                                    new_w = int(frame_w * scale)
                                    new_h = int(frame_h * scale)
                                    frame_copy = cv2.resize(frame_copy, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
                    
                    cv2.imshow(live_viewer_window, frame_copy)
                    # Handle window close (press 'q' to quit) - non-blocking wait
                    key = cv2.waitKey(1) & 0xFF
                    if key == ord('q') or key == 27:
                        print("\nâš  Live viewer closed by user (pressed 'q')")
                        cv2.destroyWindow(live_viewer_window)
                        live_viewer_window = None
                except Exception as e:
                    # Silently fail if window was closed
                    if "window" not in str(e).lower():
                        pass
            
            if not watch_only:  # Skip writing in watch-only mode
                # CRITICAL FIX: Copy frame to avoid modifying original
                frame_copy = frame.copy()
                frames_to_write.append(frame_copy)
                
                # Write frames at output frame rate (downsample if needed)
                # CRITICAL FIX: Write frames immediately when no downsampling for smooth playback
                if output_frame_skip > 1:
                    # Downsampling: batch frames and write every Nth frame
                    if len(frames_to_write) >= output_frame_skip or frame_count == total_frames - 1:
                        out.write(frames_to_write[0])
                        frames_to_write = []
                else:
                    # No downsampling: write each frame immediately for smooth playback
                    out.write(frame_copy)
                    frames_to_write = []  # Clear queue since we wrote immediately
        
        if False:  # Legacy code path - not used when player tracking is enabled
            # No player tracking, process normally
            player_centers = {}
            if track_players_flag and model is not None:
                # OPTIMIZATION: Use optimized inference parameters
                frame_h, frame_w = frame.shape[:2]
                # Round to nearest multiple of 32 (YOLO stride requirement)
                max_dim = max(frame_w, frame_h)
                imgsz = int(round(max_dim / 32) * 32)
                use_half = device == 'cuda' and torch.cuda.is_available() if 'device' in locals() else False
                results = model(
                    frame, 
                    classes=[0], 
                    verbose=False,
                    imgsz=imgsz,
                    half=use_half,
                    max_det=25,
                    stream=use_yolo_streaming  # Streaming mode for memory efficiency
                )
                detections = sv.Detections.from_ultralytics(results[0])
                # Handle ByteTrack, OC-SORT, and BoxMOT APIs
                if hasattr(tracker, '__class__') and 'BoxMOT' in tracker.__class__.__name__:
                    # BoxMOT wrapper needs frame for appearance features
                    detections = tracker.update(detections, frame)
                elif OCSORT_AVAILABLE and hasattr(tracker, '__class__') and tracker.__class__.__name__ == 'OCSortTracker':
                    detections = tracker.update(detections)
                else:
                    detections = tracker.update_with_detections(detections)
                
                # HOTA-GUIDED TRACKING: Collect tracking data (full resolution path)
                if hota_guided_tracker is not None and detections is not None and len(detections) > 0:
                    try:
                        hota_detections = []
                        tracker_ids_temp = detections.tracker_id if detections.tracker_id is not None else [None] * len(detections)
                        for i, (xyxy, track_id) in enumerate(zip(detections.xyxy, tracker_ids_temp)):
                            if track_id is not None:
                                x1, y1, x2, y2 = xyxy
                                hota_detections.append((int(track_id), float(x1), float(y1), float(x2), float(y2)))
                        
                        if hota_detections:
                            hota_guided_tracker.add_frame_data(frame_count, hota_detections)
                    except Exception as e:
                        if frame_count % 500 == 0:
                            print(f"âš  HOTA data collection failed: {e}")
                
                labels = []
                # Handle case where tracker_id is None
                tracker_ids = detections.tracker_id if detections.tracker_id is not None else [None] * len(detections.xyxy)
                for track_id, conf in zip(tracker_ids, detections.confidence):
                    if track_id is not None:
                        # ENHANCED: Limit ID to 2 digits for cleaner display
                        display_id = int(track_id) % 100  # Last 2 digits only
                        labels.append(f"#{display_id}")
                    else:
                        labels.append(f"{conf:.2f}")
                
                frame = box_annotator.annotate(scene=frame, detections=detections)
                frame = label_annotator.annotate(scene=frame, detections=detections, labels=labels)
                
                if len(detections.xyxy) > 0:
                    # FOOT-BASED TRACKING FIX: Use foot position (x at center, y at bottom) 
                    # instead of center of body when foot_based_tracking is enabled
                    for i, (track_id, xyxy) in enumerate(zip(tracker_ids, detections.xyxy)):
                        # CRITICAL FIX: Skip None and -1 track IDs (invalid tracks)
                        if track_id is not None and track_id != -1:
                            x1, y1, x2, y2 = xyxy
                            if foot_based_tracking:
                                # Use foot position: center X, bottom Y
                                player_x = int((x1 + x2) / 2)
                                player_y = int(y2)  # Bottom of box = foot position
                            else:
                                # Use center of body (legacy behavior)
                                player_x = int((x1 + x2) / 2)
                                player_y = int((y1 + y2) / 2)
                            
                            player_centers[track_id] = (player_x, player_y)
                            heatmap_data.append([player_x, player_y])
            
            # Calculate possession
            possession_player_id = None
            distance_to_ball = None
            if ball_center is not None and player_centers:
                possession_player_id, distance_to_ball = calculate_possession(
                    ball_center, player_centers, width, height
                )
            
            # Export to CSV (legacy path - player tracking disabled, limited analytics)
            if export_csv and csv_writer is not None:
                # Calculate ball analytics if available
                ball_x_m = None
                ball_y_m = None
                trajectory_angle = None
                ball_speed_mps = None
                # Note: Player analytics not available in this path (player tracking disabled)
                
                if player_centers:
                    for player_id, (px, py) in player_centers.items():
                        conf = 0.0
                        # Calculate distance to ball for all players
                        player_distance_to_ball = None
                        if ball_center is not None:
                            player_distance_to_ball = np.sqrt((ball_center[0] - px)**2 + (ball_center[1] - py)**2)
                        
                        csv_writer.writerow([
                            frame_count, timestamp,
                            ball_center[0] if ball_center else '', ball_center[1] if ball_center else '', ball_detected,
                            ball_x_m if ball_x_m is not None else '',
                            ball_y_m if ball_y_m is not None else '',
                            trajectory_angle if trajectory_angle is not None else '',
                            ball_speed_mps if ball_speed_mps is not None else '',
                            player_id, px, py,
                            '', '',  # player_x_m, player_y_m (not available without player tracking)
                            '', '',  # player_speed_mps, player_acceleration_mps2 (not available without player tracking)
                            '',  # player_movement_angle (not available without player tracking)
                            player_distance_to_ball if player_distance_to_ball is not None else '',
                            '', '', '',  # distance_traveled_m, max_speed_mps, sprint_count (not available without player tracking)
                            conf,
                            possession_player_id if possession_player_id == player_id else ''
                        ])
                else:
                    csv_writer.writerow([
                        frame_count, timestamp,
                        ball_center[0] if ball_center else '', ball_center[1] if ball_center else '', ball_detected,
                        ball_x_m if ball_x_m is not None else '',
                        ball_y_m if ball_y_m is not None else '',
                        trajectory_angle if trajectory_angle is not None else '',
                        ball_speed_mps if ball_speed_mps is not None else '',
                        '', '', '', '', '', '', '', '', '', '', '', ''
                    ])
            
            # Store frame for output (write at output frame rate)
            # This code path is only used when player tracking is disabled
            # CRITICAL FIX: Copy frame to avoid modifying original
            frame_copy = frame.copy()
            frames_to_write.append(frame_copy)
            
            # Write frames at output frame rate (downsample if needed)
            # CRITICAL FIX: Write frames immediately when no downsampling for smooth playback
            if not watch_only:  # Skip writing in watch-only mode
                if output_frame_skip > 1:
                    # Downsampling: batch frames and write every Nth frame
                    if len(frames_to_write) >= output_frame_skip or frame_count == total_frames - 1:
                        out.write(frames_to_write[0])
                        frames_to_write = []
                else:
                    # No downsampling: write each frame immediately for smooth playback
                    out.write(frame_copy)
                    frames_to_write = []  # Clear queue since we wrote immediately
        
        # SAFETY CHECK: Only increment frame_count if we haven't exceeded total_frames
        # (frame_count is already incremented earlier in the loop when reading frames)
        # This prevents the count from going over 100%
        if frame_count >= total_frames:
            break
        
        # AUTOMATIC LEARNING: Periodically update team colors from learned samples
        # OPTIMIZATION: Reduce update frequency once we have enough samples
        if len(learned_colors_by_team) > 0:
            # Check if we have enough samples for all teams
            all_teams_have_enough = all(len(samples) >= 30 for samples in learned_colors_by_team.values())
            
            # Use different update intervals based on sample count
            if all_teams_have_enough:
                # Once we have enough samples (30+), update less frequently (every 500 frames)
                # and only if there's a significant change
                update_interval = 500
            else:
                # While learning, update more frequently (every 100 frames)
                update_interval = 100
            
            if frame_count % update_interval == 0:
                updated_config = learn_team_colors_from_detections(team_colors, learned_colors_by_team, min_samples=10, max_samples=50)
                if updated_config:
                    if save_team_color_config(updated_config):
                        logger.info("Saved learned team colors to team_color_config.json")
                        # Reload team colors to use updated ranges IMMEDIATELY
                        team_colors = load_team_color_config()
                        # Also update the global team_colors reference used in classify_player_team
                        # This ensures the updated colors are used right away
                        if team_colors:
                            logger.info("Reloaded team colors - new ranges will be used for classification")
        
        # PERIODIC PLAYER GALLERY SAVE: Save every 1000 frames to prevent data loss
        if frame_count % 1000 == 0 and player_gallery and frame_count > 0:
            try:
                player_gallery.save_gallery()
                # Count players and reference frames for save message
                total_players = len(player_gallery.players)
                total_ref_frames = sum(len(p.reference_frames) if p.reference_frames else 0 for p in player_gallery.players.values())
                # SAFETY: Cap frame_count to total_frames to prevent >100% progress
                capped_frame_count = min(frame_count, total_frames) if total_frames > 0 else frame_count
                progress_pct = (capped_frame_count / total_frames) * 100 if total_frames > 0 else 0
                gallery_logger.info(f"Auto-saved player gallery (checkpoint at frame {capped_frame_count}/{total_frames}, {progress_pct:.1f}%): {total_players} players, {total_ref_frames} reference frames")
            except Exception as e:
                gallery_logger.error(f"Could not auto-save gallery at frame {frame_count}: {e}", exc_info=True)
            except Exception as e:
                pass  # Silently fail - saving is optional
        
        # UPDATE GUI: Send all current track assignments to live viewer controls
        if watch_only and show_live_viewer and frame_count % 30 == 0:  # Update every 30 frames (~1 second at 30fps)
            try:
                import shared_state
                live_viewer_controls = shared_state.get_live_viewer_controls()
                if live_viewer_controls:
                    # Send all active track assignments from player_names dict
                    # player_names is defined at function scope, so it's accessible here
                    try:
                        for track_id_str, player_name_raw in player_names.items():
                            # CRITICAL FIX: Handle case where player_name might be a list
                            if isinstance(player_name_raw, list) and len(player_name_raw) > 0:
                                player_name = str(player_name_raw[0])
                            elif player_name_raw:
                                player_name = str(player_name_raw)
                            else:
                                player_name = None
                            
                            if player_name and isinstance(player_name, str) and player_name.strip():
                                try:
                                    track_id_int = int(track_id_str)
                                    live_viewer_controls.update_track_assignment(track_id_int, player_name)
                                except (ValueError, TypeError) as e:
                                    pass  # Skip invalid track IDs
                    except (NameError, KeyError):
                        pass  # player_names might not be initialized yet
            except Exception as e:
                pass  # Silently fail if GUI not available
        
        # LEARNING PROGRESS REPORTING: Show what's being learned (more frequent updates)
        # CRITICAL FIX: Use last_frame_in_batch instead of frame_count to get actual processed frame number
        # frame_count might be 0 or low when first batch is processed, but last_frame_in_batch has the actual frame number
        current_processed_frame = last_frame_in_batch if last_frame_in_batch > 0 else frame_count
        # FIX: Only print if this is a new frame that matches the condition (prevent duplicate messages)
        if current_processed_frame % 200 == 0 and current_processed_frame > 0 and current_processed_frame != last_printed_learning_frame and (watch_only or len(learned_colors_by_team) > 0 or player_gallery):
            learning_stats = []
            if len(learned_colors_by_team) > 0:
                for team_name, colors in learned_colors_by_team.items():
                    learning_stats.append(f"{team_name}: {len(colors)} color samples")
            if player_gallery:
                players_with_data = 0
                total_shape_samples = 0
                total_movement_samples = 0
                total_position_samples = 0
                for player_id, profile in player_gallery.players.items():
                    if profile.shape_samples > 0 or profile.movement_samples > 0 or profile.position_samples > 0:
                        players_with_data += 1
                        total_shape_samples += profile.shape_samples
                        total_movement_samples += profile.movement_samples
                        total_position_samples += profile.position_samples
                if players_with_data > 0:
                    learning_stats.append(f"{players_with_data} players learning: {total_shape_samples} shape, {total_movement_samples} movement, {total_position_samples} position samples")
            if learning_stats:
                # Add frame count and time info
                elapsed = time.time() - start_time
                # SAFETY: Cap current_processed_frame to total_frames to prevent >100% progress
                capped_frame_count = min(current_processed_frame, total_frames) if total_frames > 0 else current_processed_frame
                progress_pct = (capped_frame_count / total_frames) * 100 if total_frames > 0 else 0
                rate = capped_frame_count / elapsed if elapsed > 0 else 0
                eta_seconds = (total_frames - capped_frame_count) / rate if rate > 0 else 0
                eta_min = int(eta_seconds / 60)
                eta_sec = int(eta_seconds % 60)
                print(f"   ðŸ“š Learning progress (Frame {capped_frame_count}/{total_frames}, {progress_pct:.1f}%): {', '.join(learning_stats)} | Rate: {rate:.1f} fps | ETA: {eta_min}m {eta_sec}s")
                last_printed_learning_frame = current_processed_frame  # Track that we printed for this frame
        
        # PROGRESS CALLBACK: Update GUI progress bar more frequently (every 50 frames)
        if progress_callback and frame_count % 50 == 0 and frame_count > 0:
            try:
                capped_frame_count = min(frame_count, total_frames) if total_frames > 0 else frame_count
                progress_pct = (capped_frame_count / total_frames) * 100 if total_frames > 0 else 0
                progress_callback(capped_frame_count, total_frames, progress_pct)
            except Exception as e:
                # Silently fail if callback has issues
                pass
        
        # DETAILED PROGRESS UPDATE: Every 500 frames, show more details
        if frame_count % 500 == 0 and frame_count > 0:
            elapsed = time.time() - start_time
            # SAFETY: Cap frame_count to total_frames to prevent >100% progress
            capped_frame_count = min(frame_count, total_frames) if total_frames > 0 else frame_count
            progress_pct = (capped_frame_count / total_frames) * 100 if total_frames > 0 else 0
            rate = capped_frame_count / elapsed if elapsed > 0 else 0
            eta_seconds = (total_frames - capped_frame_count) / rate if rate > 0 else 0
            eta_min = int(eta_seconds / 60)
            eta_sec = int(eta_seconds % 60)
            
            # Count active tracks
            active_tracks = len(player_names) if player_names else 0
            
            # Count matches per player (ONLY real names from gallery, no generic names)
            player_match_counts = {}
            for track_id_str, player_name_raw in player_names.items():
                # CRITICAL FIX: Handle case where player_name might be a list
                if isinstance(player_name_raw, list) and len(player_name_raw) > 0:
                    player_name = str(player_name_raw[0])
                elif player_name_raw:
                    player_name = str(player_name_raw)
                else:
                    player_name = None
                
                if player_name and isinstance(player_name, str) and player_name.strip():
                    # CRITICAL: Filter out generic names - only count real player names from gallery
                    # Generic names start with "Player ", "#", or are just track IDs
                    is_generic = (
                        player_name.startswith("Player ") or 
                        player_name.startswith("#") or
                        player_name.strip() == track_id_str or
                        player_name.startswith("Anonymous")
                    )
                    if not is_generic:
                        player_match_counts[player_name] = player_match_counts.get(player_name, 0) + 1
            
            # Show summary
            print(f"\n   ðŸ“Š Progress Update (Frame {capped_frame_count}/{total_frames}, {progress_pct:.1f}%):")
            print(f"      â€¢ Processing rate: {rate:.1f} fps | ETA: {eta_min}m {eta_sec}s")
            print(f"      â€¢ Active tracks: {active_tracks}")
            if player_match_counts:
                top_players = sorted(player_match_counts.items(), key=lambda x: x[1], reverse=True)[:5]
                player_list = ", ".join([f"{name} ({count})" for name, count in top_players])
                print(f"      â€¢ Top matched players: {player_list}")
            else:
                # Show warning if no real players matched
                unmatched_count = sum(1 for name in player_names.values() if name and (name.startswith("#") or name.startswith("Player ") or name.startswith("Anonymous")))
                if unmatched_count > 0:
                    print(f"      â€¢ âš  No gallery matches yet - {unmatched_count} tracks still need identification")
            if player_gallery:
                total_ref_frames = sum(len(p.reference_frames) if p.reference_frames else 0 for p in player_gallery.players.values())
                print(f"      â€¢ Gallery: {len(player_gallery.players)} players, {total_ref_frames} reference frames")
            print()
        
        # PROGRESS CALLBACK: Update GUI progress bar for non-tracking mode (every 50 frames)
        if progress_callback and (not (track_players_flag and model is not None) or watch_only):
            current_processed_frame = last_frame_in_batch if last_frame_in_batch > 0 else frame_count
            if current_processed_frame % 50 == 0 and current_processed_frame > 0:
                try:
                    capped_frame_count = min(current_processed_frame, total_frames) if total_frames > 0 else current_processed_frame
                    progress_pct = (capped_frame_count / total_frames) * 100 if total_frames > 0 else 0
                    progress_callback(capped_frame_count, total_frames, progress_pct)
                except Exception as e:
                    # Silently fail if callback has issues
                    pass
        
        # GENERAL PROGRESS UPDATE: For non-tracking mode or watch-only mode
        # CRITICAL FIX: Use last_frame_in_batch instead of frame_count to get actual processed frame number
        # Only report progress when we've actually processed frames (avoid frame 0 spam)
        current_processed_frame = last_frame_in_batch if last_frame_in_batch > 0 else frame_count
        if current_processed_frame % 200 == 0 and current_processed_frame > 0 and (not (track_players_flag and model is not None) or watch_only):
            # SAFETY: Cap current_processed_frame to total_frames to prevent >100% progress
            capped_frame_count = min(current_processed_frame, total_frames) if total_frames > 0 else current_processed_frame
            progress = (capped_frame_count / total_frames) * 100 if total_frames > 0 else 0
            elapsed = time.time() - start_time
            if elapsed > 0:
                rate = capped_frame_count / elapsed
                eta = (total_frames - capped_frame_count) / rate if rate > 0 else 0
                eta_min = int(eta / 60)
                eta_sec = int(eta % 60)
                print(f"â³ Progress: {progress:.1f}% ({capped_frame_count}/{total_frames} frames) | "
                      f"Rate: {rate:.1f} fps | ETA: {eta_min}m {eta_sec}s")
            else:
                print(f"â³ Progress: {progress:.1f}% ({capped_frame_count}/{total_frames} frames)")
    
    # Mark processing as complete (prevents gallery match messages after loop ends)
    is_processing = False
    
    # Final progress update (100%)
    if progress_callback:
        try:
            progress_callback(total_frames, total_frames, 100.0)
        except Exception as e:
            pass
    
    # Flush output before cleanup to ensure all messages are printed in order
    sys.stdout.flush()
    
    # Cleanup parallel preprocessing executor
    if preprocess_executor is not None:
        preprocess_executor.shutdown(wait=True)
    
    # PERFORMANCE: Cleanup CPU operations executor (team classification, uniform extraction)
    if 'cpu_ops_executor' in locals() and cpu_ops_executor is not None:
        cpu_ops_executor.shutdown(wait=True)
    
    cap.release()
    
    if not watch_only:
        # Release base video writer
        if base_video_writer is not None:
            base_video_writer.release()
            if base_video_path:
                print(f"âœ“ Base video saved: {base_video_path}")
                print(f"   â„¹ Original video ({input_path}) is also available as the base video")
        
        # Save overlay metadata (using pickle for faster serialization)
        if overlay_metadata is not None:
            try:
                # Use pickle by default for faster serialization (3-5x faster for large files)
                overlay_metadata_path = output_path.replace('.mp4', '_overlay_metadata.pkl')
                saved_path = overlay_metadata.save(overlay_metadata_path, use_pickle=True)
                print(f"âœ“ Overlay metadata saved: {saved_path}")
                print(f"   â†’ {len(overlay_metadata.overlays)} frames with overlay data")
                print(f"   â†’ Format: {'Pickle (fast)' if saved_path.endswith('.pkl') else 'JSON (portable)'}")
            except Exception as e:
                print(f"âš  Could not save overlay metadata: {e}")
        
        if out is not None:
            out.release()
            if enable_video_encoding:
                print(f"âœ“ Video output saved: {output_path}")
            else:
                print(f"â„¹ Video encoding was disabled - no analyzed video created")
        else:
            if enable_video_encoding and not watch_only:
                print(f"âš  WARNING: Video output was not created (out is None)")
                print(f"   â†’ Expected output path: {output_path}")
                print(f"   â†’ Watch-only mode was: {watch_only}")
                print(f"   â†’ This may indicate video encoding was disabled or failed to initialize")
            elif watch_only:
                # This is expected in watch-only mode
                pass
            elif not enable_video_encoding:
                # This is expected when video encoding is disabled
                pass
        
        csv_filename = None
        if csv_file:
            csv_filename = csv_file.name
            csv_file.close()
            print(f"âœ“ Tracking data exported to: {csv_filename}")
            # Show CSV export statistics
            if csv_export_stats['total_player_rows'] > 0:
                print(f"   â†’ CSV contains {csv_export_stats['total_player_rows']} player data row(s) from {csv_export_stats['frames_with_players']} frame(s)")
            else:
                print(f"   âš  CSV contains NO player data!")
                print(f"      â†’ Frames with players: {csv_export_stats['frames_with_players']}")
                print(f"      â†’ Frames with empty player_centers: {csv_export_stats['frames_with_empty_centers']}")
            
            # Show tracker statistics to diagnose why CSV has few tracks
            if 'tracker_stats' in csv_export_stats:
                stats = csv_export_stats['tracker_stats']
                print(f"\nðŸ“Š Tracker Statistics:")
                print(f"   â†’ Frames processed: {stats['frames_processed']}")
                print(f"   â†’ Frames with detections: {stats['frames_with_detections']}")
                print(f"   â†’ Frames with tracker IDs assigned: {stats['frames_with_tracker_ids']}")
                print(f"   â†’ Total detections: {stats['total_detections']}")
                print(f"   â†’ Total tracker IDs assigned: {stats['total_tracker_ids_assigned']}")
                if stats['frames_with_detections'] > 0:
                    assignment_rate = (stats['total_tracker_ids_assigned'] / stats['total_detections']) * 100 if stats['total_detections'] > 0 else 0
                    print(f"   â†’ Tracker ID assignment rate: {assignment_rate:.1f}%")
                    if assignment_rate < 50:
                        print(f"   âš  WARNING: Low tracker ID assignment rate ({assignment_rate:.1f}%)")
                        print(f"      â†’ This explains why CSV has few tracks")
                        print(f"      â†’ Possible causes:")
                        print(f"         - Tracker needs warmup frames (first 5-10 frames)")
                        print(f"         - Detection confidence too low for tracker threshold")
                        print(f"         - Tracker configuration issue")
                if stats['frames_tracker_failed'] > 0:
                    print(f"   âš  Frames where tracker received detections but assigned 0 IDs: {stats['frames_tracker_failed']}")
                if field_boundary_filter_disabled:
                    print(f"      â†’ Field boundary filter was disabled (was removing too many detections)")
                else:
                    print(f"      â†’ Field boundary filter status: {'enabled' if field_calibration else 'disabled (no calibration)'}")
        elif export_csv:
            print(f"âš  WARNING: CSV export was requested but file was not created")
        
        # Merge audio from original video if available
        # NOTE: Audio merge only works if video encoding was enabled (need a video file to merge into)
        if preserve_audio and enable_video_encoding:
            print("\nMerging audio from original video...")
            if merge_audio_from_source(input_path, output_path, preserve_audio=True):
                print("âœ“ Audio merged successfully!")
            else:
                print("âš  Could not merge audio (may not be available or FFmpeg not found)")
        elif preserve_audio and not enable_video_encoding:
            print("\nâš  Audio merge skipped: Video encoding was disabled (no video file to merge audio into)")
            print("   â†’ Enable 'Video Encoding' in GUI to create video with audio")
        
        # Generate heatmap
        if heatmap_data and MATPLOTLIB_AVAILABLE:
            print("Generating heatmap...")
            heatmap_data = np.array(heatmap_data)
            plt.figure(figsize=(width/100, height/100), dpi=100)
            plt.hist2d(heatmap_data[:, 0], heatmap_data[:, 1], bins=50, cmap='hot')
            plt.colorbar(label='Player Density')
            plt.xlabel('X Position (pixels)')
            plt.ylabel('Y Position (pixels)')
            plt.title('Player Position Heatmap')
            heatmap_filename = output_path.replace('.mp4', '_heatmap.png')
            plt.savefig(heatmap_filename, dpi=100, bbox_inches='tight')
            plt.close()
            print(f"Heatmap saved: {heatmap_filename}")
    else:
        # Watch-only mode: close CSV file if opened
        if csv_file:
            csv_file.close()
    
    # AUTOMATIC LEARNING: Final save and summary (only print once at the end)
    # Flush any pending output before printing summary
    sys.stdout.flush()
    
    print("\n" + "=" * 60)
    print("ðŸ“š LEARNING SUMMARY")
    print("=" * 60)
    
    # ANCHOR FRAME PROTECTION SUMMARY
    if 'anchor_frame_stats' in locals() and anchor_frame_stats.get('total_anchor_tags', 0) > 0:
        print(f"\nðŸ›¡ï¸ ANCHOR FRAME PROTECTION SUMMARY:")
        print(f"   â†’ Total anchor frames loaded: {anchor_frame_stats.get('total_anchor_frames', 0)}")
        print(f"   â†’ Total anchor tags: {anchor_frame_stats.get('total_anchor_tags', 0)}")
        print(f"   â†’ Tracks with anchor protection: {len(anchor_frame_stats.get('protected_tracks', set()))}")
        
        if anchor_frame_stats.get('successful_matches'):
            print(f"   â†’ Successful anchor matches:")
            for player_name, count in sorted(anchor_frame_stats['successful_matches'].items()):
                print(f"      â€¢ {player_name}: {count} frame(s) protected")
        
        if anchor_frame_stats.get('failed_matches'):
            print(f"   âš  FAILED anchor matches (NO PROTECTION APPLIED):")
            for player_name, count in sorted(anchor_frame_stats['failed_matches'].items()):
                print(f"      â€¢ {player_name}: {count} frame(s) FAILED - player is NOT protected from Re-ID override")
                print(f"        â†’ Check diagnostic messages above for 'ANCHOR MATCH FAILED'")
                print(f"        â†’ Possible causes: track_id mismatch, bbox coordinate mismatch, or no detections")
        
        total_successful = sum(anchor_frame_stats.get('successful_matches', {}).values())
        total_failed = sum(anchor_frame_stats.get('failed_matches', {}).values())
        total_tags = anchor_frame_stats.get('total_anchor_tags', 0)
        success_rate = (total_successful / total_tags * 100) if total_tags > 0 else 0
        print(f"   â†’ Success rate: {total_successful}/{total_tags} ({success_rate:.1f}%)")
        
        if total_failed > 0:
            print(f"   âš  WARNING: {total_failed} anchor frame(s) failed to match - these players are NOT protected!")
            print(f"      â†’ Re-ID may override their identities")
            print(f"      â†’ Review anchor frame track_ids and bbox coordinates")
    
    # Team color learning summary
    if len(learned_colors_by_team) > 0:
        print(f"\nðŸŽ¨ Team Color Learning:")
        for team_name, colors in learned_colors_by_team.items():
            print(f"   â€¢ {team_name}: {len(colors)} color samples collected")
        updated_config = learn_team_colors_from_detections(team_colors, learned_colors_by_team, min_samples=5)  # Lower threshold for final save
        if updated_config:
            if save_team_color_config(updated_config):
                print(f"   ðŸ’¾ Saved learned team colors to team_color_config.json")
    else:
        print(f"\nðŸŽ¨ Team Color Learning: No samples collected yet")
    
    # Player learning summary
    if player_gallery:
        try:
            players_with_data = 0
            total_shape_samples = 0
            total_movement_samples = 0
            total_position_samples = 0
            total_ball_interaction_samples = 0
            players_with_features = 0
            
            for player_id, profile in player_gallery.players.items():
                has_data = False
                if profile.shape_samples > 0:
                    total_shape_samples += profile.shape_samples
                    has_data = True
                if profile.movement_samples > 0:
                    total_movement_samples += profile.movement_samples
                    has_data = True
                if profile.position_samples > 0:
                    total_position_samples += profile.position_samples
                    has_data = True
                if profile.ball_interaction_samples > 0:
                    total_ball_interaction_samples += profile.ball_interaction_samples
                    has_data = True
                if profile.features is not None:
                    players_with_features += 1
                if has_data:
                    players_with_data += 1
            
            print(f"\nðŸ‘¤ Player Learning:")
            print(f"   â€¢ {players_with_data} players with learned data")
            print(f"   â€¢ {players_with_features} players with Re-ID features")
            if total_shape_samples > 0:
                print(f"   â€¢ Shape samples: {total_shape_samples} total")
            if total_movement_samples > 0:
                print(f"   â€¢ Movement samples: {total_movement_samples} total")
            if total_position_samples > 0:
                print(f"   â€¢ Position samples: {total_position_samples} total")
            if total_ball_interaction_samples > 0:
                print(f"   â€¢ Ball interaction samples: {total_ball_interaction_samples} total")
            
            # Show detailed stats for each player (normalize names to prevent duplicates)
            if players_with_data > 0:
                print(f"\n   Detailed player stats:")
                # Group by normalized name to prevent duplicates
                player_stats_by_name = {}
                for player_id, profile in player_gallery.players.items():
                    # Normalize player name to handle list format and prevent duplicates
                    normalized_name = extract_player_name(profile.name)
                    if normalized_name not in player_stats_by_name:
                        player_stats_by_name[normalized_name] = {
                            'shape': 0,
                            'movement': 0,
                            'position': 0,
                            'ball': 0,
                            'movement_style': None,
                            'player_ids': []
                        }
                    stats_dict = player_stats_by_name[normalized_name]
                    stats_dict['shape'] += profile.shape_samples
                    stats_dict['movement'] += profile.movement_samples
                    stats_dict['position'] += profile.position_samples
                    stats_dict['ball'] += profile.ball_interaction_samples
                    if profile.movement_style and not stats_dict['movement_style']:
                        stats_dict['movement_style'] = profile.movement_style
                    stats_dict['player_ids'].append(player_id)
                
                # Print consolidated stats
                for normalized_name in sorted(player_stats_by_name.keys()):
                    stats_dict = player_stats_by_name[normalized_name]
                    stats = []
                    if stats_dict['shape'] > 0:
                        stats.append(f"shape({stats_dict['shape']})")
                    if stats_dict['movement'] > 0:
                        stats.append(f"movement({stats_dict['movement']})")
                        if stats_dict['movement_style']:
                            stats.append(f"style:{stats_dict['movement_style']}")
                    if stats_dict['position'] > 0:
                        stats.append(f"position({stats_dict['position']})")
                    if stats_dict['ball'] > 0:
                        stats.append(f"ball({stats_dict['ball']})")
                    if stats:
                        duplicate_note = f" (merged {len(stats_dict['player_ids'])} duplicate entries)" if len(stats_dict['player_ids']) > 1 else ""
                        print(f"      â€¢ {normalized_name}{duplicate_note}: {', '.join(stats)}")
            
            # ðŸŽ“ ANCHOR LEARNING SUMMARY: Show how many frames we learned from anchor-protected players
            print(f"\nðŸŽ“ Anchor Frame Learning (Ground Truth):")
            # DIAGNOSTIC: Check if anchor_learning_stats exists and has data
            # Note: anchor_learning_stats is defined in the main function scope, so it should be accessible
            try:
                stats_available = len(anchor_learning_stats) > 0
                if not stats_available:
                    print(f"   âš  DEBUG: anchor_learning_stats is empty (length: 0)")
                else:
                    print(f"   âœ“ DEBUG: anchor_learning_stats has {len(anchor_learning_stats)} entries: {list(anchor_learning_stats.keys())}")
            except NameError:
                print(f"   âš  DEBUG: anchor_learning_stats not found in scope")
                stats_available = False
            except Exception as e:
                print(f"   âš  DEBUG: Error accessing anchor_learning_stats: {e}")
                stats_available = False
            
            if stats_available:
                print(f"   â†’ Learning from anchor-protected players with 10x weight (ground truth)")
                total_frames_learned = sum(stats['frames_learned'] for stats in anchor_learning_stats.values())
                print(f"   â†’ Total frames learned from anchor frames: {total_frames_learned}")
                for player_name, stats in sorted(anchor_learning_stats.items()):
                    frame_range = stats['last_frame'] - stats['first_frame'] + 1
                    coverage = (stats['frames_learned'] / frame_range * 100) if frame_range > 0 else 0
                    
                    # Get current reference frame count for this player (including uniform variants)
                    player_ref_count = 0
                    for pid, profile in player_gallery.players.items():
                        if profile.name == player_name:
                            player_ref_count = player_gallery.get_total_reference_frames(pid)
                            break
                    
                    print(f"      â€¢ {player_name}: {stats['frames_learned']} frames learned â†’ {player_ref_count} total ref frames")
                    print(f"        â†’ Frame range: {stats['first_frame']}-{stats['last_frame']} ({frame_range} frames)")
                    print(f"        â†’ Coverage: {coverage:.1f}% of frames where player appeared")
                    print(f"        â†’ Features added: {stats['total_features']} (ground truth, 10x weight)")
                print(f"\n   ðŸ’¡ TIP: Re-run this video to continue learning!")
                print(f"      â†’ Each run adds more features to the gallery")
                print(f"      â†’ Goal: Build up features until reaching 100% accuracy")
            else:
                # Show message if no anchor learning occurred (helps diagnose issues)
                print(f"   â†’ No anchor-protected players were learned from in this run")
                print(f"   â†’ Possible reasons:")
                print(f"      â€¢ No anchor frames were tagged for this video")
                print(f"      â€¢ Re-ID features were not available (reid_features is None)")
                print(f"      â€¢ Anchor-protected tracks did not match gallery players")
                print(f"      â€¢ Detection index mismatch (track not found in detections)")
                print(f"   â†’ To enable learning: Tag players in anchor frames using Setup Wizard or Gallery Seeder")
                print(f"   â†’ Check diagnostic messages above for 'ANCHOR LEARNING SKIPPED' warnings")
                
                # Show skipped reasons if we tracked them
                try:
                    if 'anchor_learning_stats' in locals() and len(anchor_learning_stats) > 0:
                        print(f"   â†’ Attempted learning for {len(anchor_learning_stats)} player(s) but conditions weren't met:")
                        for player_name, stats in anchor_learning_stats.items():
                            if stats.get('frames_learned', 0) == 0 and 'skipped_reasons' in stats:
                                reasons = stats['skipped_reasons']
                                unique_reasons = list(set(reasons))  # Remove duplicates
                                print(f"      â€¢ {player_name}: {', '.join(unique_reasons[:3])}")  # Show first 3 reasons
                except Exception:
                    pass
            
            # ðŸŽ¯ UNTAGGED PLAYER IDENTIFICATION SUMMARY: Show which untagged players were identified
            print(f"\nðŸŽ¯ Untagged Player Identification:")
            if len(untagged_player_stats) > 0:
                print(f"   â†’ Re-ID identified {len(untagged_player_stats)} untagged player(s) in this video")
                total_untagged_frames = sum(stats['total_frames'] for stats in untagged_player_stats.values())
                print(f"   â†’ Total frames with untagged players identified: {total_untagged_frames}")
                for player_name, stats in sorted(untagged_player_stats.items()):
                    frame_range = stats['last_identified_frame'] - stats['first_identified_frame'] + 1
                    track_ids_str = ', '.join([f"#{tid}" for tid in sorted(stats['track_ids'])])
                    print(f"      â€¢ {player_name}:")
                    print(f"        â†’ Identified on {len(stats['track_ids'])} track(s): {track_ids_str}")
                    print(f"        â†’ Frame range: {stats['first_identified_frame']}-{stats['last_identified_frame']} ({frame_range} frames)")
                    print(f"        â†’ Total frames: {stats['total_frames']} ({stats['total_frames']/frame_range*100:.1f}% coverage)")
                    print(f"        â†’ Confidence: avg={stats['avg_confidence']:.3f}, max={stats['max_confidence']:.3f}, min={stats['min_confidence']:.3f}")
                    if stats['frames_learned'] > 0:
                        print(f"        â†’ Gallery learning: {stats['frames_learned']} frames learned (5x weight)")
                print(f"\n   ðŸ’¡ TIP: Untagged players are identified automatically via Re-ID matching")
                print(f"      â†’ High-confidence matches (â‰¥0.75) are learned and added to gallery")
                print(f"      â†’ This improves future identification accuracy")
            else:
                print(f"   â†’ No untagged players were identified in this run")
                print(f"   â†’ Possible reasons:")
                print(f"      â€¢ All players were tagged in anchor frames")
                print(f"      â€¢ Re-ID similarity threshold too high (current: {gallery_similarity_threshold:.2f})")
                print(f"      â€¢ Gallery doesn't have features for untagged players")
                print(f"   â†’ To identify untagged players: Ensure gallery has features for all players")
                print(f"   â†’ Lower similarity threshold in GUI if needed (current: {gallery_similarity_threshold:.2f})")
            
            # ðŸŽ“ UNTAGGED LEARNING SUMMARY: Show learning from high-confidence untagged matches
            print(f"\nðŸŽ“ Untagged Player Learning (High-Confidence Matches):")
            if len(untagged_learning_stats) > 0:
                print(f"   â†’ Learning from untagged players with 5x weight (similarity â‰¥0.75)")
                total_untagged_frames_learned = sum(stats['frames_learned'] for stats in untagged_learning_stats.values())
                print(f"   â†’ Total frames learned from untagged matches: {total_untagged_frames_learned}")
                for player_name, stats in sorted(untagged_learning_stats.items()):
                    frame_range = stats['last_frame'] - stats['first_frame'] + 1
                    coverage = (stats['frames_learned'] / frame_range * 100) if frame_range > 0 else 0
                    
                    # Get current reference frame count for this player
                    player_ref_count = 0
                    for pid, profile in player_gallery.players.items():
                        if profile.name == player_name:
                            player_ref_count = player_gallery.get_total_reference_frames(pid)
                            break
                    
                    print(f"      â€¢ {player_name}: {stats['frames_learned']} frames learned â†’ {player_ref_count} total ref frames")
                    print(f"        â†’ Frame range: {stats['first_frame']}-{stats['last_frame']} ({frame_range} frames)")
                    print(f"        â†’ Coverage: {coverage:.1f}% of frames where player appeared")
                    print(f"        â†’ Features added: {stats['total_features']} (high-confidence, 5x weight)")
                    print(f"        â†’ Average confidence: {stats['avg_confidence']:.3f}")
                print(f"\n   ðŸ’¡ TIP: Untagged learning improves gallery features for future videos")
                print(f"      â†’ High-confidence matches (â‰¥0.75) are automatically learned")
                print(f"      â†’ Each run builds up more features for better accuracy")
            else:
                print(f"   â†’ No high-confidence untagged matches were learned from (similarity <0.75)")
                print(f"   â†’ To enable learning: Ensure Re-ID matches have similarity â‰¥0.75")
                print(f"   â†’ Current similarity threshold: {gallery_similarity_threshold:.2f}")
            
            # Show reference frame counts for all players (including uniform variants)
            print(f"\nðŸ“Š Gallery Reference Frame Summary:")
            total_ref_frames_all = 0
            players_with_refs = 0
            for player_id, profile in sorted(player_gallery.players.items(), key=lambda x: x[1].name):
                total_refs = player_gallery.get_total_reference_frames(player_id)
                if total_refs > 0:
                    players_with_refs += 1
                    total_ref_frames_all += total_refs
                    # Count anchor frames specifically
                    anchor_count = 0
                    if profile.reference_frames:
                        anchor_count = sum(1 for rf in profile.reference_frames if rf.get('is_anchor', False) or (rf.get('confidence', 0.0) >= 1.00 and rf.get('similarity', 0.0) >= 1.00))
                    if profile.uniform_variants:
                        for variant_refs in profile.uniform_variants.values():
                            if variant_refs:
                                anchor_count += sum(1 for rf in variant_refs if rf.get('is_anchor', False) or (rf.get('confidence', 0.0) >= 1.00 and rf.get('similarity', 0.0) >= 1.00))
                    
                    if anchor_count > 0:
                        print(f"   â€¢ {profile.name}: {total_refs} total ref frames ({anchor_count} anchor frames)")
                    else:
                        print(f"   â€¢ {profile.name}: {total_refs} total ref frames")
            
            if players_with_refs == 0:
                print(f"   â†’ No reference frames in gallery yet")
            else:
                print(f"   â†’ Total: {total_ref_frames_all} reference frames across {players_with_refs} players")
            
            player_gallery.save_gallery()
            if frame_count > 0:
                total_players = len(player_gallery.players)
                gallery_logger.info(f"Saved {total_players} players to gallery: player_gallery.json")
        except Exception as e:
            logger.error(f"Error generating learning summary: {e}", exc_info=True)
    
    print("=" * 60)
    
    # COMPREHENSIVE METRICS EVALUATION: Evaluate tracking quality if CSV was exported
    # Evaluates HOTA, MOTA, and IDF1 - all work together with Re-ID
    if export_csv and csv_filename and os.path.exists(csv_filename):
        try:
            from tracking_metrics_evaluator import evaluate_tracking_metrics
            
            print("\nðŸ“Š Comprehensive Tracking Metrics Evaluation:")
            print("   Evaluating tracking quality (HOTA, MOTA, IDF1)...")
            
            # Use anchor frames as ground truth if available
            # Use the file path that was loaded during anchor frame loading
            anchor_frames_file = anchor_frames_file_path if anchor_frames and anchor_frames_file_path else None
            
            if anchor_frames_file:
                print(f"   Using anchor frames from: {os.path.basename(anchor_frames_file)}")
            elif anchor_frames:
                # If we have anchor frames but no file path, try to find it
                video_dir = os.path.dirname(os.path.abspath(input_path))
                video_basename = os.path.splitext(os.path.basename(input_path))[0]
                
                # Try multiple locations
                possible_files = [
                    os.path.join(video_dir, f"PlayerTagsSeed-{video_basename}-Project.json"),
                    os.path.join(video_dir, f"PlayerTagsSeed-{video_basename}.json"),
                    os.path.join(video_dir, "seed_config.json"),
                    os.path.abspath("seed_config.json"),  # Project root
                ]
                
                # Also search for any PlayerTagsSeed-*.json files (including part*.json)
                try:
                    for filename in os.listdir(video_dir):
                        if filename.startswith("PlayerTagsSeed-") and filename.endswith(".json"):
                            possible_files.append(os.path.join(video_dir, filename))
                except:
                    pass
                
                for possible_file in possible_files:
                    if os.path.exists(possible_file):
                        # Verify it's for this video
                        try:
                            import json
                            with open(possible_file, 'r') as f:
                                seed_data = json.load(f)
                            if "anchor_frames" in seed_data:
                                file_video_path = seed_data.get("video_path", "")
                                if file_video_path and (os.path.basename(file_video_path) == os.path.basename(input_path) or 
                                                       os.path.abspath(file_video_path) == os.path.abspath(input_path)):
                                    anchor_frames_file = possible_file
                                    break
                        except:
                            continue
                
                if anchor_frames_file:
                    print(f"   Found anchor frames file: {os.path.basename(anchor_frames_file)}")
                else:
                    print(f"   âš  No anchor frames file found for {video_basename} (but anchor frames were loaded)")
            
            metrics_results = evaluate_tracking_metrics(csv_filename, anchor_frames_file)
            
            if 'error' not in metrics_results:
                print(f"\n   ðŸ“Š HOTA (Higher Order Tracking Accuracy):")
                print(f"      Overall HOTA Score: {metrics_results.get('HOTA', 0):.4f} (higher is better, max: 1.0)")
                print(f"      Detection Accuracy (DetA): {metrics_results.get('DetA', 0):.4f}")
                print(f"      Association Accuracy (AssA): {metrics_results.get('AssA', 0):.4f}")
                print(f"      Detection: Recall={metrics_results.get('DetRe', 0):.4f}, Precision={metrics_results.get('DetPr', 0):.4f}")
                print(f"      Association: Recall={metrics_results.get('AssRe', 0):.4f}, Precision={metrics_results.get('AssPr', 0):.4f}")
                
                print(f"\n   ðŸ“Š MOTA (Multiple Object Tracking Accuracy):")
                print(f"      MOTA Score: {metrics_results.get('MOTA', 0):.4f} (higher is better, max: 1.0)")
                print(f"      MOTP (Precision): {metrics_results.get('MOTP', 0):.4f}")
                print(f"      False Negatives: {metrics_results.get('FN', 0)}, False Positives: {metrics_results.get('FP', 0)}")
                print(f"      ID Switches: {metrics_results.get('IDSW', 0)}, Ground Truth: {metrics_results.get('GT', 0)}")
                
                print(f"\n   ðŸ“Š IDF1 (ID F1 Score):")
                print(f"      IDF1 Score: {metrics_results.get('IDF1', 0):.4f} (higher is better, max: 1.0)")
                print(f"      ID Precision: {metrics_results.get('IDP', 0):.4f}, ID Recall: {metrics_results.get('IDR', 0):.4f}")
                print(f"      ID True Positives: {metrics_results.get('IDTP', 0)}")
                print(f"      ID False Positives: {metrics_results.get('IDFP', 0)}, ID False Negatives: {metrics_results.get('IDFN', 0)}")
                
                # Save comprehensive results to file
                metrics_results_file = csv_filename.replace('.csv', '_tracking_metrics_results.txt')
                with open(metrics_results_file, 'w') as f:
                    f.write("Comprehensive Tracking Metrics Evaluation Results\n")
                    f.write("=" * 60 + "\n\n")
                    f.write("All metrics work together with Re-ID for comprehensive evaluation\n\n")
                    
                    f.write("HOTA (Higher Order Tracking Accuracy):\n")
                    f.write(f"  Overall HOTA Score: {metrics_results.get('HOTA', 0):.4f}\n")
                    f.write(f"  Detection Accuracy (DetA): {metrics_results.get('DetA', 0):.4f}\n")
                    f.write(f"  Association Accuracy (AssA): {metrics_results.get('AssA', 0):.4f}\n")
                    f.write(f"  Detection: Recall={metrics_results.get('DetRe', 0):.4f}, Precision={metrics_results.get('DetPr', 0):.4f}\n")
                    f.write(f"  Association: Recall={metrics_results.get('AssRe', 0):.4f}, Precision={metrics_results.get('AssPr', 0):.4f}\n\n")
                    
                    f.write("MOTA (Multiple Object Tracking Accuracy):\n")
                    f.write(f"  MOTA Score: {metrics_results.get('MOTA', 0):.4f}\n")
                    f.write(f"  MOTP (Precision): {metrics_results.get('MOTP', 0):.4f}\n")
                    f.write(f"  False Negatives: {metrics_results.get('FN', 0)}\n")
                    f.write(f"  False Positives: {metrics_results.get('FP', 0)}\n")
                    f.write(f"  ID Switches: {metrics_results.get('IDSW', 0)}\n")
                    f.write(f"  Ground Truth: {metrics_results.get('GT', 0)}\n\n")
                    
                    f.write("IDF1 (ID F1 Score):\n")
                    f.write(f"  IDF1 Score: {metrics_results.get('IDF1', 0):.4f}\n")
                    f.write(f"  ID Precision: {metrics_results.get('IDP', 0):.4f}\n")
                    f.write(f"  ID Recall: {metrics_results.get('IDR', 0):.4f}\n")
                    f.write(f"  ID True Positives: {metrics_results.get('IDTP', 0)}\n")
                    f.write(f"  ID False Positives: {metrics_results.get('IDFP', 0)}\n")
                    f.write(f"  ID False Negatives: {metrics_results.get('IDFN', 0)}\n\n")
                    
                    f.write("Interpretation:\n")
                    f.write("â€¢ HOTA: Balanced detection and association accuracy\n")
                    f.write("â€¢ MOTA: Traditional tracking accuracy (penalizes false positives, negatives, ID switches)\n")
                    f.write("â€¢ IDF1: ID consistency over time (measures how well IDs are maintained)\n")
                    f.write("â€¢ Re-ID: Improves tracking during analysis, all metrics evaluate the results\n")
                print(f"   ðŸ’¾ Saved comprehensive metrics results to: {metrics_results_file}")
            else:
                print(f"   âš  Metrics evaluation skipped: {metrics_results.get('error', 'Unknown error')}")
        except ImportError:
            print("   âš  Tracking metrics evaluator not available (tracking_metrics_evaluator.py not found)")
        except Exception as e:
            print(f"   âš  Metrics evaluation failed: {e}")
    
    # Progress update: Analysis complete
    try:
        import shared_state
        shared_state.update_analysis_progress(
            current=total_frames,
            total=total_frames,
            status="Analysis complete",
            details=f"Processed {total_frames} frames",
            phase="Complete"
        )
        # Clear progress after a short delay (let GUI see completion)
        import time as time_module
        time_module.sleep(0.5)  # Brief delay so GUI can display completion
        shared_state.clear_analysis_progress()
    except (ImportError, Exception):
        pass  # shared_state not available, continue normally
    
    total_time = time.time() - start_time
    # OPTIMIZATION: Cleanup threaded viewer
    if viewer_thread_running:
        viewer_thread_running = False
        if viewer_queue is not None:
            try:
                viewer_queue.put((None, None), block=False)  # Shutdown signal
            except:
                pass
        if viewer_thread is not None:
            try:
                viewer_thread.join(timeout=1.0)  # Wait up to 1 second for thread to finish
            except:
                pass
    
    # Close live viewer window if it was opened
    if live_viewer_window:
        try:
            cv2.destroyWindow(live_viewer_window)
        except:
            pass
    
    # Clear global dynamic_settings reference
    if dynamic_settings:
        try:
            import shared_state
            shared_state.clear_dynamic_settings()
        except ImportError:
            # Fallback to module-level variable
            try:
                import combined_analysis_optimized as analysis_module
                if hasattr(analysis_module, '_current_dynamic_settings'):
                    delattr(analysis_module, '_current_dynamic_settings')
            except:
                pass
    
    if watch_only:
        print("=" * 60)
        print("ðŸ‘€ WATCH-ONLY MODE COMPLETE!")
        print("=" * 60)
        print(f"   â†’ Processed {total_frames} frames")
        print(f"   â†’ Total time: {total_time/60:.1f} minutes")
        print(f"   â†’ Processing rate: {total_frames/total_time:.1f} fps")
        print(f"   â†’ Learned data saved to:")
        print(f"      â€¢ player_gallery.json")
        print(f"      â€¢ team_color_config.json")
        print("=" * 60)
    else:
        # Print filter module statistics if available
        if reid_tracker and hasattr(reid_tracker, 'filter_module') and reid_tracker.filter_module is not None:
            reid_tracker.filter_module.print_statistics()
        
        print(f"Analysis complete! Output saved: {output_path}")
        print(f"Total processing time: {total_time/60:.1f} minutes")
        print(f"Average processing rate: {total_frames/total_time:.1f} fps")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Optimized combined soccer analysis with batch processing"
    )
    parser.add_argument("--input", required=True, help="Input video path")
    parser.add_argument("--output", required=True, help="Output video path")
    parser.add_argument("--dewarp", action="store_true", help="Apply dewarping correction")
    parser.add_argument("--no-ball", action="store_true", help="Skip ball tracking")
    parser.add_argument("--no-players", action="store_true", help="Skip player tracking")
    parser.add_argument("--no-csv", action="store_true", help="Skip CSV export")
    parser.add_argument("--buffer", type=int, default=64, help="Ball trail length (default: 64)")
    parser.add_argument("--batch-size", type=int, default=8, help="YOLO batch size (default: 8, higher = more GPU usage)")
    parser.add_argument("--ball-min-radius", type=int, default=5, help="Minimum ball radius in pixels (default: 5)")
    parser.add_argument("--ball-max-radius", type=int, default=50, help="Maximum ball radius in pixels (default: 50)")
    parser.add_argument("--remove-net", action="store_true", help="Attempt to reduce net visibility (for indoor practice)")
    parser.add_argument("--optical-flow", action="store_true", help="Enable optical flow motion prediction (reduces tracking blinking, especially with indoor nets)")
    parser.add_argument("--no-ball-trail", action="store_true", help="Hide ball trail (red lines)")
    parser.add_argument("--track-thresh", type=float, default=0.25, help="Tracker detection threshold (default: 0.25, lower = more detections)")
    parser.add_argument("--match-thresh", type=float, default=0.8, help="Tracker matching threshold (default: 0.8, higher = stricter matching)")
    parser.add_argument("--track-buffer", type=int, default=30, help="Tracker buffer frames (default: 30, higher = more persistent tracking)")
    parser.add_argument("--tracker-type", type=str, default="bytetrack", choices=["bytetrack", "ocsort"], help="Tracker type: 'bytetrack' (faster) or 'ocsort' (better occlusion handling, default: bytetrack)")
    parser.add_argument("--output-fps", type=float, default=None, help="Output video frame rate (default: same as input). Lower = smaller file, slower playback")
    parser.add_argument("--process-every-nth", type=int, default=1, help="Process every Nth frame for tracking (default: 1 = all frames). Higher = faster but less accurate")
    parser.add_argument("--no-temporal-smoothing", action="store_true", help="Disable temporal smoothing (default: enabled for better stability)")
    parser.add_argument("--yolo-resolution", type=str, default="full", choices=["full", "1080p", "720p"], help="YOLO processing resolution (default: full). Lower = faster processing")
    parser.add_argument("--no-foot-based-tracking", action="store_true", help="Disable foot-based tracking (default: enabled for better stability)")
    parser.add_argument("--watch-only", action="store_true", help="Watch-only mode: learn from video without saving output (faster, no video encoding)")
    parser.add_argument("--show-live-viewer", action="store_true", help="Show live viewer window during watch-only mode (press 'q' to close)")
    parser.add_argument("--viewer-downscale", type=str, default="720p", choices=["none", "720p", "1080p"], help="Downscale live viewer for performance (default: 720p, 'none' = full resolution)")
    parser.add_argument("--viewer-threaded", action="store_true", help="Use threaded viewer (non-blocking, better for high-res videos)")
    parser.add_argument("--seed-frame", type=int, default=None, help="Force gallery mapping every N frames (default: None, 0 = every 100 frames)")
    parser.add_argument("--gui", action="store_true", help="Launch GUI instead of CLI mode")
    parser.add_argument("--visualize", action="store_true", help="Enable visualization overlays (default: always enabled, this flag is for clarity)")
    args = parser.parse_args()
    
    combined_analysis_optimized(
        args.input, args.output,
        dewarp=args.dewarp,
        output_fps=args.output_fps,
        process_every_nth_frame=args.process_every_nth,
        temporal_smoothing=not args.no_temporal_smoothing,
        yolo_resolution=args.yolo_resolution,
        foot_based_tracking=not args.no_foot_based_tracking,
        use_reid=not args.no_reid,
        reid_similarity_threshold=args.reid_threshold,
        use_enhanced_kalman=not args.no_enhanced_kalman,
        use_ema_smoothing=not args.no_ema_smoothing,
        confidence_filtering=not args.no_confidence_filtering,
        adaptive_confidence=not args.no_adaptive_confidence,
        track_ball_flag=not args.no_ball,
        track_players_flag=not args.no_players,
        export_csv=not args.no_csv,
        buffer=args.buffer,
        batch_size=args.batch_size,
        ball_min_radius=args.ball_min_radius,
        ball_max_radius=args.ball_max_radius,
        remove_net=args.remove_net,
        use_optical_flow=args.optical_flow,
        show_ball_trail=not args.no_ball_trail,
        track_thresh=args.track_thresh,
        match_thresh=args.match_thresh,
        track_buffer=args.track_buffer,
        tracker_type=args.tracker_type,
        watch_only=args.watch_only,
        show_live_viewer=args.show_live_viewer,
        viewer_downscale=args.viewer_downscale,
        viewer_threaded=args.viewer_threaded
    )
